{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty line\n"
     ]
    }
   ],
   "source": [
    "#read in the word embeddings\n",
    "vec_length = 100\n",
    "embeddings = np.zeros((1193514+2, vec_length))\n",
    "\n",
    "#two-way map, index->word and word->index\n",
    "glove = {}\n",
    "\n",
    "#add special tokens for unknown and padding\n",
    "embeddings[0] = np.zeros(vec_length)\n",
    "glove[0] = 'UNK'\n",
    "glove['UNK'] = 0\n",
    "\n",
    "embeddings[1] = np.zeros(vec_length)\n",
    "glove[1] = 'PAD'\n",
    "glove['PAD'] = 1\n",
    "\n",
    "index = 2\n",
    "with open('glove.twitter.27B.%dd.txt' % vec_length) as f:\n",
    "    for l in f:\n",
    "        line = []\n",
    "        try:\n",
    "            line = l.split()\n",
    "            if len(line) != vec_length+1:\n",
    "                print('empty line')\n",
    "                continue\n",
    "            \n",
    "            word = line[0]\n",
    "            embeddings[index] = np.array(line[1:]).astype(np.float)\n",
    "            glove[index] = word\n",
    "            glove[word] = index\n",
    "            index += 1\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4078, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>Urgency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>harveystorm water waist deep respect cop help ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>find help affect hurricane harvey yeg hurrican...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mountainview heroes deploy help harvey &lt;url&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>help impact hurricaneharvey weave activate don...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>much flood houston wow tune news prayers sympa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Relevancy  Urgency\n",
       "0  harveystorm water waist deep respect cop help ...          0        0\n",
       "1  find help affect hurricane harvey yeg hurrican...          1        0\n",
       "2       mountainview heroes deploy help harvey <url>          3        0\n",
       "3  help impact hurricaneharvey weave activate don...          0        0\n",
       "4  much flood houston wow tune news prayers sympa...          0        0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the dataset\n",
    "df = pd.read_csv('final_dataset_processed.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.035785945166337485\n",
      "735\n"
     ]
    }
   ],
   "source": [
    "#now convert the tweets into a list of indices\n",
    "X = []\n",
    "unk_percent = []\n",
    "unk_words = set()\n",
    "max_len = 0\n",
    "for tweet in df['Text']:\n",
    "    indices = []\n",
    "    words = tweet.split()\n",
    "    if len(words) > max_len:\n",
    "        max_len = len(words)\n",
    "    unknown = 0\n",
    "    for word in words:\n",
    "        if word in glove:\n",
    "            indices.append(glove[word])\n",
    "        else:\n",
    "            indices.append(glove['UNK'])\n",
    "            unk_words.add(word)\n",
    "            unknown += 1\n",
    "        unk_percent.append(unknown/len(words))\n",
    "    X.append(indices)\n",
    "\n",
    "# add padding to make every tweet the same length\n",
    "for i in range(len(X)):\n",
    "    tweet = X[i]\n",
    "    if len(tweet) < max_len:\n",
    "        tweet = np.append(tweet, np.ones(max_len - len(tweet)))\n",
    "    X[i] = tweet\n",
    "\n",
    "X = np.asarray(X, dtype=np.int64)\n",
    "y = np.array(list(map(lambda x: 1 if x > 0 else 0, df['Relevancy'].values)), dtype=np.int64)\n",
    "print(np.mean(unk_percent))\n",
    "print(len(unk_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\"\"\"\n",
    "This architecture is inspired by the one used in the paper\n",
    "'Twitter Sentiment Analysis with Deep Convolutional Neural Networks' (Severyn et al., 2015)\n",
    "\"\"\"\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, embeddings, n_filters, filter_sizes, n_classes, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #length of the word embeddings\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        \n",
    "        #architecture\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings)\n",
    "        \n",
    "        self.conv_0 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[0], embedding_dim))\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[1], embedding_dim))\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[2], embedding_dim))\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, n_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, tweet_indices):\n",
    "        \n",
    "        embedded = self.embedding(tweet_indices)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
    "        \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "        \n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim = 1))\n",
    "        \n",
    "        return self.softmax(self.fc(cat))\n",
    "    \n",
    "    def predict(self, tweet):\n",
    "        return np.argmax(self.forward(tweet).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X_train: 2d np array, where each row is the indices corresponding to each word of a specific tweet\n",
    "y_train: 1d np array of same length as X_train with 0/1 based on relevant/not relevant or urgent/not urgent\n",
    "embeddings: GloVe word embeddings created above\n",
    "\"\"\"\n",
    "def train_cnn_classifier(X_train, y_train, embeddings, num_classes, manual_params=None, verbose=False):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        embeddings = torch.from_numpy(embeddings).float().to(device)\n",
    "        embed_len = len(embeddings[0])\n",
    "        seq_len = len(X_train[0])\n",
    "        \n",
    "        #default parameters for the model\n",
    "        params = {'batch_size': 10, 'epochs': 50, 'lr': 0.0001, 'n_filters': 100, 'filter_sizes': [3,4,5],\n",
    "                 'dropout': 0.5}\n",
    "        \n",
    "        #replace default parameters with any user-defined ones\n",
    "        if manual_params is not None:\n",
    "            for p in manual_params:\n",
    "                params[p] = manual_params[p]\n",
    "                \n",
    "        batch_size = params['batch_size']\n",
    "        epochs = params['epochs']\n",
    "        lr = params['lr']\n",
    "        \n",
    "        #initialize network and optimizer\n",
    "        cnn = CNN(embeddings, n_filters=params['n_filters'], filter_sizes=params['filter_sizes'], \n",
    "                n_classes=num_classes, dropout=params['dropout'])\n",
    "        cnn.to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(cnn.parameters(), lr=lr)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        cnn.train()\n",
    "        for epoch in range(epochs):\n",
    "            ex_indices = [i for i in range(len(X_train))]\n",
    "            random.shuffle(ex_indices)\n",
    "            total_loss = 0.0\n",
    "            for idx in range(len(ex_indices)//batch_size):\n",
    "                \n",
    "                #create input batch to feed in\n",
    "                cur_batch_idx = ex_indices[idx*batch_size:(idx+1)*batch_size]\n",
    "                cur_X = torch.from_numpy(np.asarray([X_train[i] for i in cur_batch_idx])).long().to(device)\n",
    "                cur_y = torch.from_numpy(np.asarray([y_train[i] for i in cur_batch_idx])).to(device)\n",
    "                \n",
    "                #train\n",
    "                cnn.zero_grad()\n",
    "                probs = cnn.forward(cur_X)\n",
    "                \n",
    "                #calculate loss and update weights\n",
    "                cur_loss = loss(probs, cur_y)\n",
    "                total_loss += cur_loss\n",
    "                cur_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Avg loss on epoch %i: %f\" % (epoch+1, total_loss/len(ex_indices)))\n",
    "        end = time.time()\n",
    "        print(\"Time taken: %f seconds\" % (end-start))\n",
    "        return cnn\n",
    "    except KeyboardInterrupt:\n",
    "        end = time.time()\n",
    "        print(\"Time taken: %f seconds\" % (end-start))\n",
    "        return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates binary classification model\n",
    "def calc_metrics(model, X_test, y_test):\n",
    "    num_correct = 0\n",
    "    num_true_pos = 0\n",
    "    num_false_pos = 0\n",
    "    num_false_neg = 0\n",
    "    \n",
    "    num_test_exs = len(X_test)\n",
    "\n",
    "    model.eval()\n",
    "    for i in range(num_test_exs):\n",
    "        \n",
    "        cur_batch_idx = [i]\n",
    "        cur_X = torch.from_numpy(np.asarray([X_test[i] for i in cur_batch_idx])).long().to(device)\n",
    "        \n",
    "        y_pred = model.predict(cur_X)\n",
    "        y_gold = y_test[i]\n",
    "        if y_pred == y_gold:\n",
    "            num_correct += 1\n",
    "            if y_gold > 0:\n",
    "                num_true_pos += 1\n",
    "        else:\n",
    "            if y_pred == 0:\n",
    "                num_false_neg += 1\n",
    "            else:\n",
    "                num_false_pos += 1\n",
    "\n",
    "    accuracy = num_correct/num_test_exs\n",
    "    precision = num_true_pos/(num_true_pos + num_false_pos)\n",
    "    recall = num_true_pos/(num_true_pos + num_false_neg)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(X, y, embeddings, manual_params=None, k=10):\n",
    "    ex_indices = list(range(X.shape[0]))\n",
    "    random.shuffle(ex_indices)\n",
    "    \n",
    "    accuracy = np.zeros(k)\n",
    "    precision = np.zeros(k)\n",
    "    recall = np.zeros(k)\n",
    "    f1 = np.zeros(k)\n",
    "    \n",
    "    #calculate the splitting scheme\n",
    "    splits = [X.shape[0]//k] * k\n",
    "    for i in range(X.shape[0] % k):\n",
    "        splits[i] += 1\n",
    "    \n",
    "    #keeps track of current location in \n",
    "    index = 0\n",
    "    for i in range(k):\n",
    "        #come up with the train-test split\n",
    "        X_test = np.asarray([X[i] for i in ex_indices[index:index+splits[i]]])\n",
    "        y_test = np.asarray([y[i] for i in ex_indices[index:index+splits[i]]])\n",
    "        \n",
    "        train_indices = ex_indices[0:index] + ex_indices[index+splits[i]:]\n",
    "        X_train = np.asarray([X[i] for i in train_indices])\n",
    "        y_train = np.asarray([y[i] for i in train_indices])\n",
    "        \n",
    "        #now train the model on this split and save the metrics\n",
    "        cnn = train_cnn_classifier(X_train, y_train, embeddings, num_classes=2, manual_params=manual_params, verbose=False)\n",
    "        \n",
    "        results = calc_metrics(cnn, X_test, y_test)\n",
    "        accuracy[i] = results['accuracy']\n",
    "        precision[i] = results['precision']\n",
    "        recall[i] = results['recall']\n",
    "        f1[i] = results['f1']\n",
    "        \n",
    "        index += splits[i]\n",
    "    \n",
    "    return {'accuracy': np.mean(accuracy), 'precision': np.mean(precision), \n",
    "           'recall': np.mean(recall), 'f1': np.mean(f1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(X, y, embeddings, params, metric='f1', k=10):\n",
    "    \n",
    "    results = []\n",
    "    keys = []\n",
    "    values = []\n",
    "    for key in params:\n",
    "        keys.append(key)\n",
    "        values.append(params[key])\n",
    "    \n",
    "    for config in product(*values):\n",
    "        p = {}\n",
    "        for i, v in enumerate(config):\n",
    "            p[keys[i]] = v\n",
    "        \n",
    "        res = kfold(X, y, embeddings, manual_params=p, k=k)\n",
    "        results.append((p, res))\n",
    "    \n",
    "    return sorted(results, reverse=True, key=lambda x: x[1][metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold(X, y, embeddings, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#figure out region size first\n",
    "filter_sizes = []\n",
    "for i in range(10):\n",
    "    filter_sizes.append([i+1])\n",
    "filter_sizes.append([15])\n",
    "filter_sizes_search = gridsearch(X, y, embeddings, params={'filter_sizes': filter_sizes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now try multiple filters with sizes around opt\n",
    "fs_opt = filter_sizes_search[0][0]['filter_sizes'][0]\n",
    "print('optimal filter size: ' + str(fs_opt))\n",
    "\n",
    "filter_sizes2 = [[fs_opt], [fs_opt]*2, [fs_opt]*3, [fs_opt]*4]\n",
    "\n",
    "if fs_opt > 3:\n",
    "    filter_sizes2.append([fs_opt-3, fs_opt-2, fs_opt-1])\n",
    "if fs_opt > 2:\n",
    "    filter_sizes2.append([fs_opt-2, fs_opt-1, fs_opt])\n",
    "    filter_sizes2.append([fs_opt-2, fs_opt-1, fs_opt, fs_opt+1])\n",
    "    filter_sizes2.append([fs_opt-2, fs_opt-1, fs_opt, fs_opt+1, fs_opt+2])\n",
    "if fs_opt > 1:\n",
    "    filter_sizes2.append([fs_opt-1, fs_opt, fs_opt+1])\n",
    "    filter_sizes2.append([fs_opt-1, fs_opt-1, fs_opt, fs_opt])\n",
    "\n",
    "filter_sizes2.append([fs_opt, fs_opt+1, fs_opt+2])\n",
    "filter_sizes2.append([fs_opt, fs_opt, fs_opt+1, fs_opt+1])\n",
    "filter_sizes2.append([fs_opt, fs_opt+1, fs_opt+2, fs_opt+3])\n",
    "\n",
    "filter_sizes_search2 = gridsearch(X, y, embeddings, params={'filter_sizes': filter_sizes2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes_search2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_opt2 = filter_sizes_search2[0][0]['filter_sizes']\n",
    "print('optimal filter size 2: ' + str(fs_opt2))\n",
    "\n",
    "#now adjust the number of feature maps for each filter size to find best one\n",
    "n_filters = [100,200,300,400,500,600,700,1000]\n",
    "n_filters_search = gridsearch(X, y, embeddings, params={'dropout': [0.1], 'n_filters': n_filters, 'filter_sizes': [fs_opt2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal n_filters: 100\n",
      "Time taken: 55.411257 seconds\n",
      "Time taken: 55.921366 seconds\n",
      "Time taken: 56.436141 seconds\n",
      "Time taken: 55.501133 seconds\n",
      "Time taken: 54.791711 seconds\n",
      "Time taken: 55.152720 seconds\n",
      "Time taken: 55.614873 seconds\n",
      "Time taken: 55.211932 seconds\n",
      "Time taken: 54.935635 seconds\n",
      "Time taken: 54.456240 seconds\n",
      "Time taken: 54.977714 seconds\n",
      "Time taken: 55.548565 seconds\n",
      "Time taken: 54.816603 seconds\n",
      "Time taken: 56.436665 seconds\n",
      "Time taken: 55.529351 seconds\n",
      "Time taken: 55.977760 seconds\n",
      "Time taken: 55.363546 seconds\n",
      "Time taken: 54.436212 seconds\n",
      "Time taken: 55.826600 seconds\n",
      "Time taken: 55.520164 seconds\n",
      "Time taken: 55.533190 seconds\n",
      "Time taken: 54.887025 seconds\n",
      "Time taken: 54.544723 seconds\n",
      "Time taken: 56.047434 seconds\n",
      "Time taken: 54.831809 seconds\n",
      "Time taken: 55.841985 seconds\n",
      "Time taken: 55.745844 seconds\n",
      "Time taken: 55.250174 seconds\n",
      "Time taken: 55.585490 seconds\n",
      "Time taken: 55.630190 seconds\n",
      "Time taken: 56.433008 seconds\n",
      "Time taken: 55.620598 seconds\n",
      "Time taken: 54.855819 seconds\n",
      "Time taken: 54.666297 seconds\n",
      "Time taken: 56.064749 seconds\n",
      "Time taken: 55.186435 seconds\n",
      "Time taken: 55.576933 seconds\n",
      "Time taken: 56.076306 seconds\n",
      "Time taken: 55.374448 seconds\n",
      "Time taken: 56.407100 seconds\n",
      "Time taken: 57.950806 seconds\n",
      "Time taken: 57.614532 seconds\n",
      "Time taken: 57.497723 seconds\n",
      "Time taken: 58.134599 seconds\n",
      "Time taken: 57.641150 seconds\n",
      "Time taken: 57.761123 seconds\n",
      "Time taken: 57.795880 seconds\n",
      "Time taken: 58.052524 seconds\n",
      "Time taken: 57.694355 seconds\n",
      "Time taken: 57.910292 seconds\n",
      "Time taken: 79.634079 seconds\n",
      "Time taken: 79.595457 seconds\n",
      "Time taken: 79.578340 seconds\n",
      "Time taken: 79.559984 seconds\n",
      "Time taken: 79.733543 seconds\n",
      "Time taken: 79.822765 seconds\n",
      "Time taken: 79.446170 seconds\n",
      "Time taken: 79.512687 seconds\n",
      "Time taken: 79.517863 seconds\n",
      "Time taken: 79.467644 seconds\n",
      "Time taken: 55.406855 seconds\n",
      "Time taken: 55.397540 seconds\n",
      "Time taken: 55.352440 seconds\n",
      "Time taken: 55.472473 seconds\n",
      "Time taken: 54.451161 seconds\n",
      "Time taken: 55.002197 seconds\n",
      "Time taken: 55.613487 seconds\n",
      "Time taken: 55.826983 seconds\n",
      "Time taken: 56.110657 seconds\n",
      "Time taken: 55.342629 seconds\n",
      "Time taken: 55.483684 seconds\n",
      "Time taken: 55.716345 seconds\n",
      "Time taken: 55.572984 seconds\n",
      "Time taken: 55.506132 seconds\n",
      "Time taken: 57.258037 seconds\n",
      "Time taken: 55.706941 seconds\n",
      "Time taken: 56.125756 seconds\n",
      "Time taken: 55.732845 seconds\n",
      "Time taken: 55.662679 seconds\n",
      "Time taken: 55.862189 seconds\n",
      "Time taken: 55.502383 seconds\n",
      "Time taken: 55.508037 seconds\n",
      "Time taken: 55.640990 seconds\n",
      "Time taken: 55.548977 seconds\n",
      "Time taken: 55.403188 seconds\n",
      "Time taken: 57.030311 seconds\n",
      "Time taken: 54.990860 seconds\n",
      "Time taken: 54.583666 seconds\n",
      "Time taken: 56.190645 seconds\n",
      "Time taken: 55.339170 seconds\n",
      "Time taken: 54.483714 seconds\n",
      "Time taken: 55.055957 seconds\n",
      "Time taken: 54.906499 seconds\n",
      "Time taken: 55.869340 seconds\n",
      "Time taken: 55.653997 seconds\n",
      "Time taken: 55.163760 seconds\n",
      "Time taken: 55.074644 seconds\n",
      "Time taken: 55.644070 seconds\n",
      "Time taken: 55.251176 seconds\n",
      "Time taken: 55.068527 seconds\n",
      "Time taken: 58.362291 seconds\n",
      "Time taken: 57.817432 seconds\n",
      "Time taken: 58.483392 seconds\n",
      "Time taken: 58.527086 seconds\n",
      "Time taken: 57.601547 seconds\n",
      "Time taken: 57.983504 seconds\n",
      "Time taken: 57.848775 seconds\n",
      "Time taken: 58.079023 seconds\n",
      "Time taken: 58.663011 seconds\n",
      "Time taken: 58.258686 seconds\n",
      "Time taken: 79.702682 seconds\n",
      "Time taken: 79.650153 seconds\n",
      "Time taken: 79.520547 seconds\n",
      "Time taken: 79.621208 seconds\n",
      "Time taken: 79.456187 seconds\n",
      "Time taken: 79.585750 seconds\n",
      "Time taken: 79.567444 seconds\n",
      "Time taken: 79.489970 seconds\n",
      "Time taken: 79.491513 seconds\n",
      "Time taken: 79.557181 seconds\n"
     ]
    }
   ],
   "source": [
    "n_filters_opt = 100\n",
    "dropout_opt = 0.1\n",
    "print('optimal n_filters: ' + str(n_filters_opt))\n",
    "\n",
    "# we need to increase the dropout and try again with higher values\n",
    "n_filters2 = [400,500,600,700,1000,2000]\n",
    "n_filters_search2 = gridsearch(X, y, embeddings, params={'dropout': [0.5,0.75], 'n_filters': n_filters2, 'filter_sizes': [[1,1,1]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'dropout': 0.75, 'n_filters': 400, 'filter_sizes': [1, 1, 1]},\n",
       "  {'accuracy': 0.7170291949703714,\n",
       "   'precision': 0.7108499987233488,\n",
       "   'recall': 0.7333624974603714,\n",
       "   'f1': 0.7212108573859786}),\n",
       " ({'dropout': 0.75, 'n_filters': 500, 'filter_sizes': [1, 1, 1]},\n",
       "  {'accuracy': 0.7177458206869971,\n",
       "   'precision': 0.7168908465633443,\n",
       "   'recall': 0.7207278323401358,\n",
       "   'f1': 0.7179898960524425}),\n",
       " ({'dropout': 0.5, 'n_filters': 400, 'filter_sizes': [1, 1, 1]},\n",
       "  {'accuracy': 0.7128667437490966,\n",
       "   'precision': 0.7055637933851523,\n",
       "   'recall': 0.7293383015203121,\n",
       "   'f1': 0.7169432988044626}),\n",
       " ({'dropout': 0.75, 'n_filters': 1000, 'filter_sizes': [1, 1, 1]},\n",
       "  {'accuracy': 0.7145601483836778,\n",
       "   'precision': 0.7130406361017637,\n",
       "   'recall': 0.7211848764500807,\n",
       "   'f1': 0.7154566986638359}),\n",
       " ({'dropout': 0.75, 'n_filters': 600, 'filter_sizes': [1, 1, 1]},\n",
       "  {'accuracy': 0.7091697981403864,\n",
       "   'precision': 0.7010760122269847,\n",
       "   'recall': 0.7318993496138287,\n",
       "   'f1': 0.7150301538000413}),\n",
       " ({'dropout': 0.75, 'n_filters': 700, 'filter_sizes': [1, 1, 1]},\n",
       "  {'accuracy': 0.7133479067302597,\n",
       "   'precision': 0.7146303598598907,\n",
       "   'recall': 0.7108689921950069,\n",
       "   'f1': 0.7116313386528292}),\n",
       " ({'dropout': 0.5, 'n_filters': 600, 'filter_sizes': [1, 1, 1]},\n",
       "  {'accuracy': 0.7052494339259046,\n",
       "   'precision': 0.6969281292026123,\n",
       "   'recall': 0.7292999214516263,\n",
       "   'f1': 0.7115516522882601}),\n",
       " ({'dropout': 0.5, 'n_filters': 500, 'filter_sizes': [1, 1, 1]},\n",
       "  {'accuracy': 0.7076937900467313,\n",
       "   'precision': 0.7066785586421271,\n",
       "   'recall': 0.7170186462215405,\n",
       "   'f1': 0.7099221460137117}),\n",
       " ({'dropout': 0.75, 'n_filters': 2000, 'filter_sizes': [1, 1, 1]},\n",
       "  {'accuracy': 0.7069705400587755,\n",
       "   'precision': 0.7101332570076643,\n",
       "   'recall': 0.6965665730912763,\n",
       "   'f1': 0.7020567295273763}),\n",
       " ({'dropout': 0.5, 'n_filters': 700, 'filter_sizes': [1, 1, 1]},\n",
       "  {'accuracy': 0.6993586500939442,\n",
       "   'precision': 0.698757502737626,\n",
       "   'recall': 0.7021359354327844,\n",
       "   'f1': 0.7000158366641178}),\n",
       " ({'dropout': 0.5, 'n_filters': 1000, 'filter_sizes': [1, 1, 1]},\n",
       "  {'accuracy': 0.69788143758732,\n",
       "   'precision': 0.6968653141862784,\n",
       "   'recall': 0.7004199588989856,\n",
       "   'f1': 0.6974098473966307}),\n",
       " ({'dropout': 0.5, 'n_filters': 2000, 'filter_sizes': [1, 1, 1]},\n",
       "  {'accuracy': 0.6915082863612275,\n",
       "   'precision': 0.694685469229819,\n",
       "   'recall': 0.6852066956465658,\n",
       "   'f1': 0.6883661113572231})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_filters_search2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated optimal n_filters: 400\n",
      "updated optimal dropout: 0.75\n"
     ]
    }
   ],
   "source": [
    "n_filters_opt2 = n_filters_search2[0][0]['n_filters']\n",
    "dropout_opt = 0.75\n",
    "\n",
    "print('updated optimal n_filters: ' + str(n_filters_opt2))\n",
    "print('updated optimal dropout: ' + str(dropout_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 11.475208 seconds\n",
      "Time taken: 11.781382 seconds\n",
      "Time taken: 11.755190 seconds\n",
      "Time taken: 11.277146 seconds\n",
      "Time taken: 11.619232 seconds\n",
      "Time taken: 11.378665 seconds\n",
      "Time taken: 11.339352 seconds\n",
      "Time taken: 11.338720 seconds\n",
      "Time taken: 11.046167 seconds\n",
      "Time taken: 11.712664 seconds\n",
      "Time taken: 33.269053 seconds\n",
      "Time taken: 33.276424 seconds\n",
      "Time taken: 33.248067 seconds\n",
      "Time taken: 33.228979 seconds\n",
      "Time taken: 33.229292 seconds\n",
      "Time taken: 33.305548 seconds\n",
      "Time taken: 33.217199 seconds\n",
      "Time taken: 33.205963 seconds\n",
      "Time taken: 33.191382 seconds\n",
      "Time taken: 33.217854 seconds\n",
      "Time taken: 3.708146 seconds\n",
      "Time taken: 3.716348 seconds\n",
      "Time taken: 3.714643 seconds\n",
      "Time taken: 3.696690 seconds\n",
      "Time taken: 3.692441 seconds\n",
      "Time taken: 3.719661 seconds\n",
      "Time taken: 3.705287 seconds\n",
      "Time taken: 3.708778 seconds\n",
      "Time taken: 3.686179 seconds\n",
      "Time taken: 3.683019 seconds\n",
      "Time taken: 3.127100 seconds\n",
      "Time taken: 3.102632 seconds\n",
      "Time taken: 3.104588 seconds\n",
      "Time taken: 3.146895 seconds\n",
      "Time taken: 3.102578 seconds\n",
      "Time taken: 3.107368 seconds\n",
      "Time taken: 3.112959 seconds\n",
      "Time taken: 3.099940 seconds\n",
      "Time taken: 3.081101 seconds\n",
      "Time taken: 3.106751 seconds\n",
      "Time taken: 29.864503 seconds\n",
      "Time taken: 29.870928 seconds\n",
      "Time taken: 29.861076 seconds\n",
      "Time taken: 29.859419 seconds\n",
      "Time taken: 29.859961 seconds\n",
      "Time taken: 29.871269 seconds\n",
      "Time taken: 29.878788 seconds\n",
      "Time taken: 29.867708 seconds\n",
      "Time taken: 29.911208 seconds\n",
      "Time taken: 29.872760 seconds\n",
      "Time taken: 55.337280 seconds\n",
      "Time taken: 55.461710 seconds\n",
      "Time taken: 55.527759 seconds\n",
      "Time taken: 55.359579 seconds\n",
      "Time taken: 56.122708 seconds\n",
      "Time taken: 54.964244 seconds\n",
      "Time taken: 55.751908 seconds\n",
      "Time taken: 57.812268 seconds\n",
      "Time taken: 59.255322 seconds\n",
      "Time taken: 59.461113 seconds\n",
      "Time taken: 163.329097 seconds\n",
      "Time taken: 163.279939 seconds\n",
      "Time taken: 163.269587 seconds\n",
      "Time taken: 163.264822 seconds\n",
      "Time taken: 163.337264 seconds\n",
      "Time taken: 163.300928 seconds\n",
      "Time taken: 163.381022 seconds\n",
      "Time taken: 163.303527 seconds\n",
      "Time taken: 163.231719 seconds\n",
      "Time taken: 163.240088 seconds\n",
      "Time taken: 16.168922 seconds\n",
      "Time taken: 16.129674 seconds\n",
      "Time taken: 16.136411 seconds\n",
      "Time taken: 16.118692 seconds\n",
      "Time taken: 16.104203 seconds\n",
      "Time taken: 16.184937 seconds\n",
      "Time taken: 16.066102 seconds\n",
      "Time taken: 16.115465 seconds\n",
      "Time taken: 16.154720 seconds\n",
      "Time taken: 16.123343 seconds\n",
      "Time taken: 13.164491 seconds\n",
      "Time taken: 13.152699 seconds\n",
      "Time taken: 13.169471 seconds\n",
      "Time taken: 13.154505 seconds\n",
      "Time taken: 13.182092 seconds\n",
      "Time taken: 13.149151 seconds\n",
      "Time taken: 13.149180 seconds\n",
      "Time taken: 13.145009 seconds\n",
      "Time taken: 13.141421 seconds\n",
      "Time taken: 13.174434 seconds\n",
      "Time taken: 147.596186 seconds\n",
      "Time taken: 147.561803 seconds\n",
      "Time taken: 147.579646 seconds\n",
      "Time taken: 147.553512 seconds\n",
      "Time taken: 147.525401 seconds\n",
      "Time taken: 147.507913 seconds\n",
      "Time taken: 147.537247 seconds\n",
      "Time taken: 147.569160 seconds\n",
      "Time taken: 147.547229 seconds\n",
      "Time taken: 147.567272 seconds\n",
      "Time taken: 109.537037 seconds\n",
      "Time taken: 109.588945 seconds\n",
      "Time taken: 108.379744 seconds\n",
      "Time taken: 108.186064 seconds\n",
      "Time taken: 107.706823 seconds\n",
      "Time taken: 109.581261 seconds\n",
      "Time taken: 109.794198 seconds\n",
      "Time taken: 107.929138 seconds\n",
      "Time taken: 109.138764 seconds\n",
      "Time taken: 109.990821 seconds\n",
      "Time taken: 326.152093 seconds\n",
      "Time taken: 326.132389 seconds\n",
      "Time taken: 326.120566 seconds\n",
      "Time taken: 326.145606 seconds\n",
      "Time taken: 326.079772 seconds\n",
      "Time taken: 326.066467 seconds\n",
      "Time taken: 326.148912 seconds\n",
      "Time taken: 326.023972 seconds\n",
      "Time taken: 326.030509 seconds\n",
      "Time taken: 325.969959 seconds\n",
      "Time taken: 31.665766 seconds\n",
      "Time taken: 31.701245 seconds\n",
      "Time taken: 31.627590 seconds\n",
      "Time taken: 31.768256 seconds\n",
      "Time taken: 31.617422 seconds\n",
      "Time taken: 31.621947 seconds\n",
      "Time taken: 31.635532 seconds\n",
      "Time taken: 31.616420 seconds\n",
      "Time taken: 31.617395 seconds\n",
      "Time taken: 31.619323 seconds\n",
      "Time taken: 25.801488 seconds\n",
      "Time taken: 25.723358 seconds\n",
      "Time taken: 25.726976 seconds\n",
      "Time taken: 25.748514 seconds\n",
      "Time taken: 25.783277 seconds\n",
      "Time taken: 25.762396 seconds\n",
      "Time taken: 25.747612 seconds\n",
      "Time taken: 25.735001 seconds\n",
      "Time taken: 25.743303 seconds\n",
      "Time taken: 25.745729 seconds\n",
      "Time taken: 294.836278 seconds\n",
      "Time taken: 294.903476 seconds\n",
      "Time taken: 294.700660 seconds\n",
      "Time taken: 294.697294 seconds\n",
      "Time taken: 294.725523 seconds\n",
      "Time taken: 294.701572 seconds\n",
      "Time taken: 294.767209 seconds\n",
      "Time taken: 294.789365 seconds\n",
      "Time taken: 294.695430 seconds\n",
      "Time taken: 294.807093 seconds\n",
      "Time taken: 216.102114 seconds\n",
      "Time taken: 218.666062 seconds\n",
      "Time taken: 217.972444 seconds\n",
      "Time taken: 215.759949 seconds\n",
      "Time taken: 215.214271 seconds\n",
      "Time taken: 214.367890 seconds\n",
      "Time taken: 217.730408 seconds\n",
      "Time taken: 216.737606 seconds\n",
      "Time taken: 215.527743 seconds\n",
      "Time taken: 216.640145 seconds\n",
      "Time taken: 651.734003 seconds\n",
      "Time taken: 651.629232 seconds\n",
      "Time taken: 651.696996 seconds\n",
      "Time taken: 651.654649 seconds\n",
      "Time taken: 651.584598 seconds\n",
      "Time taken: 651.435167 seconds\n",
      "Time taken: 651.580756 seconds\n",
      "Time taken: 651.313034 seconds\n",
      "Time taken: 651.258109 seconds\n",
      "Time taken: 651.375390 seconds\n",
      "Time taken: 62.928214 seconds\n",
      "Time taken: 62.790599 seconds\n",
      "Time taken: 62.777010 seconds\n",
      "Time taken: 62.824079 seconds\n",
      "Time taken: 62.879500 seconds\n",
      "Time taken: 62.802231 seconds\n",
      "Time taken: 62.811238 seconds\n",
      "Time taken: 62.787738 seconds\n",
      "Time taken: 62.872348 seconds\n",
      "Time taken: 62.801322 seconds\n",
      "Time taken: 51.022465 seconds\n",
      "Time taken: 50.917449 seconds\n",
      "Time taken: 50.904762 seconds\n",
      "Time taken: 50.958330 seconds\n",
      "Time taken: 50.901186 seconds\n",
      "Time taken: 50.953512 seconds\n",
      "Time taken: 50.964657 seconds\n",
      "Time taken: 50.908708 seconds\n",
      "Time taken: 50.891512 seconds\n",
      "Time taken: 50.881987 seconds\n",
      "Time taken: 589.331449 seconds\n",
      "Time taken: 589.310102 seconds\n",
      "Time taken: 589.130347 seconds\n",
      "Time taken: 589.318621 seconds\n",
      "Time taken: 589.193553 seconds\n",
      "Time taken: 589.267398 seconds\n",
      "Time taken: 589.257759 seconds\n",
      "Time taken: 589.293602 seconds\n",
      "Time taken: 589.226247 seconds\n",
      "Time taken: 589.250074 seconds\n"
     ]
    }
   ],
   "source": [
    "#now fine-tune epochs and batch size\n",
    "epochs = [10,50,100,200]\n",
    "batch_size = [10,25,50,100,200]\n",
    "\n",
    "final_search = gridsearch(X, y, embeddings, params={'dropout': [dropout_opt], \n",
    "                                                    'n_filters': [n_filters_opt2], \n",
    "                                                    'filter_sizes': [[1,1,1]],\n",
    "                                                    'epochs': epochs,\n",
    "                                                    'batch_size': batch_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 200,\n",
       "   'batch_size': 50},\n",
       "  {'accuracy': 0.7165305680011562,\n",
       "   'precision': 0.7105332920476017,\n",
       "   'recall': 0.7316548997856122,\n",
       "   'f1': 0.7203522038738812}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 50,\n",
       "   'batch_size': 10},\n",
       "  {'accuracy': 0.7194723466782291,\n",
       "   'precision': 0.7210557020517993,\n",
       "   'recall': 0.7158398179089841,\n",
       "   'f1': 0.7178588187252875}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 50,\n",
       "   'batch_size': 25},\n",
       "  {'accuracy': 0.715291227055933,\n",
       "   'precision': 0.7128450501673116,\n",
       "   'recall': 0.7220024533730067,\n",
       "   'f1': 0.7163535968996311}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 200,\n",
       "   'batch_size': 200},\n",
       "  {'accuracy': 0.7155477670183552,\n",
       "   'precision': 0.7133871406861227,\n",
       "   'recall': 0.7188571738935536,\n",
       "   'f1': 0.7157658659770727}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 200,\n",
       "   'batch_size': 100},\n",
       "  {'accuracy': 0.7135857782916606,\n",
       "   'precision': 0.7116516613326174,\n",
       "   'recall': 0.7182872181078729,\n",
       "   'f1': 0.7145429123519963}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 100,\n",
       "   'batch_size': 200},\n",
       "  {'accuracy': 0.7094209182444476,\n",
       "   'precision': 0.704894203814052,\n",
       "   'recall': 0.7214265234199158,\n",
       "   'f1': 0.7125738966182402}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 100,\n",
       "   'batch_size': 10},\n",
       "  {'accuracy': 0.709175820205232,\n",
       "   'precision': 0.7013526646016802,\n",
       "   'recall': 0.7254201141844859,\n",
       "   'f1': 0.712543223475828}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 100,\n",
       "   'batch_size': 100},\n",
       "  {'accuracy': 0.7096575853928795,\n",
       "   'precision': 0.7069231737161961,\n",
       "   'recall': 0.7163670597599141,\n",
       "   'f1': 0.7113440294834}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 100,\n",
       "   'batch_size': 25},\n",
       "  {'accuracy': 0.7089247001011707,\n",
       "   'precision': 0.7058225949133256,\n",
       "   'recall': 0.7197168630491453,\n",
       "   'f1': 0.7105436816275315}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 100,\n",
       "   'batch_size': 50},\n",
       "  {'accuracy': 0.7074583273112685,\n",
       "   'precision': 0.7040866567517527,\n",
       "   'recall': 0.719392873164898,\n",
       "   'f1': 0.7101643896264663}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 50,\n",
       "   'batch_size': 50},\n",
       "  {'accuracy': 0.7074673604085369,\n",
       "   'precision': 0.7048152808959031,\n",
       "   'recall': 0.716530546026071,\n",
       "   'f1': 0.7096839327977815}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 50,\n",
       "   'batch_size': 100},\n",
       "  {'accuracy': 0.7059768993592523,\n",
       "   'precision': 0.7028506495034607,\n",
       "   'recall': 0.7144765933917658,\n",
       "   'f1': 0.7078679980892787}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 50,\n",
       "   'batch_size': 200},\n",
       "  {'accuracy': 0.7035500072264778,\n",
       "   'precision': 0.6995382951749871,\n",
       "   'recall': 0.7170132758625443,\n",
       "   'f1': 0.7072440510438198}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 200,\n",
       "   'batch_size': 25},\n",
       "  {'accuracy': 0.7052428096545744,\n",
       "   'precision': 0.7034474117589258,\n",
       "   'recall': 0.7088104794786589,\n",
       "   'f1': 0.7057650808499419}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 200,\n",
       "   'batch_size': 10},\n",
       "  {'accuracy': 0.7040125018066193,\n",
       "   'precision': 0.7035572075768818,\n",
       "   'recall': 0.7057500250954649,\n",
       "   'f1': 0.704191501920357}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 10,\n",
       "   'batch_size': 10},\n",
       "  {'accuracy': 0.6966728091728092,\n",
       "   'precision': 0.6987193554655213,\n",
       "   'recall': 0.6925207791091561,\n",
       "   'f1': 0.6947860044937026}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 10,\n",
       "   'batch_size': 25},\n",
       "  {'accuracy': 0.6880817314640845,\n",
       "   'precision': 0.6889763464035621,\n",
       "   'recall': 0.6936155198986632,\n",
       "   'f1': 0.6893492943802546}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 10,\n",
       "   'batch_size': 50},\n",
       "  {'accuracy': 0.6817067736185384,\n",
       "   'precision': 0.6725439105707514,\n",
       "   'recall': 0.7053284786703535,\n",
       "   'f1': 0.6879116262194973}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 10,\n",
       "   'batch_size': 100},\n",
       "  {'accuracy': 0.6731162981162981,\n",
       "   'precision': 0.6745834805194166,\n",
       "   'recall': 0.669662428563808,\n",
       "   'f1': 0.671361250894577}),\n",
       " ({'dropout': 0.75,\n",
       "   'n_filters': 400,\n",
       "   'filter_sizes': [1, 1, 1],\n",
       "   'epochs': 10,\n",
       "   'batch_size': 200},\n",
       "  {'accuracy': 0.6542461579226285,\n",
       "   'precision': 0.642419447276509,\n",
       "   'recall': 0.7004240426303083,\n",
       "   'f1': 0.669199524256265})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
