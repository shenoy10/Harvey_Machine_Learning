{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ashwin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ashwin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "\n",
    "\n",
    "#create spell checker/word splitter\n",
    "def create_symspell(max_edit_distance, prefix_length, freq_file_path):\n",
    "    # create object\n",
    "    sym_spell = SymSpell(max_edit_distance, prefix_length)\n",
    "    \n",
    "    # create dictionary using corpus.txt\n",
    "    if not sym_spell.create_dictionary(freq_file_path):\n",
    "        print(\"Corpus file not found\")\n",
    "        return None\n",
    "    return sym_spell\n",
    "\n",
    "def is_valid_token(w):\n",
    "    special = ['<url>','<number>', '<user>']\n",
    "    return w.isalpha() or w in special\n",
    "\n",
    "def process_tweet(tweet, tknzr, sym_spell=None, advanced=False):\n",
    "    st_1 = []\n",
    "    for w in tknzr.tokenize(tweet):\n",
    "        #remove retweet annotation if present:\n",
    "        if w == 'RT':\n",
    "            if advanced:\n",
    "                st_1.append('rt')\n",
    "        elif w[0] == '@':\n",
    "            if advanced:\n",
    "                st_1.append('<user>')\n",
    "        #remove hashtag symbol\n",
    "        elif w[0] == '#':\n",
    "            st_1.append(w[1:])\n",
    "        #replace link with LINK keyword\n",
    "        elif w[:4] == 'http':\n",
    "            st_1.append('<url>')\n",
    "        elif w.isnumeric():\n",
    "            if advanced:\n",
    "                st_1.append('<number>')\n",
    "        else:\n",
    "            st_1.append(w)\n",
    "    \n",
    "    st_2 = []\n",
    "    \n",
    "    #remove stop words and punctuation, make everything lowercase\n",
    "    if sym_spell != None:\n",
    "        st_2 = [sym_spell.word_segmentation(w.lower()).corrected_string \n",
    "                for w in st_1 if w.isalpha() and not w.lower() in stop_words]\n",
    "    elif advanced:\n",
    "        st_2 = [w.lower() for w in st_1 if is_valid_token(w) and \n",
    "                    not w.lower() in stop_words]\n",
    "    else:\n",
    "        st_2 = [w.lower() for w in st_1 if w.isalpha() and\n",
    "                not w.lower() in stop_words]\n",
    "    \n",
    "    #lemmatization (converts all words to root form for standardization)\n",
    "    lem = WordNetLemmatizer()\n",
    "    st_3 = list(map(lambda x: lem.lemmatize(x, pos='v'), st_2))\n",
    "    \n",
    "    #now do word segmentation/spell check\n",
    "    return ' '.join(st_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4003, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>Urgency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n#Harvey a marathon not a sprint | Severe th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>your rescue boats, vehicles, volunteer craft ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>! @Billcassidy on adding short-term debt limit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!!  According to #PBS, #Houston convention cen...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"#FEMA flood maps outdated do not reflect incr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Relevancy  Urgency\n",
       "0   \\n#Harvey a marathon not a sprint | Severe th...          0        0\n",
       "1   your rescue boats, vehicles, volunteer craft ...          0        0\n",
       "2  ! @Billcassidy on adding short-term debt limit...          0        0\n",
       "3  !!  According to #PBS, #Houston convention cen...          0        0\n",
       "4  \"#FEMA flood maps outdated do not reflect incr...          0        0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/labeled_prelim.csv').dropna().astype(\n",
    "        {'Relevancy':np.int32, 'Urgency':np.int32}).reset_index(drop=True)\n",
    "df.pop('Id')\n",
    "\n",
    "# df2 = pd.read_csv('data/mturk_results_0-2000_processed.csv')\n",
    "#df = pd.concat([df, df2], ignore_index=True)\n",
    "df2 = pd.read_csv('data/mturk_0-4000_manual2.csv')\n",
    "df = df2\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>Urgency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>harvey marathon sprint severe threat remain bo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rescue boat vehicles volunteer craft need txwx...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;user&gt; add debt limit increase disaster aid bi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accord pbs houston convention center need whee...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fema flood map outdated reflect increase threa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Relevancy  Urgency\n",
       "0  harvey marathon sprint severe threat remain bo...          0        0\n",
       "1  rescue boat vehicles volunteer craft need txwx...          0        0\n",
       "2  <user> add debt limit increase disaster aid bi...          0        0\n",
       "3  accord pbs houston convention center need whee...          0        0\n",
       "4  fema flood map outdated reflect increase threa...          0        0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sym_spell = create_symspell(2,7,'data/frequency_dictionary_en_82_765.txt')\n",
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "df['Text'] = df['Text'].map(lambda x: process_tweet(x, tknzr, None, True))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<user> hello world'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_tweet('@user #hello world', tknzr, None,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty line\n"
     ]
    }
   ],
   "source": [
    "#list of embeddings\n",
    "vec_length = 100\n",
    "embeddings = np.zeros((1193514, vec_length))\n",
    "\n",
    "#two-way map, index->word and word->index\n",
    "glove = {}\n",
    "\n",
    "index = 0\n",
    "with open('data/glove.twitter.27B/glove.twitter.27B.%dd.txt' % vec_length) as f:\n",
    "    for l in f:\n",
    "        line = []\n",
    "        try:\n",
    "            line = l.split()\n",
    "            if len(line) != vec_length+1:\n",
    "                print('empty line')\n",
    "                continue\n",
    "            \n",
    "            word = line[0]\n",
    "            embeddings[index] = np.array(line[1:]).astype(np.float)\n",
    "            glove[index] = word\n",
    "            glove[word] = index\n",
    "            index += 1\n",
    "        except:\n",
    "            print(line)\n",
    "            print(index)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only handles binary classification for now\n",
    "def tweets_to_df(df, labels, embeddings, glove):\n",
    "    \n",
    "    weights = []\n",
    "    index_omit = []\n",
    "    index = -1\n",
    "    tweets = df['Text']\n",
    "    \n",
    "    #a column for each entry in the embedding vector\n",
    "    for i in range(vec_length+1):\n",
    "        weights.append([])\n",
    "    \n",
    "    for i in range(len(tweets)):\n",
    "        index += 1\n",
    "        cur_embed = []\n",
    "        cur_tweet = tweets[i]\n",
    "        cur_label = labels[i]\n",
    "        for i in cur_tweet.split():\n",
    "            if i in glove:\n",
    "                cur_embed.append(embeddings[glove[i]])\n",
    "        \n",
    "        if len(cur_embed) == 0:\n",
    "            #make sure we drop this row from the input dataframe\n",
    "            index_omit.append(index)\n",
    "            continue\n",
    "        \n",
    "        x = np.asarray(np.mean(cur_embed, axis=0))\n",
    "        \n",
    "        for j in range(vec_length):\n",
    "            weights[j].append(x[j])\n",
    "        weights[vec_length].append(0 if cur_label == 0 else 1)\n",
    "        #weights[vec_length].append(cur_label)\n",
    "        \n",
    "    df_pruned = df.drop(index_omit)\n",
    "    \n",
    "    #convert to dataframe\n",
    "    cols = {}\n",
    "    for i in range(vec_length):\n",
    "       cols['v' + str(i)] = weights[i]\n",
    "    \n",
    "    cols['class'] = weights[vec_length]\n",
    "    \n",
    "    df2 = pd.DataFrame(data=cols)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v90</th>\n",
       "      <th>v91</th>\n",
       "      <th>v92</th>\n",
       "      <th>v93</th>\n",
       "      <th>v94</th>\n",
       "      <th>v95</th>\n",
       "      <th>v96</th>\n",
       "      <th>v97</th>\n",
       "      <th>v98</th>\n",
       "      <th>v99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.221984</td>\n",
       "      <td>0.107015</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>-0.389919</td>\n",
       "      <td>0.016036</td>\n",
       "      <td>-0.082146</td>\n",
       "      <td>0.171843</td>\n",
       "      <td>-0.040019</td>\n",
       "      <td>0.012981</td>\n",
       "      <td>0.036663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095241</td>\n",
       "      <td>-0.043918</td>\n",
       "      <td>0.083236</td>\n",
       "      <td>-0.108283</td>\n",
       "      <td>0.049672</td>\n",
       "      <td>-0.159878</td>\n",
       "      <td>-0.030408</td>\n",
       "      <td>0.125839</td>\n",
       "      <td>-0.018757</td>\n",
       "      <td>-0.046692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.193666</td>\n",
       "      <td>0.375576</td>\n",
       "      <td>-0.156056</td>\n",
       "      <td>-0.389625</td>\n",
       "      <td>0.251616</td>\n",
       "      <td>-0.243753</td>\n",
       "      <td>0.275591</td>\n",
       "      <td>0.215695</td>\n",
       "      <td>0.200658</td>\n",
       "      <td>-0.517700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.285520</td>\n",
       "      <td>-0.000538</td>\n",
       "      <td>0.029960</td>\n",
       "      <td>-0.280072</td>\n",
       "      <td>-0.026053</td>\n",
       "      <td>-0.071345</td>\n",
       "      <td>0.108964</td>\n",
       "      <td>0.345654</td>\n",
       "      <td>-0.139135</td>\n",
       "      <td>0.204263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.101977</td>\n",
       "      <td>0.429065</td>\n",
       "      <td>0.019283</td>\n",
       "      <td>0.031628</td>\n",
       "      <td>0.074446</td>\n",
       "      <td>-0.198176</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.063222</td>\n",
       "      <td>-0.069707</td>\n",
       "      <td>-0.033108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102900</td>\n",
       "      <td>-0.129451</td>\n",
       "      <td>0.112553</td>\n",
       "      <td>-0.063206</td>\n",
       "      <td>0.078754</td>\n",
       "      <td>-0.297892</td>\n",
       "      <td>-0.169865</td>\n",
       "      <td>0.088659</td>\n",
       "      <td>0.107301</td>\n",
       "      <td>-0.102119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.061705</td>\n",
       "      <td>0.257456</td>\n",
       "      <td>-0.028243</td>\n",
       "      <td>0.074250</td>\n",
       "      <td>-0.054962</td>\n",
       "      <td>-0.185502</td>\n",
       "      <td>0.102158</td>\n",
       "      <td>0.139254</td>\n",
       "      <td>0.090483</td>\n",
       "      <td>-0.173074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247859</td>\n",
       "      <td>-0.140237</td>\n",
       "      <td>0.206388</td>\n",
       "      <td>-0.164850</td>\n",
       "      <td>0.195620</td>\n",
       "      <td>0.074273</td>\n",
       "      <td>-0.045535</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>-0.099641</td>\n",
       "      <td>-0.057109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.343619</td>\n",
       "      <td>0.253092</td>\n",
       "      <td>-0.081679</td>\n",
       "      <td>-0.118190</td>\n",
       "      <td>0.464423</td>\n",
       "      <td>0.155281</td>\n",
       "      <td>0.142048</td>\n",
       "      <td>-0.275874</td>\n",
       "      <td>0.179293</td>\n",
       "      <td>-0.096489</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160492</td>\n",
       "      <td>0.151300</td>\n",
       "      <td>0.345881</td>\n",
       "      <td>-0.455922</td>\n",
       "      <td>0.323749</td>\n",
       "      <td>-0.223704</td>\n",
       "      <td>0.230631</td>\n",
       "      <td>0.048046</td>\n",
       "      <td>0.240613</td>\n",
       "      <td>0.013992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v0        v1        v2        v3        v4        v5        v6  \\\n",
       "0  0.221984  0.107015  0.006513 -0.389919  0.016036 -0.082146  0.171843   \n",
       "1  0.193666  0.375576 -0.156056 -0.389625  0.251616 -0.243753  0.275591   \n",
       "2  0.101977  0.429065  0.019283  0.031628  0.074446 -0.198176  0.472222   \n",
       "3  0.061705  0.257456 -0.028243  0.074250 -0.054962 -0.185502  0.102158   \n",
       "4  0.343619  0.253092 -0.081679 -0.118190  0.464423  0.155281  0.142048   \n",
       "\n",
       "         v7        v8        v9    ...          v90       v91       v92  \\\n",
       "0 -0.040019  0.012981  0.036663    ...    -0.095241 -0.043918  0.083236   \n",
       "1  0.215695  0.200658 -0.517700    ...    -0.285520 -0.000538  0.029960   \n",
       "2  0.063222 -0.069707 -0.033108    ...    -0.102900 -0.129451  0.112553   \n",
       "3  0.139254  0.090483 -0.173074    ...    -0.247859 -0.140237  0.206388   \n",
       "4 -0.275874  0.179293 -0.096489    ...    -0.160492  0.151300  0.345881   \n",
       "\n",
       "        v93       v94       v95       v96       v97       v98       v99  \n",
       "0 -0.108283  0.049672 -0.159878 -0.030408  0.125839 -0.018757 -0.046692  \n",
       "1 -0.280072 -0.026053 -0.071345  0.108964  0.345654 -0.139135  0.204263  \n",
       "2 -0.063206  0.078754 -0.297892 -0.169865  0.088659  0.107301 -0.102119  \n",
       "3 -0.164850  0.195620  0.074273 -0.045535  0.031400 -0.099641 -0.057109  \n",
       "4 -0.455922  0.323749 -0.223704  0.230631  0.048046  0.240613  0.013992  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfv = tweets_to_df(df, df['Relevancy'], embeddings, glove)\n",
    "labels = dfv.pop('class')\n",
    "dfv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import * \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import *\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def average(x):\n",
    "    return sum(x)/len(x)\n",
    "\n",
    "def get_stats(model, X, y, cv, verbose=False):\n",
    "    \n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    auc = []\n",
    "        \n",
    "    cv_results = cross_validate(model, X, y, scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc'], \n",
    "                                cv=cv, return_train_score=False)\n",
    "    \n",
    "    if verbose:\n",
    "        print(cv_results)\n",
    "    \n",
    "    #now return the data\n",
    "    return cv_results\n",
    "\n",
    "def print_stats(models, method, dfv, labels):\n",
    "    \n",
    "    vals = []\n",
    "    metric = []\n",
    "    model_name = []\n",
    "\n",
    "    f1 = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    accuracy = []\n",
    "    auc = []\n",
    "\n",
    "    cv = 10\n",
    "    for k,v in models.items():\n",
    "        stats = get_stats(v, dfv, labels, cv)\n",
    "        accuracy_avg = np.average(stats['test_accuracy'])\n",
    "        accuracy_std = np.std(stats['test_accuracy'])\n",
    "        precision_avg = np.average(stats['test_precision'])\n",
    "        precision_std = np.std(stats['test_precision'])\n",
    "        recall_avg = np.average(stats['test_recall'])\n",
    "        recall_std = np.std(stats['test_recall'])\n",
    "        f1_avg = np.average(stats['test_f1'])\n",
    "        f1_std = np.std(stats['test_f1'])\n",
    "        auc_avg = np.average(stats['test_roc_auc'])\n",
    "\n",
    "        f1.append('%.2f ± %.2f' % (f1_avg, f1_std))\n",
    "        precision.append('%.2f ± %.2f' % (precision_avg, precision_std))\n",
    "        recall.append('%.2f ± %.2f' % (recall_avg, recall_std))\n",
    "        accuracy.append('%.2f ± %.2f' % (accuracy_avg, accuracy_std))\n",
    "        auc.append('%.2f' % auc_avg)\n",
    "\n",
    "    df_view = pd.DataFrame(data={'Method': method, 'f1': f1, \n",
    "                                 'precision':precision, 'recall':recall,\n",
    "                                 'accuracy':accuracy, 'auc':auc})\n",
    "    display(df_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.69 ± 0.02</td>\n",
       "      <td>0.61 ± 0.03</td>\n",
       "      <td>0.80 ± 0.05</td>\n",
       "      <td>0.62 ± 0.03</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Voting</td>\n",
       "      <td>0.71 ± 0.02</td>\n",
       "      <td>0.63 ± 0.03</td>\n",
       "      <td>0.80 ± 0.05</td>\n",
       "      <td>0.64 ± 0.03</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.69 ± 0.02</td>\n",
       "      <td>0.67 ± 0.02</td>\n",
       "      <td>0.71 ± 0.06</td>\n",
       "      <td>0.66 ± 0.02</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.69 ± 0.02</td>\n",
       "      <td>0.66 ± 0.02</td>\n",
       "      <td>0.71 ± 0.03</td>\n",
       "      <td>0.65 ± 0.02</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.73 ± 0.04</td>\n",
       "      <td>0.67 ± 0.04</td>\n",
       "      <td>0.80 ± 0.05</td>\n",
       "      <td>0.68 ± 0.04</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Method           f1    precision       recall     accuracy  \\\n",
       "0             Naive Bayes  0.69 ± 0.02  0.61 ± 0.03  0.80 ± 0.05  0.62 ± 0.03   \n",
       "1                  Voting  0.71 ± 0.02  0.63 ± 0.03  0.80 ± 0.05  0.64 ± 0.03   \n",
       "2                     MLP  0.69 ± 0.02  0.67 ± 0.02  0.71 ± 0.06  0.66 ± 0.02   \n",
       "3                AdaBoost  0.69 ± 0.02  0.66 ± 0.02  0.71 ± 0.03  0.65 ± 0.02   \n",
       "4  Support Vector Machine  0.73 ± 0.04  0.67 ± 0.04  0.80 ± 0.05  0.68 ± 0.04   \n",
       "\n",
       "    auc  \n",
       "0  0.69  \n",
       "1  0.72  \n",
       "2  0.72  \n",
       "3  0.69  \n",
       "4  0.73  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = {'Naive Bayes': GaussianNB(),\n",
    "          'Voting': VotingClassifier(estimators=[('mlp', MLPClassifier()),\n",
    "                                            ('ada', AdaBoostClassifier()),\n",
    "                                            ('nb', GaussianNB())], voting='soft'),\n",
    "          'Perceptron': MLPClassifier(),\n",
    "          'AdaBoost': AdaBoostClassifier(),\n",
    "          'Support Vector Machine': SVC()\n",
    "        }\n",
    "method = ['Naive Bayes', 'Voting', 'MLP', 'AdaBoost', 'Support Vector Machine']\n",
    "\n",
    "print_stats(models, method, dfv, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Parameters and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try using randomized logistic regression to extract the most useful features from the 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Features:\n",
      "[(0.0, 'v12'), (0.0, 'v20'), (0.0, 'v21'), (0.0, 'v24'), (0.0, 'v28'), (0.0, 'v31'), (0.0, 'v32'), (0.0, 'v33'), (0.0, 'v34'), (0.0, 'v38'), (0.0, 'v4'), (0.0, 'v40'), (0.0, 'v43'), (0.0, 'v44'), (0.0, 'v48'), (0.0, 'v5'), (0.0, 'v56'), (0.0, 'v62'), (0.0, 'v63'), (0.0, 'v66'), (0.0, 'v71'), (0.0, 'v75'), (0.0, 'v76'), (0.0, 'v82'), (0.0, 'v83'), (0.0, 'v92'), (0.0, 'v93'), (0.0, 'v96'), (0.0, 'v97'), (0.005, 'v30'), (0.005, 'v42'), (0.005, 'v46'), (0.005, 'v51'), (0.005, 'v53'), (0.005, 'v55'), (0.005, 'v70'), (0.005, 'v8'), (0.01, 'v37'), (0.01, 'v47'), (0.01, 'v57'), (0.015, 'v87'), (0.02, 'v6'), (0.025, 'v23'), (0.025, 'v95'), (0.03, 'v73'), (0.035, 'v35'), (0.04, 'v52'), (0.04, 'v72'), (0.045, 'v41'), (0.05, 'v49'), (0.06, 'v15'), (0.06, 'v18'), (0.06, 'v39'), (0.065, 'v58'), (0.07, 'v74'), (0.08, 'v61'), (0.085, 'v68'), (0.1, 'v17'), (0.1, 'v98'), (0.12, 'v26'), (0.145, 'v54'), (0.145, 'v94'), (0.155, 'v86'), (0.16, 'v7'), (0.18, 'v90'), (0.185, 'v65'), (0.185, 'v91'), (0.205, 'v0'), (0.25, 'v13'), (0.26, 'v22'), (0.28, 'v25'), (0.285, 'v64'), (0.3, 'v1'), (0.305, 'v14'), (0.305, 'v2'), (0.315, 'v27'), (0.37, 'v89'), (0.38, 'v10'), (0.38, 'v78'), (0.385, 'v81'), (0.4, 'v3'), (0.4, 'v36'), (0.455, 'v45'), (0.46, 'v85'), (0.475, 'v19'), (0.475, 'v99'), (0.48, 'v77'), (0.505, 'v69'), (0.515, 'v67'), (0.525, 'v60'), (0.56, 'v50'), (0.56, 'v84'), (0.615, 'v16'), (0.635, 'v80'), (0.73, 'v79'), (0.84, 'v11'), (0.985, 'v29'), (0.99, 'v59'), (0.995, 'v88'), (0.995, 'v9')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.71 ± 0.03</td>\n",
       "      <td>0.65 ± 0.03</td>\n",
       "      <td>0.78 ± 0.07</td>\n",
       "      <td>0.65 ± 0.03</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Voting</td>\n",
       "      <td>0.72 ± 0.03</td>\n",
       "      <td>0.67 ± 0.03</td>\n",
       "      <td>0.78 ± 0.06</td>\n",
       "      <td>0.67 ± 0.03</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.71 ± 0.03</td>\n",
       "      <td>0.69 ± 0.02</td>\n",
       "      <td>0.73 ± 0.06</td>\n",
       "      <td>0.67 ± 0.02</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.70 ± 0.02</td>\n",
       "      <td>0.67 ± 0.02</td>\n",
       "      <td>0.73 ± 0.04</td>\n",
       "      <td>0.66 ± 0.02</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.73 ± 0.03</td>\n",
       "      <td>0.67 ± 0.03</td>\n",
       "      <td>0.80 ± 0.06</td>\n",
       "      <td>0.68 ± 0.03</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Method           f1    precision       recall     accuracy  \\\n",
       "0             Naive Bayes  0.71 ± 0.03  0.65 ± 0.03  0.78 ± 0.07  0.65 ± 0.03   \n",
       "1                  Voting  0.72 ± 0.03  0.67 ± 0.03  0.78 ± 0.06  0.67 ± 0.03   \n",
       "2                     MLP  0.71 ± 0.03  0.69 ± 0.02  0.73 ± 0.06  0.67 ± 0.02   \n",
       "3                AdaBoost  0.70 ± 0.02  0.67 ± 0.02  0.73 ± 0.04  0.66 ± 0.02   \n",
       "4  Support Vector Machine  0.73 ± 0.03  0.67 ± 0.03  0.80 ± 0.06  0.68 ± 0.03   \n",
       "\n",
       "    auc  \n",
       "0  0.72  \n",
       "1  0.74  \n",
       "2  0.72  \n",
       "3  0.71  \n",
       "4  0.74  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import RandomizedLogisticRegression, LogisticRegression\n",
    "\n",
    "#Use randomized lasso to select most useful subset of features\n",
    "rlog = RandomizedLogisticRegression()\n",
    "rlog.fit(dfv, labels)\n",
    "\n",
    "names = map(lambda x: 'v'+str(x), range(vec_length))\n",
    "features = sorted(zip(rlog.scores_, names))\n",
    "\n",
    "print('Ranked Features:')\n",
    "print(features)\n",
    "\n",
    "#now create a modified dataset with only features with score of at least 0.1\n",
    "drop_features = []\n",
    "for score, feat in features:\n",
    "    if score >= 0.1:\n",
    "        break\n",
    "    else:\n",
    "        drop_features.append(feat)\n",
    "\n",
    "dfv_rlog = dfv.drop(drop_features, axis=1)\n",
    "\n",
    "#now train the models on this dataset and show the results\n",
    "print_stats(models, method, dfv_rlog, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try a different approach, recursive feature elimination, and compare the selected features to those selected with randomized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.69 ± 0.02</td>\n",
       "      <td>0.62 ± 0.02</td>\n",
       "      <td>0.79 ± 0.05</td>\n",
       "      <td>0.62 ± 0.02</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Voting</td>\n",
       "      <td>0.71 ± 0.02</td>\n",
       "      <td>0.64 ± 0.03</td>\n",
       "      <td>0.80 ± 0.05</td>\n",
       "      <td>0.65 ± 0.03</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.70 ± 0.02</td>\n",
       "      <td>0.69 ± 0.02</td>\n",
       "      <td>0.72 ± 0.04</td>\n",
       "      <td>0.67 ± 0.02</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.68 ± 0.02</td>\n",
       "      <td>0.66 ± 0.02</td>\n",
       "      <td>0.71 ± 0.03</td>\n",
       "      <td>0.65 ± 0.02</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.73 ± 0.04</td>\n",
       "      <td>0.67 ± 0.04</td>\n",
       "      <td>0.80 ± 0.05</td>\n",
       "      <td>0.68 ± 0.04</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Method           f1    precision       recall     accuracy  \\\n",
       "0             Naive Bayes  0.69 ± 0.02  0.62 ± 0.02  0.79 ± 0.05  0.62 ± 0.02   \n",
       "1                  Voting  0.71 ± 0.02  0.64 ± 0.03  0.80 ± 0.05  0.65 ± 0.03   \n",
       "2                     MLP  0.70 ± 0.02  0.69 ± 0.02  0.72 ± 0.04  0.67 ± 0.02   \n",
       "3                AdaBoost  0.68 ± 0.02  0.66 ± 0.02  0.71 ± 0.03  0.65 ± 0.02   \n",
       "4  Support Vector Machine  0.73 ± 0.04  0.67 ± 0.04  0.80 ± 0.05  0.68 ± 0.04   \n",
       "\n",
       "    auc  \n",
       "0  0.69  \n",
       "1  0.72  \n",
       "2  0.72  \n",
       "3  0.70  \n",
       "4  0.74  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.69 ± 0.02</td>\n",
       "      <td>0.62 ± 0.02</td>\n",
       "      <td>0.77 ± 0.05</td>\n",
       "      <td>0.63 ± 0.03</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Voting</td>\n",
       "      <td>0.71 ± 0.03</td>\n",
       "      <td>0.65 ± 0.03</td>\n",
       "      <td>0.77 ± 0.05</td>\n",
       "      <td>0.66 ± 0.03</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.71 ± 0.02</td>\n",
       "      <td>0.69 ± 0.03</td>\n",
       "      <td>0.74 ± 0.06</td>\n",
       "      <td>0.68 ± 0.02</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.69 ± 0.02</td>\n",
       "      <td>0.67 ± 0.02</td>\n",
       "      <td>0.72 ± 0.03</td>\n",
       "      <td>0.66 ± 0.03</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.73 ± 0.03</td>\n",
       "      <td>0.68 ± 0.03</td>\n",
       "      <td>0.80 ± 0.05</td>\n",
       "      <td>0.69 ± 0.04</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Method           f1    precision       recall     accuracy  \\\n",
       "0             Naive Bayes  0.69 ± 0.02  0.62 ± 0.02  0.77 ± 0.05  0.63 ± 0.03   \n",
       "1                  Voting  0.71 ± 0.03  0.65 ± 0.03  0.77 ± 0.05  0.66 ± 0.03   \n",
       "2                     MLP  0.71 ± 0.02  0.69 ± 0.03  0.74 ± 0.06  0.68 ± 0.02   \n",
       "3                AdaBoost  0.69 ± 0.02  0.67 ± 0.02  0.72 ± 0.03  0.66 ± 0.03   \n",
       "4  Support Vector Machine  0.73 ± 0.03  0.68 ± 0.03  0.80 ± 0.05  0.69 ± 0.04   \n",
       "\n",
       "    auc  \n",
       "0  0.69  \n",
       "1  0.73  \n",
       "2  0.74  \n",
       "3  0.70  \n",
       "4  0.74  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "models = {'Naive Bayes': GaussianNB(),\n",
    "          'Voting': VotingClassifier(estimators=[('mlp', MLPClassifier()),\n",
    "                                            ('ada', AdaBoostClassifier()),\n",
    "                                            ('nb', GaussianNB())], voting='soft'),\n",
    "          'Perceptron': MLPClassifier(),\n",
    "          'AdaBoost': AdaBoostClassifier(),\n",
    "          'Support Vector Machine': SVC()\n",
    "        }\n",
    "method = ['Naive Bayes', 'Voting', 'MLP', 'AdaBoost', 'Support Vector Machine']\n",
    "models2 = [('Logistic Regression', LogisticRegression()), ('LinearSVC', LinearSVC())]\n",
    "\n",
    "for name, model in models2:\n",
    "    rfecv = RFECV(model, step=1, cv=10)\n",
    "    rfecv = rfecv.fit(dfv, labels)\n",
    "    mask = rfecv.support_\n",
    "    \n",
    "    drop_features = []\n",
    "    index = 0\n",
    "    for val in mask:\n",
    "        if not val:\n",
    "            drop_features.append('v'+str(index))\n",
    "        index += 1\n",
    "    \n",
    "    df_rfecv = dfv.drop(drop_features, axis=1)\n",
    "    print(name + ':')\n",
    "    print_stats(models, method, df_rfecv, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally try using random forests to rank the features and select a subset of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.70 ± 0.03</td>\n",
       "      <td>0.62 ± 0.02</td>\n",
       "      <td>0.80 ± 0.07</td>\n",
       "      <td>0.63 ± 0.03</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Voting</td>\n",
       "      <td>0.72 ± 0.02</td>\n",
       "      <td>0.64 ± 0.03</td>\n",
       "      <td>0.81 ± 0.06</td>\n",
       "      <td>0.65 ± 0.03</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.70 ± 0.02</td>\n",
       "      <td>0.70 ± 0.03</td>\n",
       "      <td>0.71 ± 0.04</td>\n",
       "      <td>0.67 ± 0.02</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.69 ± 0.03</td>\n",
       "      <td>0.66 ± 0.03</td>\n",
       "      <td>0.72 ± 0.04</td>\n",
       "      <td>0.65 ± 0.03</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.73 ± 0.03</td>\n",
       "      <td>0.67 ± 0.03</td>\n",
       "      <td>0.80 ± 0.05</td>\n",
       "      <td>0.67 ± 0.04</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Method           f1    precision       recall     accuracy  \\\n",
       "0             Naive Bayes  0.70 ± 0.03  0.62 ± 0.02  0.80 ± 0.07  0.63 ± 0.03   \n",
       "1                  Voting  0.72 ± 0.02  0.64 ± 0.03  0.81 ± 0.06  0.65 ± 0.03   \n",
       "2                     MLP  0.70 ± 0.02  0.70 ± 0.03  0.71 ± 0.04  0.67 ± 0.02   \n",
       "3                AdaBoost  0.69 ± 0.03  0.66 ± 0.03  0.72 ± 0.04  0.65 ± 0.03   \n",
       "4  Support Vector Machine  0.73 ± 0.03  0.67 ± 0.03  0.80 ± 0.05  0.67 ± 0.04   \n",
       "\n",
       "    auc  \n",
       "0  0.69  \n",
       "1  0.73  \n",
       "2  0.73  \n",
       "3  0.69  \n",
       "4  0.73  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#just keep the top half highest-scoring features\n",
    "forest = ExtraTreesClassifier()\n",
    "forest.fit(dfv, labels)\n",
    "scored_features = list(sorted(zip(forest.feature_importances_, map(lambda x: 'v' + str(x), range(vec_length))))[:25])\n",
    "drop_features = []\n",
    "\n",
    "for score, feat in scored_features[:100]:\n",
    "    drop_features.append(feat)\n",
    "\n",
    "df_forest = dfv.drop(drop_features, axis=1)\n",
    "print_stats(models, method, df_forest, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have gone through various feature selection methods, we conclude that RFECV with logistic regression is the best subset selection method. Our next step is to remove variables that are strongly correlated with each other (if any exist) to further prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v0</th>\n",
       "      <th>v3</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>v11</th>\n",
       "      <th>v14</th>\n",
       "      <th>v15</th>\n",
       "      <th>v16</th>\n",
       "      <th>v21</th>\n",
       "      <th>v29</th>\n",
       "      <th>...</th>\n",
       "      <th>v66</th>\n",
       "      <th>v67</th>\n",
       "      <th>v69</th>\n",
       "      <th>v72</th>\n",
       "      <th>v79</th>\n",
       "      <th>v80</th>\n",
       "      <th>v84</th>\n",
       "      <th>v88</th>\n",
       "      <th>v91</th>\n",
       "      <th>v95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.221984</td>\n",
       "      <td>-0.389919</td>\n",
       "      <td>0.036663</td>\n",
       "      <td>0.371330</td>\n",
       "      <td>-0.212883</td>\n",
       "      <td>0.181515</td>\n",
       "      <td>-0.194724</td>\n",
       "      <td>0.091855</td>\n",
       "      <td>-0.026420</td>\n",
       "      <td>0.255846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360816</td>\n",
       "      <td>-0.207129</td>\n",
       "      <td>-0.438700</td>\n",
       "      <td>-0.085548</td>\n",
       "      <td>-0.254994</td>\n",
       "      <td>1.085853</td>\n",
       "      <td>0.259061</td>\n",
       "      <td>-0.104013</td>\n",
       "      <td>-0.043918</td>\n",
       "      <td>-0.159878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.193666</td>\n",
       "      <td>-0.389625</td>\n",
       "      <td>-0.517700</td>\n",
       "      <td>-0.013028</td>\n",
       "      <td>0.248499</td>\n",
       "      <td>0.436000</td>\n",
       "      <td>-0.291994</td>\n",
       "      <td>0.164406</td>\n",
       "      <td>-0.265976</td>\n",
       "      <td>0.040028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040469</td>\n",
       "      <td>-0.137576</td>\n",
       "      <td>-0.632241</td>\n",
       "      <td>0.356269</td>\n",
       "      <td>-0.413745</td>\n",
       "      <td>1.083600</td>\n",
       "      <td>0.295365</td>\n",
       "      <td>0.159127</td>\n",
       "      <td>-0.000538</td>\n",
       "      <td>-0.071345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.101977</td>\n",
       "      <td>0.031628</td>\n",
       "      <td>-0.033108</td>\n",
       "      <td>0.218414</td>\n",
       "      <td>-0.270698</td>\n",
       "      <td>-0.007702</td>\n",
       "      <td>-0.084873</td>\n",
       "      <td>-0.080752</td>\n",
       "      <td>-0.093006</td>\n",
       "      <td>-0.065598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033689</td>\n",
       "      <td>0.244048</td>\n",
       "      <td>-0.098421</td>\n",
       "      <td>0.218490</td>\n",
       "      <td>0.042034</td>\n",
       "      <td>1.115348</td>\n",
       "      <td>-0.059687</td>\n",
       "      <td>-0.040710</td>\n",
       "      <td>-0.129451</td>\n",
       "      <td>-0.297892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.061705</td>\n",
       "      <td>0.074250</td>\n",
       "      <td>-0.173074</td>\n",
       "      <td>0.186495</td>\n",
       "      <td>0.022198</td>\n",
       "      <td>0.345279</td>\n",
       "      <td>-0.292078</td>\n",
       "      <td>-0.486195</td>\n",
       "      <td>-0.188117</td>\n",
       "      <td>-0.216139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070280</td>\n",
       "      <td>0.184151</td>\n",
       "      <td>-0.268234</td>\n",
       "      <td>0.268239</td>\n",
       "      <td>-0.141654</td>\n",
       "      <td>0.907921</td>\n",
       "      <td>0.017485</td>\n",
       "      <td>0.106431</td>\n",
       "      <td>-0.140237</td>\n",
       "      <td>0.074273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.343619</td>\n",
       "      <td>-0.118190</td>\n",
       "      <td>-0.096489</td>\n",
       "      <td>0.157339</td>\n",
       "      <td>-0.358852</td>\n",
       "      <td>0.036292</td>\n",
       "      <td>-0.349285</td>\n",
       "      <td>0.116080</td>\n",
       "      <td>0.046927</td>\n",
       "      <td>0.182346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095079</td>\n",
       "      <td>-0.052792</td>\n",
       "      <td>-0.447672</td>\n",
       "      <td>0.068924</td>\n",
       "      <td>-0.255653</td>\n",
       "      <td>0.714259</td>\n",
       "      <td>0.103810</td>\n",
       "      <td>-0.320353</td>\n",
       "      <td>0.151300</td>\n",
       "      <td>-0.223704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v0        v3        v9       v10       v11       v14       v15  \\\n",
       "0  0.221984 -0.389919  0.036663  0.371330 -0.212883  0.181515 -0.194724   \n",
       "1  0.193666 -0.389625 -0.517700 -0.013028  0.248499  0.436000 -0.291994   \n",
       "2  0.101977  0.031628 -0.033108  0.218414 -0.270698 -0.007702 -0.084873   \n",
       "3  0.061705  0.074250 -0.173074  0.186495  0.022198  0.345279 -0.292078   \n",
       "4  0.343619 -0.118190 -0.096489  0.157339 -0.358852  0.036292 -0.349285   \n",
       "\n",
       "        v16       v21       v29    ...          v66       v67       v69  \\\n",
       "0  0.091855 -0.026420  0.255846    ...     0.360816 -0.207129 -0.438700   \n",
       "1  0.164406 -0.265976  0.040028    ...    -0.040469 -0.137576 -0.632241   \n",
       "2 -0.080752 -0.093006 -0.065598    ...     0.033689  0.244048 -0.098421   \n",
       "3 -0.486195 -0.188117 -0.216139    ...     0.070280  0.184151 -0.268234   \n",
       "4  0.116080  0.046927  0.182346    ...     0.095079 -0.052792 -0.447672   \n",
       "\n",
       "        v72       v79       v80       v84       v88       v91       v95  \n",
       "0 -0.085548 -0.254994  1.085853  0.259061 -0.104013 -0.043918 -0.159878  \n",
       "1  0.356269 -0.413745  1.083600  0.295365  0.159127 -0.000538 -0.071345  \n",
       "2  0.218490  0.042034  1.115348 -0.059687 -0.040710 -0.129451 -0.297892  \n",
       "3  0.268239 -0.141654  0.907921  0.017485  0.106431 -0.140237  0.074273  \n",
       "4  0.068924 -0.255653  0.714259  0.103810 -0.320353  0.151300 -0.223704  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of correlated features: 0\n"
     ]
    }
   ],
   "source": [
    "rfecv = RFECV(LogisticRegression(), step=1, cv=10)\n",
    "rfecv = rfecv.fit(dfv, labels)\n",
    "mask = rfecv.support_\n",
    "\n",
    "drop_features = []\n",
    "index = 0\n",
    "for val in mask:\n",
    "    if not val:\n",
    "        drop_features.append('v'+str(index))\n",
    "    index += 1\n",
    "\n",
    "df_rfecv = dfv.drop(drop_features, axis=1)\n",
    "display(df_rfecv.head())\n",
    "\n",
    "correlated_features = set()  \n",
    "correlation_matrix = df_rfecv.corr()\n",
    "\n",
    "for i in range(len(correlation_matrix .columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "\n",
    "print('number of correlated features: ' + str(len(correlated_features)))\n",
    "df_corr = df_rfecv.drop(labels=correlated_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turned out that none of the remaining features are negatively correlated, so we continue just using the features retained by RFECV with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#rescale the data\n",
    "data = dfv.values\n",
    "scaler = MinMaxScaler(feature_range=[0, 1])\n",
    "data_rescaled = scaler.fit_transform(data)\n",
    "\n",
    "pca = PCA().fit(data_rescaled)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above calculation says that we should keep about 70 of the 100 principal components to capture over 90% of the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.71 ± 0.01</td>\n",
       "      <td>0.64 ± 0.03</td>\n",
       "      <td>0.81 ± 0.04</td>\n",
       "      <td>0.65 ± 0.02</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Voting</td>\n",
       "      <td>0.72 ± 0.02</td>\n",
       "      <td>0.67 ± 0.02</td>\n",
       "      <td>0.79 ± 0.04</td>\n",
       "      <td>0.67 ± 0.02</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.71 ± 0.02</td>\n",
       "      <td>0.69 ± 0.02</td>\n",
       "      <td>0.74 ± 0.04</td>\n",
       "      <td>0.68 ± 0.02</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.69 ± 0.03</td>\n",
       "      <td>0.67 ± 0.03</td>\n",
       "      <td>0.72 ± 0.05</td>\n",
       "      <td>0.65 ± 0.03</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.73 ± 0.03</td>\n",
       "      <td>0.67 ± 0.03</td>\n",
       "      <td>0.82 ± 0.05</td>\n",
       "      <td>0.68 ± 0.04</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Method           f1    precision       recall     accuracy  \\\n",
       "0             Naive Bayes  0.71 ± 0.01  0.64 ± 0.03  0.81 ± 0.04  0.65 ± 0.02   \n",
       "1                  Voting  0.72 ± 0.02  0.67 ± 0.02  0.79 ± 0.04  0.67 ± 0.02   \n",
       "2                     MLP  0.71 ± 0.02  0.69 ± 0.02  0.74 ± 0.04  0.68 ± 0.02   \n",
       "3                AdaBoost  0.69 ± 0.03  0.67 ± 0.03  0.72 ± 0.05  0.65 ± 0.03   \n",
       "4  Support Vector Machine  0.73 ± 0.03  0.67 ± 0.03  0.82 ± 0.05  0.68 ± 0.04   \n",
       "\n",
       "    auc  \n",
       "0  0.71  \n",
       "1  0.74  \n",
       "2  0.73  \n",
       "3  0.71  \n",
       "4  0.73  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#now test the models using this projected dataset\n",
    "pca = PCA(n_components=50)\n",
    "data_pca = pca.fit_transform(data_rescaled)\n",
    "df_pca = pd.DataFrame(data_pca)\n",
    "\n",
    "rfecv = RFECV(LogisticRegression(), step=1, cv=10)\n",
    "rfecv = rfecv.fit(df_pca, labels)\n",
    "mask = rfecv.support_\n",
    "\n",
    "drop_features = []\n",
    "index = 0\n",
    "for val in mask:\n",
    "    if not val:\n",
    "        drop_features.append(index)\n",
    "    index += 1\n",
    "\n",
    "df_pca = df_pca.drop(drop_features, axis=1)\n",
    "print_stats(models, method, df_pca, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Dataset  Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364956, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>Ok. Houston, stay home. Stay out of the attic....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>Trump killed flood safety rules 10 days before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>@funder @KariannHart Take that fucking wall do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>water vapor satellite image shows extensive dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>Alcohol helps cause traffic accidents.\\nSmokin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "1500  Ok. Houston, stay home. Stay out of the attic....\n",
       "1501  Trump killed flood safety rules 10 days before...\n",
       "1502  @funder @KariannHart Take that fucking wall do...\n",
       "1503  water vapor satellite image shows extensive dr...\n",
       "1504  Alcohol helps cause traffic accidents.\\nSmokin..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now generate a dataset for MTurk from the subset of 300k tweets\n",
    "df_subset = pd.read_csv('data/shuffled_subset_all.csv', lineterminator='\\n').drop(['Unnamed: 0'], axis=1).loc[1500:,:]\n",
    "print(df_subset.shape)\n",
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320472, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>Ok. Houston, stay home. Stay out of the attic....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>Trump killed flood safety rules 10 days before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>@funder @KariannHart Take that fucking wall do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>water vapor satellite image shows extensive dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>Alcohol helps cause traffic accidents.\\nSmokin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "1500  Ok. Houston, stay home. Stay out of the attic....\n",
       "1501  Trump killed flood safety rules 10 days before...\n",
       "1502  @funder @KariannHart Take that fucking wall do...\n",
       "1503  water vapor satellite image shows extensive dr...\n",
       "1504  Alcohol helps cause traffic accidents.\\nSmokin..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a crude way to filter out some donation tweets\n",
    "prune_indices = df_subset[df_subset['text'].str.contains('donat')].index.values.tolist()\n",
    "df_subset = df_subset.drop(prune_indices)\n",
    "print(df_subset.shape)\n",
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "#now get rid of non-English stuff\n",
    "drop_indices =[]\n",
    "for index, row in enumerate(df_subset.itertuples()):\n",
    "    \n",
    "    if index >= 20000:\n",
    "        break\n",
    "    elif index % 10000 == 0:\n",
    "        print(index)\n",
    "    \n",
    "    lang = detect(row.text)\n",
    "    if lang !='en' and lang != 'nl':\n",
    "        drop_indices.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320373, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now prune the tweets that are not English\n",
    "df_english = df_subset.drop(df_subset.index[drop_indices])\n",
    "df_english.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>Ok. Houston, stay home. Stay out of the attic....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>Trump killed flood safety rules 10 days before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>@funder @KariannHart Take that fucking wall do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>water vapor satellite image shows extensive dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>Alcohol helps cause traffic accidents.\\nSmokin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "1500  Ok. Houston, stay home. Stay out of the attic....\n",
       "1501  Trump killed flood safety rules 10 days before...\n",
       "1502  @funder @KariannHart Take that fucking wall do...\n",
       "1503  water vapor satellite image shows extensive dr...\n",
       "1504  Alcohol helps cause traffic accidents.\\nSmokin..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english_small = df_english.iloc[:20000,:]\n",
    "print(df_english_small.shape)\n",
    "df_english_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>Ok. Houston, stay home. Stay out of the attic....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>Trump killed flood safety rules 10 days before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>@funder @KariannHart Take that fucking wall do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>water vapor satellite image shows extensive dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>Alcohol helps cause traffic accidents.\\nSmokin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "1500  Ok. Houston, stay home. Stay out of the attic....\n",
       "1501  Trump killed flood safety rules 10 days before...\n",
       "1502  @funder @KariannHart Take that fucking wall do...\n",
       "1503  water vapor satellite image shows extensive dr...\n",
       "1504  Alcohol helps cause traffic accidents.\\nSmokin..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')\n",
    "df_noemoji = pd.DataFrame(data={'text' : df_english_small['text'].map(deEmojify)})\n",
    "print(df_noemoji.shape)\n",
    "df_noemoji.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ok houston stay home stay attic call &lt;number&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trump kill flood safety rule &lt;number&gt; days har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; take fuck wall &lt;user&gt; hurricaneh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>water vapor satellite image show extensive dry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alcohol help cause traffic accidents smoke hel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  ok houston stay home stay attic call <number> ...\n",
       "1  trump kill flood safety rule <number> days har...\n",
       "2  <user> <user> take fuck wall <user> hurricaneh...\n",
       "3  water vapor satellite image show extensive dry...\n",
       "4  alcohol help cause traffic accidents smoke hel..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now preprocess the text\n",
    "df_test = df_noemoji.copy().reset_index(drop=True)\n",
    "df_test['Text'] = df_test['text'].map(lambda x: process_tweet(x, tknzr, None, True))\n",
    "df_test.pop('text')\n",
    "\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v40</th>\n",
       "      <th>v41</th>\n",
       "      <th>v42</th>\n",
       "      <th>v43</th>\n",
       "      <th>v44</th>\n",
       "      <th>v45</th>\n",
       "      <th>v46</th>\n",
       "      <th>v47</th>\n",
       "      <th>v48</th>\n",
       "      <th>v49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.043563</td>\n",
       "      <td>0.168890</td>\n",
       "      <td>0.132577</td>\n",
       "      <td>-0.318665</td>\n",
       "      <td>-0.148289</td>\n",
       "      <td>0.082531</td>\n",
       "      <td>0.732835</td>\n",
       "      <td>0.150738</td>\n",
       "      <td>0.097140</td>\n",
       "      <td>-0.027730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.726723</td>\n",
       "      <td>0.383673</td>\n",
       "      <td>0.364157</td>\n",
       "      <td>-0.003794</td>\n",
       "      <td>0.233437</td>\n",
       "      <td>-0.222859</td>\n",
       "      <td>0.229726</td>\n",
       "      <td>-0.269067</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.182357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.111794</td>\n",
       "      <td>-0.194317</td>\n",
       "      <td>-0.372960</td>\n",
       "      <td>-0.037570</td>\n",
       "      <td>-0.161805</td>\n",
       "      <td>0.327506</td>\n",
       "      <td>0.018590</td>\n",
       "      <td>0.199540</td>\n",
       "      <td>-0.131544</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.539943</td>\n",
       "      <td>-0.106992</td>\n",
       "      <td>0.183136</td>\n",
       "      <td>-0.061690</td>\n",
       "      <td>0.293251</td>\n",
       "      <td>-0.094791</td>\n",
       "      <td>-0.082968</td>\n",
       "      <td>-0.185012</td>\n",
       "      <td>-0.121363</td>\n",
       "      <td>0.213161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.581150</td>\n",
       "      <td>0.288926</td>\n",
       "      <td>0.154654</td>\n",
       "      <td>-0.169325</td>\n",
       "      <td>0.123784</td>\n",
       "      <td>0.187431</td>\n",
       "      <td>0.353618</td>\n",
       "      <td>0.311923</td>\n",
       "      <td>0.172137</td>\n",
       "      <td>0.650069</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.555811</td>\n",
       "      <td>-0.098209</td>\n",
       "      <td>0.217706</td>\n",
       "      <td>0.436886</td>\n",
       "      <td>0.221757</td>\n",
       "      <td>-0.083573</td>\n",
       "      <td>-0.025337</td>\n",
       "      <td>0.116973</td>\n",
       "      <td>-0.093931</td>\n",
       "      <td>-0.081701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.213230</td>\n",
       "      <td>-0.330857</td>\n",
       "      <td>0.081841</td>\n",
       "      <td>-0.154605</td>\n",
       "      <td>-0.170297</td>\n",
       "      <td>0.227414</td>\n",
       "      <td>0.515372</td>\n",
       "      <td>-0.379785</td>\n",
       "      <td>0.105189</td>\n",
       "      <td>-0.330809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.582300</td>\n",
       "      <td>-0.138565</td>\n",
       "      <td>-0.083400</td>\n",
       "      <td>-0.111499</td>\n",
       "      <td>0.082342</td>\n",
       "      <td>0.159363</td>\n",
       "      <td>0.187800</td>\n",
       "      <td>-0.086185</td>\n",
       "      <td>0.011892</td>\n",
       "      <td>0.233976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.094960</td>\n",
       "      <td>0.266487</td>\n",
       "      <td>-0.848661</td>\n",
       "      <td>-0.214325</td>\n",
       "      <td>0.030654</td>\n",
       "      <td>0.126114</td>\n",
       "      <td>0.402066</td>\n",
       "      <td>-0.261091</td>\n",
       "      <td>0.113748</td>\n",
       "      <td>0.007945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373645</td>\n",
       "      <td>-0.058034</td>\n",
       "      <td>0.059335</td>\n",
       "      <td>-0.090298</td>\n",
       "      <td>0.071710</td>\n",
       "      <td>-0.197974</td>\n",
       "      <td>0.188452</td>\n",
       "      <td>0.468361</td>\n",
       "      <td>-0.197567</td>\n",
       "      <td>0.282665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v0        v1        v2        v3        v4        v5        v6  \\\n",
       "0 -0.043563  0.168890  0.132577 -0.318665 -0.148289  0.082531  0.732835   \n",
       "1  0.198020  0.111794 -0.194317 -0.372960 -0.037570 -0.161805  0.327506   \n",
       "2  0.581150  0.288926  0.154654 -0.169325  0.123784  0.187431  0.353618   \n",
       "3 -0.213230 -0.330857  0.081841 -0.154605 -0.170297  0.227414  0.515372   \n",
       "4  0.094960  0.266487 -0.848661 -0.214325  0.030654  0.126114  0.402066   \n",
       "\n",
       "         v7        v8        v9    ...          v40       v41       v42  \\\n",
       "0  0.150738  0.097140 -0.027730    ...    -0.726723  0.383673  0.364157   \n",
       "1  0.018590  0.199540 -0.131544    ...    -0.539943 -0.106992  0.183136   \n",
       "2  0.311923  0.172137  0.650069    ...    -1.555811 -0.098209  0.217706   \n",
       "3 -0.379785  0.105189 -0.330809    ...    -0.582300 -0.138565 -0.083400   \n",
       "4 -0.261091  0.113748  0.007945    ...    -0.373645 -0.058034  0.059335   \n",
       "\n",
       "        v43       v44       v45       v46       v47       v48       v49  \n",
       "0 -0.003794  0.233437 -0.222859  0.229726 -0.269067  0.002645  0.182357  \n",
       "1 -0.061690  0.293251 -0.094791 -0.082968 -0.185012 -0.121363  0.213161  \n",
       "2  0.436886  0.221757 -0.083573 -0.025337  0.116973 -0.093931 -0.081701  \n",
       "3 -0.111499  0.082342  0.159363  0.187800 -0.086185  0.011892  0.233976  \n",
       "4 -0.090298  0.071710 -0.197974  0.188452  0.468361 -0.197567  0.282665  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now convert to word embeddings\n",
    "df_testv = tweets_to_df(df_test, [0]*df_test.shape[0], embeddings, glove).reset_index(drop=True)\n",
    "df_testv.pop('class')\n",
    "df_testv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now run the model\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(dfv, labels)\n",
    "pred = mlp.predict(df_testv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looked through index 10783\n",
      "successful\n"
     ]
    }
   ],
   "source": [
    "#now run until we have 5000 positives and 5000 negatives\n",
    "pos = 0\n",
    "neg = 0\n",
    "indices = []\n",
    "success = False\n",
    "\n",
    "for index in range(len(pred)):\n",
    "    p = pred[index]\n",
    "    if pos == 2400 and neg == 1600:\n",
    "        success = True\n",
    "        print('looked through index ' + str(index))\n",
    "        break\n",
    "    elif p == 0:\n",
    "        if neg < 1600:\n",
    "            indices.append(index)\n",
    "            neg += 1\n",
    "    else:\n",
    "        if pos < 2400:\n",
    "            indices.append(index)\n",
    "            pos += 1\n",
    "\n",
    "print('successful' if success else 'failure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ok. Houston, stay home. Stay out of the attic....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump killed flood safety rules 10 days before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@funder @KariannHart Take that fucking wall do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>water vapor satellite image shows extensive dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alcohol helps cause traffic accidents.\\nSmokin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@TXInstruments @tenethealth @jcpenney can we k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How horrible, look how high the water is on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Remember those we lost, those who helped us co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How @SpringISD is helping its community recove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>For over 100 yrs @VOATexas has served those in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#Harvey aftermath: \"More flooding threatens #B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Before leaving to help or evacuate use https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>If #media had half an ounce of sense they woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Catastrophic flooding underway in Houston and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hey #Florida TAG you're it \\n#hoUSton #houston...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Shout out to the caring employees in San Anton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@CBSBigBrother @CBS  when will this episode of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I'm not home, but keep torturing myself with #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>To give y'all some context of the flooding fol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>#Harvey has dumped more than 11 trillion gallo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Man my mom went all Gangsta on me today re: #H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>How to build #resilient #infrastructure to fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"Flood-prone Houston at the mercy of Hurricane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Looking to help Hurricane Harvey &amp;amp; Eagle C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>#HoustonFloods #HarveyFlood @UnderbellyHOU #fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>@DrGregBonnen How do you sleep at night now, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Those in the path of #Harvey\\n in #Texas, plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>#BigNews in public safety: Montgomery rescue c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Volunteers needed at 14 sites for @TXAdoptABea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Humanitarian support for #HoustonFloods on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3970</th>\n",
       "      <td>So proud of my #Texas brethren brown black whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>#houstonflood Text from my sister, just now. S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3972</th>\n",
       "      <td>@TheTruth24US Insane that Sharks have found th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3973</th>\n",
       "      <td>VibeMagazine: Pimp C's son reportedly stranded...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3974</th>\n",
       "      <td>For context; he's a cartoonist for @politico h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3975</th>\n",
       "      <td>@KIII3News @KRIS6News @Action10News @Callerdot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3976</th>\n",
       "      <td>If you haven't already, please consider giving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>#Harvey  Please RT  New rescue map https://t.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3978</th>\n",
       "      <td>UT supercomputer tracking Harvey's flood risk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>If you are looking for a place to take your ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>Please help my fellow Texans who have been aff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>.@JoelOsteen, meet #FrankZappa\\nhttps://t.co/q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3982</th>\n",
       "      <td>Sad news. According to @HoustonChron: #Harvey ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3983</th>\n",
       "      <td>Anither urgent call fir help. Sick kids, no wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3984</th>\n",
       "      <td>This is where #Houstonflooding and #FlintWater...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3985</th>\n",
       "      <td>Can you imagine a year's worth of rain in a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>Your city out here flooded and you buying a ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987</th>\n",
       "      <td>It's been 2 and a half days since we left our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>#Houston #Houstonflood https://t.co/AllfqMHaOd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>@BrandenHarvey Hurricane Harvey might cause di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>Stamps QB Bo Levi Mitchell leads team onto the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>#TedCruz shld hold off Trump Visit #houstonflo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>It's good #HurricaneHarvey got graded to a lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>Raging bayou at the end of the street  #harvey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3994</th>\n",
       "      <td>I can't even go to sleep, I'm too afraid  #hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>Please send good thoughts and prayers to @Alie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>I cannot tell you the visceral reaction I had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>There are lots of worst photos, but this one i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>Ron Paul from his home in the Houston area: \"W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>#CorpusChristi #HurricaneHarvey area and are g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     Ok. Houston, stay home. Stay out of the attic....\n",
       "1     Trump killed flood safety rules 10 days before...\n",
       "2     @funder @KariannHart Take that fucking wall do...\n",
       "3     water vapor satellite image shows extensive dr...\n",
       "4     Alcohol helps cause traffic accidents.\\nSmokin...\n",
       "5     @TXInstruments @tenethealth @jcpenney can we k...\n",
       "6     How horrible, look how high the water is on th...\n",
       "7     Remember those we lost, those who helped us co...\n",
       "8     How @SpringISD is helping its community recove...\n",
       "9     For over 100 yrs @VOATexas has served those in...\n",
       "10    #Harvey aftermath: \"More flooding threatens #B...\n",
       "11    Before leaving to help or evacuate use https:/...\n",
       "12    If #media had half an ounce of sense they woul...\n",
       "13    Catastrophic flooding underway in Houston and ...\n",
       "14    Hey #Florida TAG you're it \\n#hoUSton #houston...\n",
       "15    Shout out to the caring employees in San Anton...\n",
       "16    @CBSBigBrother @CBS  when will this episode of...\n",
       "17    I'm not home, but keep torturing myself with #...\n",
       "18    To give y'all some context of the flooding fol...\n",
       "19    #Harvey has dumped more than 11 trillion gallo...\n",
       "20    Man my mom went all Gangsta on me today re: #H...\n",
       "21    How to build #resilient #infrastructure to fac...\n",
       "22    \"Flood-prone Houston at the mercy of Hurricane...\n",
       "23    Looking to help Hurricane Harvey &amp; Eagle C...\n",
       "24    #HoustonFloods #HarveyFlood @UnderbellyHOU #fo...\n",
       "25    @DrGregBonnen How do you sleep at night now, k...\n",
       "26    Those in the path of #Harvey\\n in #Texas, plea...\n",
       "27    #BigNews in public safety: Montgomery rescue c...\n",
       "28    Volunteers needed at 14 sites for @TXAdoptABea...\n",
       "29    Humanitarian support for #HoustonFloods on the...\n",
       "...                                                 ...\n",
       "3970  So proud of my #Texas brethren brown black whi...\n",
       "3971  #houstonflood Text from my sister, just now. S...\n",
       "3972  @TheTruth24US Insane that Sharks have found th...\n",
       "3973  VibeMagazine: Pimp C's son reportedly stranded...\n",
       "3974  For context; he's a cartoonist for @politico h...\n",
       "3975  @KIII3News @KRIS6News @Action10News @Callerdot...\n",
       "3976  If you haven't already, please consider giving...\n",
       "3977  #Harvey  Please RT  New rescue map https://t.c...\n",
       "3978  UT supercomputer tracking Harvey's flood risk ...\n",
       "3979  If you are looking for a place to take your ho...\n",
       "3980  Please help my fellow Texans who have been aff...\n",
       "3981  .@JoelOsteen, meet #FrankZappa\\nhttps://t.co/q...\n",
       "3982  Sad news. According to @HoustonChron: #Harvey ...\n",
       "3983  Anither urgent call fir help. Sick kids, no wa...\n",
       "3984  This is where #Houstonflooding and #FlintWater...\n",
       "3985  Can you imagine a year's worth of rain in a fe...\n",
       "3986  Your city out here flooded and you buying a ba...\n",
       "3987  It's been 2 and a half days since we left our ...\n",
       "3988     #Houston #Houstonflood https://t.co/AllfqMHaOd\n",
       "3989  @BrandenHarvey Hurricane Harvey might cause di...\n",
       "3990  Stamps QB Bo Levi Mitchell leads team onto the...\n",
       "3991  #TedCruz shld hold off Trump Visit #houstonflo...\n",
       "3992  It's good #HurricaneHarvey got graded to a lev...\n",
       "3993  Raging bayou at the end of the street  #harvey...\n",
       "3994  I can't even go to sleep, I'm too afraid  #hou...\n",
       "3995  Please send good thoughts and prayers to @Alie...\n",
       "3996  I cannot tell you the visceral reaction I had ...\n",
       "3997  There are lots of worst photos, but this one i...\n",
       "3998  Ron Paul from his home in the Houston area: \"W...\n",
       "3999  #CorpusChristi #HurricaneHarvey area and are g...\n",
       "\n",
       "[4000 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now make the final dataframe containing 10000 tweets that are relevant and not relevant\n",
    "df_mturk = df_noemoji.reset_index(drop=True).loc[indices].reset_index(drop=True)\n",
    "df_mturk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mturk.to_csv('data/mturk_final4000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
