{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ashwin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ashwin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import random\n",
    "import time\n",
    "\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "\n",
    "# import process_tweet\n",
    "# import importlib\n",
    "# importlib.reload(process_tweet)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#create spell checker/word splitter\n",
    "def create_symspell(max_edit_distance, prefix_length, freq_file_path):\n",
    "    # create object\n",
    "    sym_spell = SymSpell(max_edit_distance, prefix_length)\n",
    "    \n",
    "    # create dictionary using corpus.txt\n",
    "    if not sym_spell.create_dictionary(freq_file_path):\n",
    "        print(\"Corpus file not found\")\n",
    "        return None\n",
    "    return sym_spell\n",
    "\n",
    "def is_valid_token(w):\n",
    "    special = ['<url>','<number>', '<user>']\n",
    "    return w.isalpha() or w in special\n",
    "\n",
    "def process_tweet(tweet, tknzr, sym_spell=None, advanced=False):\n",
    "    st_1 = []\n",
    "    for w in tknzr.tokenize(tweet):\n",
    "        #remove retweet annotation if present:\n",
    "        if w == 'RT':\n",
    "            if advanced:\n",
    "                st_1.append('rt')\n",
    "        elif w[0] == '@':\n",
    "            if advanced:\n",
    "                st_1.append('<user>')\n",
    "        #remove hashtag symbol\n",
    "        elif w[0] == '#':\n",
    "            st_1.append(w[1:])\n",
    "        #replace link with LINK keyword\n",
    "        elif w[:4] == 'http':\n",
    "            st_1.append('<url>')\n",
    "        elif w.isnumeric():\n",
    "            if advanced:\n",
    "                st_1.append('<number>')\n",
    "        else:\n",
    "            st_1.append(w)\n",
    "    \n",
    "    st_2 = []\n",
    "    \n",
    "    #remove stop words and punctuation, make everything lowercase\n",
    "    if sym_spell != None:\n",
    "        st_2 = [sym_spell.word_segmentation(w.lower()).corrected_string \n",
    "                for w in st_1 if w.isalpha() and not w.lower() in stop_words]\n",
    "    elif advanced:\n",
    "        st_2 = [w.lower() for w in st_1 if is_valid_token(w) and \n",
    "                    not w.lower() in stop_words]\n",
    "    else:\n",
    "        st_2 = [w.lower() for w in st_1 if w.isalpha() and\n",
    "                not w.lower() in stop_words]\n",
    "    \n",
    "    #lemmatization (converts all words to root form for standardization)\n",
    "    lem = WordNetLemmatizer()\n",
    "    st_3 = list(map(lambda x: lem.lemmatize(x, pos='v'), st_2))\n",
    "    \n",
    "    #now do word segmentation/spell check\n",
    "    return ' '.join(st_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1741, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Urgency</th>\n",
       "      <th>Relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>your rescue boats, vehicles, volunteer craft ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!!  According to #PBS, #Houston convention cen...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"100,000 homes have been damaged by #Harvey an...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Do you need us to get a boat?\" Things I never...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Flood of epic proportions.\" #HurricaneHarvey ...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Urgency  Relevancy\n",
       "0   your rescue boats, vehicles, volunteer craft ...        0          0\n",
       "1  !!  According to #PBS, #Houston convention cen...        0          1\n",
       "2  \"100,000 homes have been damaged by #Harvey an...        0          2\n",
       "3  \"Do you need us to get a boat?\" Things I never...        0          0\n",
       "4  \"Flood of epic proportions.\" #HurricaneHarvey ...        0          3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/labeled_prelim.csv').dropna().astype(\n",
    "        {'Relevancy':np.int32, 'Urgency':np.int32}).reset_index(drop=True)\n",
    "df.pop('Id')\n",
    "\n",
    "df2 = pd.read_csv('data/mturk_results_0-2000_processed.csv')\n",
    "#df = pd.concat([df, df2], ignore_index=True)\n",
    "df = df2\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Urgency</th>\n",
       "      <th>Relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rescue boat vehicles volunteer craft need txwx...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accord pbs houston convention center need whee...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>home damage harvey financial toll begin &lt;url&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need us get boat things never think ask friend...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flood epic proportion hurricaneharvey inundate...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Urgency  Relevancy\n",
       "0  rescue boat vehicles volunteer craft need txwx...        0          0\n",
       "1  accord pbs houston convention center need whee...        0          1\n",
       "2      home damage harvey financial toll begin <url>        0          2\n",
       "3  need us get boat things never think ask friend...        0          0\n",
       "4  flood epic proportion hurricaneharvey inundate...        0          3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sym_spell = create_symspell(2,7,'data/frequency_dictionary_en_82_765.txt')\n",
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "df['Text'] = df['Text'].map(lambda x: process_tweet(x, tknzr, None, True))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<user> hello world'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_tweet('@user #hello world', tknzr, None,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty line\n"
     ]
    }
   ],
   "source": [
    "#list of embeddings\n",
    "vec_length = 50\n",
    "embeddings = np.zeros((1193514, vec_length))\n",
    "\n",
    "#two-way map, index->word and word->index\n",
    "glove = {}\n",
    "\n",
    "index = 0\n",
    "with open('data/glove.twitter.27B/glove.twitter.27B.%dd.txt' % vec_length) as f:\n",
    "    for l in f:\n",
    "        line = []\n",
    "        try:\n",
    "            line = l.split()\n",
    "            if len(line) != vec_length+1:\n",
    "                print('empty line')\n",
    "                continue\n",
    "            \n",
    "            word = line[0]\n",
    "            embeddings[index] = np.array(line[1:]).astype(np.float)\n",
    "            glove[index] = word\n",
    "            glove[word] = index\n",
    "            index += 1\n",
    "        except:\n",
    "            print(line)\n",
    "            print(index)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only handles binary classification for now\n",
    "def tweets_to_df(df, labels, embeddings, glove):\n",
    "    \n",
    "    weights = []\n",
    "    index_omit = []\n",
    "    index = -1\n",
    "    tweets = df['Text']\n",
    "    \n",
    "    #a column for each entry in the embedding vector\n",
    "    for i in range(vec_length+1):\n",
    "        weights.append([])\n",
    "    \n",
    "    for i in range(len(tweets)):\n",
    "        index += 1\n",
    "        cur_embed = []\n",
    "        cur_tweet = tweets[i]\n",
    "        cur_label = labels[i]\n",
    "        for i in cur_tweet.split():\n",
    "            if i in glove:\n",
    "                cur_embed.append(embeddings[glove[i]])\n",
    "        \n",
    "        if len(cur_embed) == 0:\n",
    "            #make sure we drop this row from the input dataframe\n",
    "            index_omit.append(index)\n",
    "            continue\n",
    "        \n",
    "        x = np.asarray(np.mean(cur_embed, axis=0))\n",
    "        \n",
    "        for j in range(vec_length):\n",
    "            weights[j].append(x[j])\n",
    "        weights[vec_length].append(0 if cur_label == 0 else 1)\n",
    "        #weights[vec_length].append(cur_label)\n",
    "        \n",
    "    df_pruned = df.drop(index_omit)\n",
    "    \n",
    "    #convert to dataframe\n",
    "    cols = {}\n",
    "    for i in range(vec_length):\n",
    "       cols['v' + str(i)] = weights[i]\n",
    "    \n",
    "    cols['class'] = weights[vec_length]\n",
    "    \n",
    "    df2 = pd.DataFrame(data=cols)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v40</th>\n",
       "      <th>v41</th>\n",
       "      <th>v42</th>\n",
       "      <th>v43</th>\n",
       "      <th>v44</th>\n",
       "      <th>v45</th>\n",
       "      <th>v46</th>\n",
       "      <th>v47</th>\n",
       "      <th>v48</th>\n",
       "      <th>v49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018780</td>\n",
       "      <td>0.019162</td>\n",
       "      <td>-0.204499</td>\n",
       "      <td>-0.702597</td>\n",
       "      <td>-0.041929</td>\n",
       "      <td>-0.200875</td>\n",
       "      <td>0.487510</td>\n",
       "      <td>-0.261408</td>\n",
       "      <td>0.475035</td>\n",
       "      <td>-0.831866</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097450</td>\n",
       "      <td>0.046975</td>\n",
       "      <td>0.353386</td>\n",
       "      <td>0.013975</td>\n",
       "      <td>0.025537</td>\n",
       "      <td>0.338198</td>\n",
       "      <td>0.196569</td>\n",
       "      <td>-0.062159</td>\n",
       "      <td>0.185096</td>\n",
       "      <td>0.603920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.115822</td>\n",
       "      <td>0.151914</td>\n",
       "      <td>-0.252038</td>\n",
       "      <td>-0.271220</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>-0.236383</td>\n",
       "      <td>0.261833</td>\n",
       "      <td>-0.037649</td>\n",
       "      <td>0.440095</td>\n",
       "      <td>-0.387786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.351785</td>\n",
       "      <td>0.037818</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>-0.001865</td>\n",
       "      <td>0.229696</td>\n",
       "      <td>0.023982</td>\n",
       "      <td>0.208820</td>\n",
       "      <td>-0.071991</td>\n",
       "      <td>-0.145986</td>\n",
       "      <td>-0.214229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.142689</td>\n",
       "      <td>0.119957</td>\n",
       "      <td>-0.359924</td>\n",
       "      <td>-0.618667</td>\n",
       "      <td>0.152206</td>\n",
       "      <td>0.008059</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>-0.316391</td>\n",
       "      <td>-0.005230</td>\n",
       "      <td>-0.621019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.572997</td>\n",
       "      <td>0.111872</td>\n",
       "      <td>0.359161</td>\n",
       "      <td>0.112237</td>\n",
       "      <td>0.236217</td>\n",
       "      <td>-0.093718</td>\n",
       "      <td>-0.026002</td>\n",
       "      <td>-0.046747</td>\n",
       "      <td>-0.053574</td>\n",
       "      <td>0.614299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129950</td>\n",
       "      <td>0.395938</td>\n",
       "      <td>-0.180748</td>\n",
       "      <td>-0.024789</td>\n",
       "      <td>-0.156947</td>\n",
       "      <td>-0.214214</td>\n",
       "      <td>0.889963</td>\n",
       "      <td>-0.050893</td>\n",
       "      <td>0.223831</td>\n",
       "      <td>0.224263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.492443</td>\n",
       "      <td>0.448336</td>\n",
       "      <td>0.199528</td>\n",
       "      <td>-0.159725</td>\n",
       "      <td>0.325608</td>\n",
       "      <td>-0.147479</td>\n",
       "      <td>0.252455</td>\n",
       "      <td>0.166972</td>\n",
       "      <td>-0.021204</td>\n",
       "      <td>0.269543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.223490</td>\n",
       "      <td>0.356330</td>\n",
       "      <td>-0.052827</td>\n",
       "      <td>-0.479262</td>\n",
       "      <td>0.179755</td>\n",
       "      <td>-0.060940</td>\n",
       "      <td>0.522521</td>\n",
       "      <td>-0.223807</td>\n",
       "      <td>0.280253</td>\n",
       "      <td>-0.206572</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.523289</td>\n",
       "      <td>-0.134802</td>\n",
       "      <td>0.155163</td>\n",
       "      <td>0.086522</td>\n",
       "      <td>-0.065962</td>\n",
       "      <td>0.267316</td>\n",
       "      <td>-0.133432</td>\n",
       "      <td>0.090854</td>\n",
       "      <td>0.073698</td>\n",
       "      <td>0.391386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v0        v1        v2        v3        v4        v5        v6  \\\n",
       "0  0.018780  0.019162 -0.204499 -0.702597 -0.041929 -0.200875  0.487510   \n",
       "1  0.115822  0.151914 -0.252038 -0.271220  0.005090 -0.236383  0.261833   \n",
       "2  0.142689  0.119957 -0.359924 -0.618667  0.152206  0.008059  0.015033   \n",
       "3  0.129950  0.395938 -0.180748 -0.024789 -0.156947 -0.214214  0.889963   \n",
       "4  0.223490  0.356330 -0.052827 -0.479262  0.179755 -0.060940  0.522521   \n",
       "\n",
       "         v7        v8        v9    ...          v40       v41       v42  \\\n",
       "0 -0.261408  0.475035 -0.831866    ...    -0.097450  0.046975  0.353386   \n",
       "1 -0.037649  0.440095 -0.387786    ...    -0.351785  0.037818  0.001821   \n",
       "2 -0.316391 -0.005230 -0.621019    ...    -0.572997  0.111872  0.359161   \n",
       "3 -0.050893  0.223831  0.224263    ...    -0.492443  0.448336  0.199528   \n",
       "4 -0.223807  0.280253 -0.206572    ...    -0.523289 -0.134802  0.155163   \n",
       "\n",
       "        v43       v44       v45       v46       v47       v48       v49  \n",
       "0  0.013975  0.025537  0.338198  0.196569 -0.062159  0.185096  0.603920  \n",
       "1 -0.001865  0.229696  0.023982  0.208820 -0.071991 -0.145986 -0.214229  \n",
       "2  0.112237  0.236217 -0.093718 -0.026002 -0.046747 -0.053574  0.614299  \n",
       "3 -0.159725  0.325608 -0.147479  0.252455  0.166972 -0.021204  0.269543  \n",
       "4  0.086522 -0.065962  0.267316 -0.133432  0.090854  0.073698  0.391386  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfv = tweets_to_df(df, df['Relevancy'], embeddings, glove)\n",
    "labels = dfv.pop('class')\n",
    "dfv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import * \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import *\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "def average(x):\n",
    "    return sum(x)/len(x)\n",
    "\n",
    "def get_stats(model, X, y, cv, verbose=False):\n",
    "    \n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    auc = []\n",
    "        \n",
    "    cv_results = cross_validate(model, X, y, scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc'], \n",
    "                                cv=cv, return_train_score=False)\n",
    "    \n",
    "    if verbose:\n",
    "        print(cv_results)\n",
    "    \n",
    "    #now return the data\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.66 ± 0.04</td>\n",
       "      <td>0.57 ± 0.05</td>\n",
       "      <td>0.78 ± 0.06</td>\n",
       "      <td>0.62 ± 0.05</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Voting</td>\n",
       "      <td>0.67 ± 0.03</td>\n",
       "      <td>0.60 ± 0.05</td>\n",
       "      <td>0.75 ± 0.05</td>\n",
       "      <td>0.66 ± 0.05</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.66 ± 0.03</td>\n",
       "      <td>0.66 ± 0.05</td>\n",
       "      <td>0.66 ± 0.04</td>\n",
       "      <td>0.68 ± 0.04</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.64 ± 0.06</td>\n",
       "      <td>0.64 ± 0.07</td>\n",
       "      <td>0.64 ± 0.08</td>\n",
       "      <td>0.66 ± 0.06</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Method           f1    precision       recall     accuracy   auc\n",
       "0  Naive Bayes  0.66 ± 0.04  0.57 ± 0.05  0.78 ± 0.06  0.62 ± 0.05  0.70\n",
       "1       Voting  0.67 ± 0.03  0.60 ± 0.05  0.75 ± 0.05  0.66 ± 0.05  0.75\n",
       "2          MLP  0.66 ± 0.03  0.66 ± 0.05  0.66 ± 0.04  0.68 ± 0.04  0.74\n",
       "3     AdaBoost  0.64 ± 0.06  0.64 ± 0.07  0.64 ± 0.08  0.66 ± 0.06  0.72"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {'Naive Bayes': GaussianNB(),\n",
    "          'Voting': VotingClassifier(estimators=[('mlp', MLPClassifier()),\n",
    "                                            ('ada', AdaBoostClassifier()),\n",
    "                                            ('nb', GaussianNB())], voting='soft'),\n",
    "          'Perceptron': MLPClassifier(),\n",
    "          'AdaBoost': AdaBoostClassifier()}\n",
    "\n",
    "vals = []\n",
    "metric = []\n",
    "model_name = []\n",
    "\n",
    "f1 = []\n",
    "precision = []\n",
    "recall = []\n",
    "accuracy = []\n",
    "auc = []\n",
    "method = ['Naive Bayes', 'Voting', 'MLP', 'AdaBoost']\n",
    "\n",
    "cv = 10\n",
    "for k,v in models.items():\n",
    "    stats = get_stats(v, dfv, labels, cv)\n",
    "    accuracy_avg = np.average(stats['test_accuracy'])\n",
    "    accuracy_std = np.std(stats['test_accuracy'])\n",
    "    precision_avg = np.average(stats['test_precision'])\n",
    "    precision_std = np.std(stats['test_precision'])\n",
    "    recall_avg = np.average(stats['test_recall'])\n",
    "    recall_std = np.std(stats['test_recall'])\n",
    "    f1_avg = np.average(stats['test_f1'])\n",
    "    f1_std = np.std(stats['test_f1'])\n",
    "    auc_avg = np.average(stats['test_roc_auc'])\n",
    "    \n",
    "    f1.append('%.2f ± %.2f' % (f1_avg, f1_std))\n",
    "    precision.append('%.2f ± %.2f' % (precision_avg, precision_std))\n",
    "    recall.append('%.2f ± %.2f' % (recall_avg, recall_std))\n",
    "    accuracy.append('%.2f ± %.2f' % (accuracy_avg, accuracy_std))\n",
    "    auc.append('%.2f' % auc_avg)\n",
    "\n",
    "df_view = pd.DataFrame(data={'Method': method, 'f1': f1, \n",
    "                             'precision':precision, 'recall':recall,\n",
    "                             'accuracy':accuracy, 'auc':auc})\n",
    "df_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364956, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>Ok. Houston, stay home. Stay out of the attic....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>Trump killed flood safety rules 10 days before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>@funder @KariannHart Take that fucking wall do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>water vapor satellite image shows extensive dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>Alcohol helps cause traffic accidents.\\nSmokin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "1500  Ok. Houston, stay home. Stay out of the attic....\n",
       "1501  Trump killed flood safety rules 10 days before...\n",
       "1502  @funder @KariannHart Take that fucking wall do...\n",
       "1503  water vapor satellite image shows extensive dr...\n",
       "1504  Alcohol helps cause traffic accidents.\\nSmokin..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now generate a dataset for MTurk from the subset of 300k tweets\n",
    "df_subset = pd.read_csv('data/shuffled_subset_all.csv', lineterminator='\\n').drop(['Unnamed: 0'], axis=1).loc[1500:,:]\n",
    "print(df_subset.shape)\n",
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320472, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>Ok. Houston, stay home. Stay out of the attic....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>Trump killed flood safety rules 10 days before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>@funder @KariannHart Take that fucking wall do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>water vapor satellite image shows extensive dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>Alcohol helps cause traffic accidents.\\nSmokin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "1500  Ok. Houston, stay home. Stay out of the attic....\n",
       "1501  Trump killed flood safety rules 10 days before...\n",
       "1502  @funder @KariannHart Take that fucking wall do...\n",
       "1503  water vapor satellite image shows extensive dr...\n",
       "1504  Alcohol helps cause traffic accidents.\\nSmokin..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a crude way to filter out some donation tweets\n",
    "prune_indices = df_subset[df_subset['text'].str.contains('donat')].index.values.tolist()\n",
    "df_subset = df_subset.drop(prune_indices)\n",
    "print(df_subset.shape)\n",
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "#now get rid of non-English stuff\n",
    "drop_indices =[]\n",
    "for index, row in enumerate(df_subset.itertuples()):\n",
    "    \n",
    "    if index >= 20000:\n",
    "        break\n",
    "    elif index % 10000 == 0:\n",
    "        print(index)\n",
    "    \n",
    "    lang = detect(row.text)\n",
    "    if lang !='en' and lang != 'nl':\n",
    "        drop_indices.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320373, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now prune the tweets that are not English\n",
    "df_english = df_subset.drop(df_subset.index[drop_indices])\n",
    "df_english.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>Ok. Houston, stay home. Stay out of the attic....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>Trump killed flood safety rules 10 days before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>@funder @KariannHart Take that fucking wall do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>water vapor satellite image shows extensive dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>Alcohol helps cause traffic accidents.\\nSmokin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "1500  Ok. Houston, stay home. Stay out of the attic....\n",
       "1501  Trump killed flood safety rules 10 days before...\n",
       "1502  @funder @KariannHart Take that fucking wall do...\n",
       "1503  water vapor satellite image shows extensive dr...\n",
       "1504  Alcohol helps cause traffic accidents.\\nSmokin..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english_small = df_english.iloc[:20000,:]\n",
    "print(df_english_small.shape)\n",
    "df_english_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>Ok. Houston, stay home. Stay out of the attic....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>Trump killed flood safety rules 10 days before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>@funder @KariannHart Take that fucking wall do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>water vapor satellite image shows extensive dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>Alcohol helps cause traffic accidents.\\nSmokin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "1500  Ok. Houston, stay home. Stay out of the attic....\n",
       "1501  Trump killed flood safety rules 10 days before...\n",
       "1502  @funder @KariannHart Take that fucking wall do...\n",
       "1503  water vapor satellite image shows extensive dr...\n",
       "1504  Alcohol helps cause traffic accidents.\\nSmokin..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')\n",
    "df_noemoji = pd.DataFrame(data={'text' : df_english_small['text'].map(deEmojify)})\n",
    "print(df_noemoji.shape)\n",
    "df_noemoji.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ok houston stay home stay attic call &lt;number&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trump kill flood safety rule &lt;number&gt; days har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; take fuck wall &lt;user&gt; hurricaneh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>water vapor satellite image show extensive dry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alcohol help cause traffic accidents smoke hel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  ok houston stay home stay attic call <number> ...\n",
       "1  trump kill flood safety rule <number> days har...\n",
       "2  <user> <user> take fuck wall <user> hurricaneh...\n",
       "3  water vapor satellite image show extensive dry...\n",
       "4  alcohol help cause traffic accidents smoke hel..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now preprocess the text\n",
    "df_test = df_noemoji.copy().reset_index(drop=True)\n",
    "df_test['Text'] = df_test['text'].map(lambda x: process_tweet(x, tknzr, None, True))\n",
    "df_test.pop('text')\n",
    "\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v40</th>\n",
       "      <th>v41</th>\n",
       "      <th>v42</th>\n",
       "      <th>v43</th>\n",
       "      <th>v44</th>\n",
       "      <th>v45</th>\n",
       "      <th>v46</th>\n",
       "      <th>v47</th>\n",
       "      <th>v48</th>\n",
       "      <th>v49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.043563</td>\n",
       "      <td>0.168890</td>\n",
       "      <td>0.132577</td>\n",
       "      <td>-0.318665</td>\n",
       "      <td>-0.148289</td>\n",
       "      <td>0.082531</td>\n",
       "      <td>0.732835</td>\n",
       "      <td>0.150738</td>\n",
       "      <td>0.097140</td>\n",
       "      <td>-0.027730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.726723</td>\n",
       "      <td>0.383673</td>\n",
       "      <td>0.364157</td>\n",
       "      <td>-0.003794</td>\n",
       "      <td>0.233437</td>\n",
       "      <td>-0.222859</td>\n",
       "      <td>0.229726</td>\n",
       "      <td>-0.269067</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.182357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.111794</td>\n",
       "      <td>-0.194317</td>\n",
       "      <td>-0.372960</td>\n",
       "      <td>-0.037570</td>\n",
       "      <td>-0.161805</td>\n",
       "      <td>0.327506</td>\n",
       "      <td>0.018590</td>\n",
       "      <td>0.199540</td>\n",
       "      <td>-0.131544</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.539943</td>\n",
       "      <td>-0.106992</td>\n",
       "      <td>0.183136</td>\n",
       "      <td>-0.061690</td>\n",
       "      <td>0.293251</td>\n",
       "      <td>-0.094791</td>\n",
       "      <td>-0.082968</td>\n",
       "      <td>-0.185012</td>\n",
       "      <td>-0.121363</td>\n",
       "      <td>0.213161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.581150</td>\n",
       "      <td>0.288926</td>\n",
       "      <td>0.154654</td>\n",
       "      <td>-0.169325</td>\n",
       "      <td>0.123784</td>\n",
       "      <td>0.187431</td>\n",
       "      <td>0.353618</td>\n",
       "      <td>0.311923</td>\n",
       "      <td>0.172137</td>\n",
       "      <td>0.650069</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.555811</td>\n",
       "      <td>-0.098209</td>\n",
       "      <td>0.217706</td>\n",
       "      <td>0.436886</td>\n",
       "      <td>0.221757</td>\n",
       "      <td>-0.083573</td>\n",
       "      <td>-0.025337</td>\n",
       "      <td>0.116973</td>\n",
       "      <td>-0.093931</td>\n",
       "      <td>-0.081701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.213230</td>\n",
       "      <td>-0.330857</td>\n",
       "      <td>0.081841</td>\n",
       "      <td>-0.154605</td>\n",
       "      <td>-0.170297</td>\n",
       "      <td>0.227414</td>\n",
       "      <td>0.515372</td>\n",
       "      <td>-0.379785</td>\n",
       "      <td>0.105189</td>\n",
       "      <td>-0.330809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.582300</td>\n",
       "      <td>-0.138565</td>\n",
       "      <td>-0.083400</td>\n",
       "      <td>-0.111499</td>\n",
       "      <td>0.082342</td>\n",
       "      <td>0.159363</td>\n",
       "      <td>0.187800</td>\n",
       "      <td>-0.086185</td>\n",
       "      <td>0.011892</td>\n",
       "      <td>0.233976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.094960</td>\n",
       "      <td>0.266487</td>\n",
       "      <td>-0.848661</td>\n",
       "      <td>-0.214325</td>\n",
       "      <td>0.030654</td>\n",
       "      <td>0.126114</td>\n",
       "      <td>0.402066</td>\n",
       "      <td>-0.261091</td>\n",
       "      <td>0.113748</td>\n",
       "      <td>0.007945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373645</td>\n",
       "      <td>-0.058034</td>\n",
       "      <td>0.059335</td>\n",
       "      <td>-0.090298</td>\n",
       "      <td>0.071710</td>\n",
       "      <td>-0.197974</td>\n",
       "      <td>0.188452</td>\n",
       "      <td>0.468361</td>\n",
       "      <td>-0.197567</td>\n",
       "      <td>0.282665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v0        v1        v2        v3        v4        v5        v6  \\\n",
       "0 -0.043563  0.168890  0.132577 -0.318665 -0.148289  0.082531  0.732835   \n",
       "1  0.198020  0.111794 -0.194317 -0.372960 -0.037570 -0.161805  0.327506   \n",
       "2  0.581150  0.288926  0.154654 -0.169325  0.123784  0.187431  0.353618   \n",
       "3 -0.213230 -0.330857  0.081841 -0.154605 -0.170297  0.227414  0.515372   \n",
       "4  0.094960  0.266487 -0.848661 -0.214325  0.030654  0.126114  0.402066   \n",
       "\n",
       "         v7        v8        v9    ...          v40       v41       v42  \\\n",
       "0  0.150738  0.097140 -0.027730    ...    -0.726723  0.383673  0.364157   \n",
       "1  0.018590  0.199540 -0.131544    ...    -0.539943 -0.106992  0.183136   \n",
       "2  0.311923  0.172137  0.650069    ...    -1.555811 -0.098209  0.217706   \n",
       "3 -0.379785  0.105189 -0.330809    ...    -0.582300 -0.138565 -0.083400   \n",
       "4 -0.261091  0.113748  0.007945    ...    -0.373645 -0.058034  0.059335   \n",
       "\n",
       "        v43       v44       v45       v46       v47       v48       v49  \n",
       "0 -0.003794  0.233437 -0.222859  0.229726 -0.269067  0.002645  0.182357  \n",
       "1 -0.061690  0.293251 -0.094791 -0.082968 -0.185012 -0.121363  0.213161  \n",
       "2  0.436886  0.221757 -0.083573 -0.025337  0.116973 -0.093931 -0.081701  \n",
       "3 -0.111499  0.082342  0.159363  0.187800 -0.086185  0.011892  0.233976  \n",
       "4 -0.090298  0.071710 -0.197974  0.188452  0.468361 -0.197567  0.282665  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now convert to word embeddings\n",
    "df_testv = tweets_to_df(df_test, [0]*df_test.shape[0], embeddings, glove).reset_index(drop=True)\n",
    "df_testv.pop('class')\n",
    "df_testv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now run the model\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(dfv, labels)\n",
    "pred = mlp.predict(df_testv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looked through index 10783\n",
      "successful\n"
     ]
    }
   ],
   "source": [
    "#now run until we have 5000 positives and 5000 negatives\n",
    "pos = 0\n",
    "neg = 0\n",
    "indices = []\n",
    "success = False\n",
    "\n",
    "for index in range(len(pred)):\n",
    "    p = pred[index]\n",
    "    if pos == 2400 and neg == 1600:\n",
    "        success = True\n",
    "        print('looked through index ' + str(index))\n",
    "        break\n",
    "    elif p == 0:\n",
    "        if neg < 1600:\n",
    "            indices.append(index)\n",
    "            neg += 1\n",
    "    else:\n",
    "        if pos < 2400:\n",
    "            indices.append(index)\n",
    "            pos += 1\n",
    "\n",
    "print('successful' if success else 'failure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ok. Houston, stay home. Stay out of the attic....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump killed flood safety rules 10 days before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@funder @KariannHart Take that fucking wall do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>water vapor satellite image shows extensive dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alcohol helps cause traffic accidents.\\nSmokin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@TXInstruments @tenethealth @jcpenney can we k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How horrible, look how high the water is on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Remember those we lost, those who helped us co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How @SpringISD is helping its community recove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>For over 100 yrs @VOATexas has served those in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#Harvey aftermath: \"More flooding threatens #B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Before leaving to help or evacuate use https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>If #media had half an ounce of sense they woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Catastrophic flooding underway in Houston and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hey #Florida TAG you're it \\n#hoUSton #houston...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Shout out to the caring employees in San Anton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@CBSBigBrother @CBS  when will this episode of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I'm not home, but keep torturing myself with #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>To give y'all some context of the flooding fol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>#Harvey has dumped more than 11 trillion gallo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Man my mom went all Gangsta on me today re: #H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>How to build #resilient #infrastructure to fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"Flood-prone Houston at the mercy of Hurricane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Looking to help Hurricane Harvey &amp;amp; Eagle C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>#HoustonFloods #HarveyFlood @UnderbellyHOU #fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>@DrGregBonnen How do you sleep at night now, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Those in the path of #Harvey\\n in #Texas, plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>#BigNews in public safety: Montgomery rescue c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Volunteers needed at 14 sites for @TXAdoptABea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Humanitarian support for #HoustonFloods on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3970</th>\n",
       "      <td>So proud of my #Texas brethren brown black whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>#houstonflood Text from my sister, just now. S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3972</th>\n",
       "      <td>@TheTruth24US Insane that Sharks have found th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3973</th>\n",
       "      <td>VibeMagazine: Pimp C's son reportedly stranded...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3974</th>\n",
       "      <td>For context; he's a cartoonist for @politico h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3975</th>\n",
       "      <td>@KIII3News @KRIS6News @Action10News @Callerdot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3976</th>\n",
       "      <td>If you haven't already, please consider giving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>#Harvey  Please RT  New rescue map https://t.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3978</th>\n",
       "      <td>UT supercomputer tracking Harvey's flood risk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>If you are looking for a place to take your ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>Please help my fellow Texans who have been aff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>.@JoelOsteen, meet #FrankZappa\\nhttps://t.co/q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3982</th>\n",
       "      <td>Sad news. According to @HoustonChron: #Harvey ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3983</th>\n",
       "      <td>Anither urgent call fir help. Sick kids, no wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3984</th>\n",
       "      <td>This is where #Houstonflooding and #FlintWater...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3985</th>\n",
       "      <td>Can you imagine a year's worth of rain in a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>Your city out here flooded and you buying a ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987</th>\n",
       "      <td>It's been 2 and a half days since we left our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>#Houston #Houstonflood https://t.co/AllfqMHaOd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>@BrandenHarvey Hurricane Harvey might cause di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>Stamps QB Bo Levi Mitchell leads team onto the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>#TedCruz shld hold off Trump Visit #houstonflo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>It's good #HurricaneHarvey got graded to a lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>Raging bayou at the end of the street  #harvey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3994</th>\n",
       "      <td>I can't even go to sleep, I'm too afraid  #hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>Please send good thoughts and prayers to @Alie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>I cannot tell you the visceral reaction I had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>There are lots of worst photos, but this one i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>Ron Paul from his home in the Houston area: \"W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>#CorpusChristi #HurricaneHarvey area and are g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     Ok. Houston, stay home. Stay out of the attic....\n",
       "1     Trump killed flood safety rules 10 days before...\n",
       "2     @funder @KariannHart Take that fucking wall do...\n",
       "3     water vapor satellite image shows extensive dr...\n",
       "4     Alcohol helps cause traffic accidents.\\nSmokin...\n",
       "5     @TXInstruments @tenethealth @jcpenney can we k...\n",
       "6     How horrible, look how high the water is on th...\n",
       "7     Remember those we lost, those who helped us co...\n",
       "8     How @SpringISD is helping its community recove...\n",
       "9     For over 100 yrs @VOATexas has served those in...\n",
       "10    #Harvey aftermath: \"More flooding threatens #B...\n",
       "11    Before leaving to help or evacuate use https:/...\n",
       "12    If #media had half an ounce of sense they woul...\n",
       "13    Catastrophic flooding underway in Houston and ...\n",
       "14    Hey #Florida TAG you're it \\n#hoUSton #houston...\n",
       "15    Shout out to the caring employees in San Anton...\n",
       "16    @CBSBigBrother @CBS  when will this episode of...\n",
       "17    I'm not home, but keep torturing myself with #...\n",
       "18    To give y'all some context of the flooding fol...\n",
       "19    #Harvey has dumped more than 11 trillion gallo...\n",
       "20    Man my mom went all Gangsta on me today re: #H...\n",
       "21    How to build #resilient #infrastructure to fac...\n",
       "22    \"Flood-prone Houston at the mercy of Hurricane...\n",
       "23    Looking to help Hurricane Harvey &amp; Eagle C...\n",
       "24    #HoustonFloods #HarveyFlood @UnderbellyHOU #fo...\n",
       "25    @DrGregBonnen How do you sleep at night now, k...\n",
       "26    Those in the path of #Harvey\\n in #Texas, plea...\n",
       "27    #BigNews in public safety: Montgomery rescue c...\n",
       "28    Volunteers needed at 14 sites for @TXAdoptABea...\n",
       "29    Humanitarian support for #HoustonFloods on the...\n",
       "...                                                 ...\n",
       "3970  So proud of my #Texas brethren brown black whi...\n",
       "3971  #houstonflood Text from my sister, just now. S...\n",
       "3972  @TheTruth24US Insane that Sharks have found th...\n",
       "3973  VibeMagazine: Pimp C's son reportedly stranded...\n",
       "3974  For context; he's a cartoonist for @politico h...\n",
       "3975  @KIII3News @KRIS6News @Action10News @Callerdot...\n",
       "3976  If you haven't already, please consider giving...\n",
       "3977  #Harvey  Please RT  New rescue map https://t.c...\n",
       "3978  UT supercomputer tracking Harvey's flood risk ...\n",
       "3979  If you are looking for a place to take your ho...\n",
       "3980  Please help my fellow Texans who have been aff...\n",
       "3981  .@JoelOsteen, meet #FrankZappa\\nhttps://t.co/q...\n",
       "3982  Sad news. According to @HoustonChron: #Harvey ...\n",
       "3983  Anither urgent call fir help. Sick kids, no wa...\n",
       "3984  This is where #Houstonflooding and #FlintWater...\n",
       "3985  Can you imagine a year's worth of rain in a fe...\n",
       "3986  Your city out here flooded and you buying a ba...\n",
       "3987  It's been 2 and a half days since we left our ...\n",
       "3988     #Houston #Houstonflood https://t.co/AllfqMHaOd\n",
       "3989  @BrandenHarvey Hurricane Harvey might cause di...\n",
       "3990  Stamps QB Bo Levi Mitchell leads team onto the...\n",
       "3991  #TedCruz shld hold off Trump Visit #houstonflo...\n",
       "3992  It's good #HurricaneHarvey got graded to a lev...\n",
       "3993  Raging bayou at the end of the street  #harvey...\n",
       "3994  I can't even go to sleep, I'm too afraid  #hou...\n",
       "3995  Please send good thoughts and prayers to @Alie...\n",
       "3996  I cannot tell you the visceral reaction I had ...\n",
       "3997  There are lots of worst photos, but this one i...\n",
       "3998  Ron Paul from his home in the Houston area: \"W...\n",
       "3999  #CorpusChristi #HurricaneHarvey area and are g...\n",
       "\n",
       "[4000 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now make the final dataframe containing 10000 tweets that are relevant and not relevant\n",
    "df_mturk = df_noemoji.reset_index(drop=True).loc[indices].reset_index(drop=True)\n",
    "df_mturk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mturk.to_csv('data/mturk_final4000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
