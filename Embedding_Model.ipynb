{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ashwin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>Urgency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>247434</td>\n",
       "      <td>More millions in #Afghanistan even with ZERO a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>294115</td>\n",
       "      <td>These are the last post my brother made on soc...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24622</td>\n",
       "      <td>In @cityofcc listening to local officials abou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37807</td>\n",
       "      <td>So so so damn proud of @5ugarcane who is tirel...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37386</td>\n",
       "      <td>How can you help with #Harvey disaster respons...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                               Text  Relevancy  \\\n",
       "0  247434  More millions in #Afghanistan even with ZERO a...          0   \n",
       "1  294115  These are the last post my brother made on soc...          2   \n",
       "2   24622  In @cityofcc listening to local officials abou...          0   \n",
       "3   37807  So so so damn proud of @5ugarcane who is tirel...          3   \n",
       "4   37386  How can you help with #Harvey disaster respons...          0   \n",
       "\n",
       "   Urgency  \n",
       "0        0  \n",
       "1        1  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/labeled_prelim.csv')\n",
    "df = df.dropna()\n",
    "df = df.astype({'Id':np.int32, 'Relevancy':np.int32, 'Urgency':np.int32})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_symspell(max_edit_distance, prefix_length, freq_file_path):\n",
    "    \n",
    "    # create object\n",
    "    sym_spell = SymSpell(max_edit_distance, prefix_length)\n",
    "    \n",
    "    # create dictionary using corpus.txt\n",
    "    if not sym_spell.create_dictionary(freq_file_path):\n",
    "        print(\"Corpus file not found\")\n",
    "        return None\n",
    "    \n",
    "    return sym_spell\n",
    "\n",
    "def process_tweet(tweet, tknzr, sym_spell):\n",
    "    st_1 = []\n",
    "    for w in tknzr.tokenize(tweet):\n",
    "        #remove retweet annotation if present:\n",
    "        if w == 'RT':\n",
    "            continue\n",
    "        #remove hashtag symbol\n",
    "        elif w[0] == '#':\n",
    "            st_1.append(w[1:])\n",
    "        #replace link with LINK keyword\n",
    "        elif w[:4] == 'http':\n",
    "            st_1.append('link')\n",
    "        else:\n",
    "            st_1.append(w)\n",
    "    \n",
    "    #remove stop words and punctuation, make everything lowercase\n",
    "    #st_2 = [sym_spell.word_segmentation(w.lower()).corrected_string \n",
    "    #            for w in st_1 if w.isalpha() and not w.lower() in stop_words]\n",
    "    \n",
    "    st_2 = [w.lower() for w in st_1 if w.isalpha() and not w.lower() in stop_words]\n",
    "    \n",
    "    #lemmatization (converts all words to root form for standardization)\n",
    "    lem = WordNetLemmatizer()\n",
    "    st_3 = list(map(lambda x: lem.lemmatize(x, pos='v'), st_2))\n",
    "    \n",
    "    #now do word segmentation/spell check\n",
    "    return ' '.join(st_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sym_spell = create_symspell(2,7,'data/frequency_dictionary_en_82_765.txt')\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'millions afghanistan even zero attack isis sympathizers invest texas nation build harvey texasflood'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_sample = df['Text'][0]\n",
    "process_tweet(doc_sample, tknzr, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty line\n"
     ]
    }
   ],
   "source": [
    "#list of embeddings\n",
    "vec_length = 25\n",
    "embeddings = np.zeros((1193514, vec_length))\n",
    "\n",
    "#two-way map, index->word and word->index\n",
    "glove = {}\n",
    "\n",
    "index = 0\n",
    "with open('data/glove.twitter.27B/glove.twitter.27B.%dd.txt' % vec_length) as f:\n",
    "    for l in f:\n",
    "        line = []\n",
    "        try:\n",
    "            line = l.split()\n",
    "            if len(line) != vec_length+1:\n",
    "                print('empty line')\n",
    "                continue\n",
    "            \n",
    "            word = line[0]\n",
    "            embeddings[index] = np.array(line[1:]).astype(np.float)\n",
    "            glove[index] = word\n",
    "            glove[word] = index\n",
    "            index += 1\n",
    "        except:\n",
    "            print(line)\n",
    "            print(index)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.5807   -1.0965   -0.59056   1.1178   -0.30615  -0.44198  -1.377\n",
      " -2.3494    2.0436   -0.15692   2.6962    1.033     0.81358  -1.7224\n",
      "  0.066939 -0.71714   1.0608   -0.43463   2.1178    0.65876   0.62825\n",
      " -1.2018    1.7123    0.79867   0.32424 ]\n",
      "if\n",
      "74\n",
      "[ 0.18243  0.70534 -0.34209 -0.10779 -0.72721 -0.58802  1.7457  -0.13666\n",
      " -0.61576  0.15336 -0.19019  0.70282 -5.725   -0.20901 -0.33692  0.16916\n",
      "  0.35872 -0.9871   0.45495 -0.36607  0.62973  0.11066  0.31315  0.08787\n",
      " -0.88679]\n"
     ]
    }
   ],
   "source": [
    "#sanity checks\n",
    "print(embeddings[1193512])\n",
    "print(glove[74])\n",
    "print(glove['if'])\n",
    "print(embeddings[74])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert columns to numpy arrays\n",
    "text = df['Text'].values\n",
    "relevancy = df['Relevancy'].values\n",
    "urgency = df['Urgency'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevancyClassifier(nn.Module):\n",
    "    def __init__(self, index, embeddings, embed_len, num_classes):\n",
    "        super(RelevancyClassifier, self).__init__()\n",
    "        self.hidden_size = 30\n",
    "        self.embed_len = embed_len\n",
    "        #print(embeddings.shape)\n",
    "        #self.embedding = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.fc1 = nn.Linear(embed_len, self.hidden_size)\n",
    "        self.nl = nn.LeakyReLU()\n",
    "        self.fc2 = nn.Linear(self.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.nl(self.fc1(x)))\n",
    "\n",
    "\n",
    "def train_relevancy_classifier(train_exs, train_labels, embeddings, index):\n",
    "    try:\n",
    "        print(type(train_exs))\n",
    "        epochs = 10\n",
    "        lr = .0001\n",
    "        num_classes = 2\n",
    "        rc = RelevancyClassifier(index, embeddings, len(embeddings[0]), num_classes)\n",
    "        optimizer = optim.Adam(rc.parameters(), lr=lr)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            ex_indices = [i for i in range(len(train_exs))]\n",
    "            random.shuffle(ex_indices)\n",
    "            total_loss = 0.0\n",
    "            num_tweets = len(ex_indices)\n",
    "            for idx in ex_indices:\n",
    "                #cur_idx = ex_indices[idx]\n",
    "                cur_tweet = train_exs[idx]\n",
    "                cur_embed = []\n",
    "                for i in cur_tweet.split():\n",
    "                    if i in index:\n",
    "                        cur_embed.append(embeddings[index[i]])\n",
    "                if len(cur_embed) == 0:\n",
    "                    num_tweets -= 1\n",
    "                    continue\n",
    "                x = torch.from_numpy(np.asarray(np.mean(cur_embed, axis=0)).reshape(1,25)).float()\n",
    "                #print(x.shape)\n",
    "                y = np.asarray(train_labels[idx]).reshape(1)\n",
    "                if y[0] > 0:\n",
    "                    y[0] = 1\n",
    "                y = torch.tensor(y).long()\n",
    "                #print(y.shape)\n",
    "                rc.zero_grad()\n",
    "                probs = rc.forward(x)\n",
    "                cur_loss = loss(probs, y)\n",
    "                total_loss += cur_loss\n",
    "                cur_loss.backward()\n",
    "                optimizer.step()\n",
    "            print(\"Avg loss on epoch %i: %f\" % (epoch, total_loss/num_tweets))\n",
    "        return rc\n",
    "    except KeyboardInterrupt:\n",
    "        return rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = df['Text'].map(lambda x: process_tweet(x, tknzr, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(processed_tweets.values, urgency, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Avg loss on epoch 0: 0.475863\n",
      "Avg loss on epoch 1: 0.323312\n",
      "Avg loss on epoch 2: 0.310519\n",
      "Avg loss on epoch 3: 0.305674\n",
      "Avg loss on epoch 4: 0.301835\n",
      "Avg loss on epoch 5: 0.298196\n",
      "Avg loss on epoch 6: 0.295057\n",
      "Avg loss on epoch 7: 0.291967\n",
      "Avg loss on epoch 8: 0.289537\n",
      "Avg loss on epoch 9: 0.288159\n"
     ]
    }
   ],
   "source": [
    "model = train_relevancy_classifier(X_train, y_train, embeddings, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8922764227642277\n"
     ]
    }
   ],
   "source": [
    "val_tweets = X_test\n",
    "val_labels = y_test\n",
    "for i in range(len(val_labels)):\n",
    "    if val_labels[i] > 0:\n",
    "        val_labels[i] = 1\n",
    "num_correct = 0\n",
    "for i in range(len(val_tweets)):\n",
    "    cur_embed = []\n",
    "    cur_tweet = val_tweets[i]\n",
    "    cur_label = val_labels[i]\n",
    "    for i in cur_tweet.split():\n",
    "        if i in glove:\n",
    "            cur_embed.append(embeddings[glove[i]])\n",
    "    if len(cur_embed) == 0:\n",
    "        #num_tweets -= 1\n",
    "        continue\n",
    "    x = torch.from_numpy(np.asarray(np.mean(cur_embed, axis=0)).reshape(1,25)).float()\n",
    "    probs = model.forward(x).detach().numpy().reshape(2)\n",
    "#     print(probs)\n",
    "#     print(np.argmax(probs))\n",
    "#     print(val_labels[i])\n",
    "    if np.argmax(probs) == cur_label:\n",
    "        num_correct += 1\n",
    "print(num_correct/len(val_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
