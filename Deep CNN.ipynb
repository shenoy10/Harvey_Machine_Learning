{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ashwin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ashwin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "import process_tweet\n",
    "import importlib\n",
    "importlib.reload(process_tweet)\n",
    "\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty line\n"
     ]
    }
   ],
   "source": [
    "#read in the word embeddings\n",
    "vec_length = 100\n",
    "embeddings = np.zeros((1193514+2, vec_length))\n",
    "\n",
    "#two-way map, index->word and word->index\n",
    "glove = {}\n",
    "\n",
    "#add special tokens for unknown and padding\n",
    "embeddings[0] = np.zeros(vec_length)\n",
    "glove[0] = 'UNK'\n",
    "glove['UNK'] = 0\n",
    "\n",
    "embeddings[1] = np.zeros(vec_length)\n",
    "glove[1] = 'PAD'\n",
    "glove['PAD'] = 1\n",
    "\n",
    "index = 2\n",
    "with open('data/glove.twitter.27B/glove.twitter.27B.%dd.txt' % vec_length) as f:\n",
    "    for l in f:\n",
    "        line = []\n",
    "        try:\n",
    "            line = l.split()\n",
    "            if len(line) != vec_length+1:\n",
    "                print('empty line')\n",
    "                continue\n",
    "            \n",
    "            word = line[0]\n",
    "            embeddings[index] = np.array(line[1:]).astype(np.float)\n",
    "            glove[index] = word\n",
    "            glove[word] = index\n",
    "            index += 1\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4078, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>Urgency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That #HarveyStorm water is waist deep. My resp...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Find out how you help those affected by Hurric...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#MountainView heroes deployed to help with #Ha...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To help those impacted by #HurricaneHarvey, we...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So much flooding in Houston. Wow! Just tuning ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Relevancy  Urgency\n",
       "0  That #HarveyStorm water is waist deep. My resp...          0        0\n",
       "1  Find out how you help those affected by Hurric...          1        0\n",
       "2  #MountainView heroes deployed to help with #Ha...          3        0\n",
       "3  To help those impacted by #HurricaneHarvey, we...          0        0\n",
       "4  So much flooding in Houston. Wow! Just tuning ...          0        0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the dataset\n",
    "df = pd.read_csv('data/final_dataset.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>Urgency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>harveystorm water waist deep respect cop help ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>find help affect hurricane harvey yeg hurrican...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mountainview heroes deploy help harvey &lt;url&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>help impact hurricaneharvey weave activate don...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>much flood houston wow tune news prayers sympa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Relevancy  Urgency\n",
       "0  harveystorm water waist deep respect cop help ...          0        0\n",
       "1  find help affect hurricane harvey yeg hurrican...          1        0\n",
       "2       mountainview heroes deploy help harvey <url>          3        0\n",
       "3  help impact hurricaneharvey weave activate don...          0        0\n",
       "4  much flood houston wow tune news prayers sympa...          0        0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess the tweets\n",
    "sym_spell = process_tweet.create_symspell(2,7,'data/frequency_dictionary_en_82_765.txt')\n",
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "df['Text'] = df['Text'].map(lambda x: process_tweet.process_tweet(x, glove, tknzr, sym_spell, True))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.035785945166337485\n",
      "735\n"
     ]
    }
   ],
   "source": [
    "#now convert the tweets into a list of indices\n",
    "X = []\n",
    "unk_percent = []\n",
    "unk_words = set()\n",
    "max_len = 0\n",
    "for tweet in df['Text']:\n",
    "    indices = []\n",
    "    words = tweet.split()\n",
    "    if len(words) > max_len:\n",
    "        max_len = len(words)\n",
    "    unknown = 0\n",
    "    for word in words:\n",
    "        if word in glove:\n",
    "            indices.append(glove[word])\n",
    "        else:\n",
    "            indices.append(glove['UNK'])\n",
    "            unk_words.add(word)\n",
    "            unknown += 1\n",
    "        unk_percent.append(unknown/len(words))\n",
    "    X.append(indices)\n",
    "\n",
    "# add padding to make every tweet the same length\n",
    "for i in range(len(X)):\n",
    "    tweet = X[i]\n",
    "    if len(tweet) < max_len:\n",
    "        tweet = np.append(tweet, np.ones(max_len - len(tweet)))\n",
    "    X[i] = tweet\n",
    "\n",
    "X = np.asarray(X, dtype=np.int64)\n",
    "y = np.array(list(map(lambda x: 1 if x > 0 else 0, df['Relevancy'].values)), dtype=np.int64)\n",
    "print(np.mean(unk_percent))\n",
    "print(len(unk_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This architecture is inspired by the one used in the paper\n",
    "'Twitter Sentiment Analysis with Deep Convolutional Neural Networks' (Severyn et al., 2015)\n",
    "\"\"\"\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, embeddings, n_filters, filter_sizes, n_classes, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #length of the word embeddings\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        \n",
    "        #architecture\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings)\n",
    "        \n",
    "        self.conv_0 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[0], embedding_dim))\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[1], embedding_dim))\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[2], embedding_dim))\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, n_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, tweet_indices):\n",
    "        \n",
    "        embedded = self.embedding(tweet_indices)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
    "        \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "        \n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim = 1))\n",
    "        \n",
    "        return self.softmax(self.fc(cat))\n",
    "    \n",
    "    def predict(self, tweet):\n",
    "        return np.argmax(self.forward(tweet).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X_train: 2d np array, where each row is the indices corresponding to each word of a specific tweet\n",
    "y_train: 1d np array of same length as X_train with 0/1 based on relevant/not relevant or urgent/not urgent\n",
    "embeddings: GloVe word embeddings created above\n",
    "\"\"\"\n",
    "def train_cnn_classifier(X_train, y_train, embeddings, num_classes, manual_params=None, verbose=False):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        embeddings = torch.from_numpy(embeddings).float()\n",
    "        embed_len = len(embeddings[0])\n",
    "        seq_len = len(X_train[0])\n",
    "        \n",
    "        #default parameters for the model\n",
    "        params = {'batch_size': 10, 'epochs': 50, 'lr': 0.0001, 'n_filters': 100, 'filter_sizes': [3,4,5],\n",
    "                 'dropout': 0.5}\n",
    "        \n",
    "        #replace default parameters with any user-defined ones\n",
    "        if manual_params is not None:\n",
    "            for p in manual_params:\n",
    "                params[p] = manual_params[p]\n",
    "                \n",
    "        batch_size = params['batch_size']\n",
    "        epochs = params['epochs']\n",
    "        lr = params['lr']\n",
    "        \n",
    "        #initialize network and optimizer\n",
    "        cnn = CNN(embeddings, n_filters=params['n_filters'], filter_sizes=params['filter_sizes'], \n",
    "                n_classes=num_classes, dropout=params['dropout'])\n",
    "        optimizer = optim.Adam(cnn.parameters(), lr=lr)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        cnn.train()\n",
    "        for epoch in range(epochs):\n",
    "            ex_indices = [i for i in range(len(X_train))]\n",
    "            random.shuffle(ex_indices)\n",
    "            total_loss = 0.0\n",
    "            for idx in range(len(ex_indices)//batch_size):\n",
    "                \n",
    "                #create input batch to feed in\n",
    "                cur_batch_idx = ex_indices[idx*batch_size:(idx+1)*batch_size]\n",
    "                cur_X = torch.from_numpy(np.asarray([X_train[i] for i in cur_batch_idx])).long()\n",
    "                cur_y = torch.from_numpy(np.asarray([y_train[i] for i in cur_batch_idx]))\n",
    "                \n",
    "                #train\n",
    "                cnn.zero_grad()\n",
    "                probs = cnn.forward(cur_X)\n",
    "                \n",
    "                #calculate loss and update weights\n",
    "                cur_loss = loss(probs, cur_y)\n",
    "                total_loss += cur_loss\n",
    "                cur_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Avg loss on epoch %i: %f\" % (epoch+1, total_loss/len(ex_indices)))\n",
    "        end = time.time()\n",
    "        print(\"Time taken: %f seconds\" % (end-start))\n",
    "        return cnn\n",
    "    except KeyboardInterrupt:\n",
    "        end = time.time()\n",
    "        print(\"Time taken: %f seconds\" % (end-start))\n",
    "        return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates binary classification model\n",
    "def calc_metrics(model, X_test, y_test):\n",
    "    num_correct = 0\n",
    "    num_true_pos = 0\n",
    "    num_false_pos = 0\n",
    "    num_false_neg = 0\n",
    "    \n",
    "    num_test_exs = len(X_test)\n",
    "\n",
    "    model.eval()\n",
    "    for i in range(num_test_exs):\n",
    "        \n",
    "        cur_batch_idx = [i]\n",
    "        cur_X = torch.from_numpy(np.asarray([X_test[i] for i in cur_batch_idx])).long()\n",
    "        \n",
    "        y_pred = model.predict(cur_X)\n",
    "        y_gold = y_test[i]\n",
    "        if y_pred == y_gold:\n",
    "            num_correct += 1\n",
    "            if y_gold > 0:\n",
    "                num_true_pos += 1\n",
    "        else:\n",
    "            if y_pred == 0:\n",
    "                num_false_neg += 1\n",
    "            else:\n",
    "                num_false_pos += 1\n",
    "\n",
    "    accuracy = num_correct/num_test_exs\n",
    "    precision = num_true_pos/(num_true_pos + num_false_pos)\n",
    "    recall = num_true_pos/(num_true_pos + num_false_neg)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(X, y, embeddings, manual_params=None, k=10):\n",
    "    ex_indices = list(range(X.shape[0]))\n",
    "    random.shuffle(ex_indices)\n",
    "    \n",
    "    accuracy = np.zeros(k)\n",
    "    precision = np.zeros(k)\n",
    "    recall = np.zeros(k)\n",
    "    f1 = np.zeros(k)\n",
    "    \n",
    "    #calculate the splitting scheme\n",
    "    splits = [X.shape[0]//k] * k\n",
    "    for i in range(X.shape[0] % k):\n",
    "        splits[i] += 1\n",
    "    \n",
    "    #keeps track of current location in \n",
    "    index = 0\n",
    "    for i in range(k):\n",
    "        #come up with the train-test split\n",
    "        X_test = np.asarray([X[i] for i in ex_indices[index:index+splits[i]]])\n",
    "        y_test = np.asarray([y[i] for i in ex_indices[index:index+splits[i]]])\n",
    "        \n",
    "        train_indices = ex_indices[0:index] + ex_indices[index+splits[i]:]\n",
    "        X_train = np.asarray([X[i] for i in train_indices])\n",
    "        y_train = np.asarray([y[i] for i in train_indices])\n",
    "        \n",
    "        #now train the model on this split and save the metrics\n",
    "        cnn = train_cnn_classifier(X_train, y_train, embeddings, num_classes=2, manual_params=manual_params, verbose=False)\n",
    "        \n",
    "        results = calc_metrics(cnn, X_test, y_test)\n",
    "        accuracy[i] = results['accuracy']\n",
    "        precision[i] = results['precision']\n",
    "        recall[i] = results['recall']\n",
    "        f1[i] = results['f1']\n",
    "        \n",
    "        index += splits[i]\n",
    "    \n",
    "    return {'accuracy': np.mean(accuracy), 'precision': np.mean(precision), \n",
    "           'recall': np.mean(recall), 'f1': np.mean(f1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(X, y, embeddings, params, metric='f1', k=10):\n",
    "    \n",
    "    results = []\n",
    "    keys = []\n",
    "    values = []\n",
    "    for key in params:\n",
    "        keys.append(key)\n",
    "        values.append(params[key])\n",
    "    \n",
    "    for config in product(*values):\n",
    "        p = {}\n",
    "        for i, v in enumerate(config):\n",
    "            p[keys[i]] = v\n",
    "        \n",
    "        res = kfold(X, y, embeddings, manual_params=p, k=k)\n",
    "        results.append((p, res))\n",
    "    \n",
    "    return sorted(results, reverse=True, key=lambda x: x[1][metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Relevancy CNN Classifier\n",
      "Time taken: 1.857088 seconds\n",
      "\n",
      "Relevancy metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6404160475482912,\n",
       " 'precision': 0.7235955056179775,\n",
       " 'recall': 0.47144948755490484,\n",
       " 'f1': 0.5709219858156028}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "print('Training Relevancy CNN Classifier')\n",
    "cnn = train_cnn_classifier(X_train, y_train, embeddings, 2, {'n_filters': 300})\n",
    "print('\\nRelevancy metrics:')\n",
    "calc_metrics(cnn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 134.847982 seconds\n",
      "Time taken: 105.110723 seconds\n",
      "Time taken: 91.684669 seconds\n",
      "Time taken: 93.566537 seconds\n",
      "Time taken: 98.692508 seconds\n",
      "Time taken: 84.461049 seconds\n",
      "Time taken: 85.574390 seconds\n",
      "Time taken: 89.420281 seconds\n",
      "Time taken: 89.701171 seconds\n",
      "Time taken: 90.888552 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6927355831767597,\n",
       " 'precision': 0.6899113483892558,\n",
       " 'recall': 0.6983622580028672,\n",
       " 'f1': 0.692984823435019}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold(X, y, embeddings, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#figure out region size first\n",
    "filter_sizes = []\n",
    "for i in range(10):\n",
    "    filter_sizes.append([i+1])\n",
    "filter_sizes.append([15])\n",
    "filter_sizes_search = gridsearch(X, y, embeddings, params={'filter_sizes': filter_sizes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now try multiple filters with sizes around opt\n",
    "fs_opt = filter_sizes_search[0][0]['filter_sizes'][0]\n",
    "print('optimal filter size: ' + str(fs_opt))\n",
    "\n",
    "filter_sizes2 = [[fs_opt], [fs_opt]*2, [fs_opt]*3, [fs_opt]*4]\n",
    "\n",
    "if fs_opt > 3:\n",
    "    filter_sizes2.append([fs_opt-3, fs_opt-2, fs_opt-1])\n",
    "if fs_opt > 2:\n",
    "    filter_sizes2.append([fs_opt-2, fs_opt-1, fs_opt])\n",
    "    filter_sizes2.append([fs_opt-2, fs_opt-1, fs_opt, fs_opt+1])\n",
    "    filter_sizes2.append([fs_opt-2, fs_opt-1, fs_opt, fs_opt+1, fs_opt+2])\n",
    "if fs_opt > 1:\n",
    "    filter_sizes2.append([fs_opt-1, fs_opt, fs_opt+1])\n",
    "    filter_sizes2.append([fs_opt-1, fs_opt-1, fs_opt, fs_opt])\n",
    "\n",
    "filter_sizes2.append([fs_opt, fs_opt+1, fs_opt+2])\n",
    "filter_sizes2.append([fs_opt, fs_opt, fs_opt+1, fs_opt+1])\n",
    "filter_sizes2.append([fs_opt, fs_opt+1, fs_opt+2, fs_opt+3])\n",
    "\n",
    "filter_sizes_search2 = gridsearch(X, y, embeddings, params={'filter_sizes': filter_sizes2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes_search2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_opt2 = filter_sizes_search2[0][0]['filter_sizes']\n",
    "print('optimal filter size 2: ' + str(fs_opt2))\n",
    "\n",
    "#now adjust the number of feature maps for each filter size to find best one\n",
    "n_filters = [100,200,300,400,500,600,700,1000]\n",
    "n_filters_search = gridsearch(X, y, embeddings, params={'dropout': [0.1], 'n_filters': n_filters, 'filter_sizes': [fs_opt2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters_search"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[({'dropout': 0.75, 'n_filters': 400, 'filter_sizes': [1, 1, 1]},\n",
    "  {'accuracy': 0.7170291949703714,\n",
    "   'precision': 0.7108499987233488,\n",
    "   'recall': 0.7333624974603714,\n",
    "   'f1': 0.7212108573859786}),\n",
    " ({'dropout': 0.75, 'n_filters': 500, 'filter_sizes': [1, 1, 1]},\n",
    "  {'accuracy': 0.7177458206869971,\n",
    "   'precision': 0.7168908465633443,\n",
    "   'recall': 0.7207278323401358,\n",
    "   'f1': 0.7179898960524425}),\n",
    " ({'dropout': 0.5, 'n_filters': 400, 'filter_sizes': [1, 1, 1]},\n",
    "  {'accuracy': 0.7128667437490966,\n",
    "   'precision': 0.7055637933851523,\n",
    "   'recall': 0.7293383015203121,\n",
    "   'f1': 0.7169432988044626}),\n",
    " ({'dropout': 0.75, 'n_filters': 1000, 'filter_sizes': [1, 1, 1]},\n",
    "  {'accuracy': 0.7145601483836778,\n",
    "   'precision': 0.7130406361017637,\n",
    "   'recall': 0.7211848764500807,\n",
    "   'f1': 0.7154566986638359}),\n",
    " ({'dropout': 0.75, 'n_filters': 600, 'filter_sizes': [1, 1, 1]},\n",
    "  {'accuracy': 0.7091697981403864,\n",
    "   'precision': 0.7010760122269847,\n",
    "   'recall': 0.7318993496138287,\n",
    "   'f1': 0.7150301538000413}),\n",
    " ({'dropout': 0.75, 'n_filters': 700, 'filter_sizes': [1, 1, 1]},\n",
    "  {'accuracy': 0.7133479067302597,\n",
    "   'precision': 0.7146303598598907,\n",
    "   'recall': 0.7108689921950069,\n",
    "   'f1': 0.7116313386528292}),\n",
    " ({'dropout': 0.5, 'n_filters': 600, 'filter_sizes': [1, 1, 1]},\n",
    "  {'accuracy': 0.7052494339259046,\n",
    "   'precision': 0.6969281292026123,\n",
    "   'recall': 0.7292999214516263,\n",
    "   'f1': 0.7115516522882601}),\n",
    " ({'dropout': 0.5, 'n_filters': 500, 'filter_sizes': [1, 1, 1]},\n",
    "  {'accuracy': 0.7076937900467313,\n",
    "   'precision': 0.7066785586421271,\n",
    "   'recall': 0.7170186462215405,\n",
    "   'f1': 0.7099221460137117}),\n",
    " ({'dropout': 0.75, 'n_filters': 2000, 'filter_sizes': [1, 1, 1]},\n",
    "  {'accuracy': 0.7069705400587755,\n",
    "   'precision': 0.7101332570076643,\n",
    "   'recall': 0.6965665730912763,\n",
    "   'f1': 0.7020567295273763}),\n",
    " ({'dropout': 0.5, 'n_filters': 700, 'filter_sizes': [1, 1, 1]},\n",
    "  {'accuracy': 0.6993586500939442,\n",
    "   'precision': 0.698757502737626,\n",
    "   'recall': 0.7021359354327844,\n",
    "   'f1': 0.7000158366641178}),\n",
    " ({'dropout': 0.5, 'n_filters': 1000, 'filter_sizes': [1, 1, 1]},\n",
    "  {'accuracy': 0.69788143758732,\n",
    "   'precision': 0.6968653141862784,\n",
    "   'recall': 0.7004199588989856,\n",
    "   'f1': 0.6974098473966307}),\n",
    " ({'dropout': 0.5, 'n_filters': 2000, 'filter_sizes': [1, 1, 1]},\n",
    "  {'accuracy': 0.6915082863612275,\n",
    "   'precision': 0.694685469229819,\n",
    "   'recall': 0.6852066956465658,\n",
    "   'f1': 0.6883661113572231})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters_opt = n_filters_search[0][0]['n_filters']\n",
    "dropout_opt = 0.1\n",
    "print('optimal n_filters: ' + str(n_filters_opt))\n",
    "\n",
    "# we need to increase the dropout and try again with higher values\n",
    "n_filters2 = [400,500,600,700,1000,2000]\n",
    "n_filters_search2 = gridsearch(X, y, embeddings, params={'dropout': [0.5,0.75], 'n_filters': n_filters2, 'filter_sizes': [fs_opt2]})\n",
    "print(n_filters_search2)\n",
    "\n",
    "n_filters_opt2 = n_filters_search2[0][0]['n_filters']\n",
    "print('optimal n_filters 2: ' + str(n_filters_opt2))\n",
    "if n_filters_search2[0][1]['f1'] > n_filters_search[0][1]['f1']:\n",
    "    print('new optimum!')\n",
    "    n_filters_opt = n_filters_opt2\n",
    "    dropout_opt = n_filters_search2[0][0]['dropout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now fine-tune epochs and batch size\n",
    "epochs = [10,30,50,70,100]\n",
    "batch_size = [10,30,50,100,200]\n",
    "\n",
    "final_search = gridsearch(X, y, embeddings, params={'dropout': [dropout_opt], \n",
    "                                                    'n_filters': [n_filters_opt], \n",
    "                                                    'filter_sizes': [fs_opt2],\n",
    "                                                    'epochs': epochs,\n",
    "                                                    'batch_size': batch_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
