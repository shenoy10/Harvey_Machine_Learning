{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/amitjoshi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#from symspellpy import SymSpell, Verbosity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from process_tweet import *\n",
    "\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import random\n",
    "import time\n",
    "\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>Urgency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>millions afghanistan even zero attack isis sym...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>last post brother make social media phone go v...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>listen local officials epa help harvey respons...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>damn proud tirelessly help fellow texans affec...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>help harvey disaster response help victims nat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Relevancy  Urgency\n",
       "0  millions afghanistan even zero attack isis sym...          0        0\n",
       "1  last post brother make social media phone go v...          2        1\n",
       "2  listen local officials epa help harvey respons...          0        0\n",
       "3  damn proud tirelessly help fellow texans affec...          3        0\n",
       "4  help harvey disaster response help victims nat...          0        0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/labeled_prelim_processed.csv')\n",
    "df = df.dropna()\n",
    "df.pop('Id')\n",
    "df = df.astype({'Relevancy':np.int32, 'Urgency':np.int32})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'millions afghanistan even zero attack isis sympathizers invest texas nation build harvey texasflood'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sym_spell = create_symspell(2,7,'data/frequency_dictionary_en_82_765.txt')\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "doc_sample = df['Text'][0]\n",
    "process_tweet(doc_sample, tknzr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of embeddings\n",
    "vec_length = 50\n",
    "embeddings = np.zeros((1193514, vec_length))\n",
    "\n",
    "keys = list()\n",
    "f = open('amitkeys2.txt', 'r')\n",
    "for line in f:\n",
    "\traw = line.lower().replace(\"\\n\",\"\").split(\" \")\n",
    "\tkeys.append(raw)\n",
    "\n",
    "#two-way map, index->word and word->index\n",
    "glove = {}\n",
    "\n",
    "index = 0\n",
    "with open('data/glove.twitter.27B/glove.twitter.27B.%dd.txt' % vec_length) as f:\n",
    "    for l in f:\n",
    "        line = []\n",
    "        try:\n",
    "            line = l.split()\n",
    "            if len(line) != vec_length+1:\n",
    "                print('empty line')\n",
    "                continue\n",
    "            \n",
    "            word = line[0]\n",
    "            embeddings[index] = np.array(line[1:]).astype(np.float)\n",
    "            glove[index] = word\n",
    "            glove[word] = index\n",
    "            index += 1\n",
    "        except:\n",
    "            print(line)\n",
    "            print(index)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 ... 3 0 0]\n"
     ]
    }
   ],
   "source": [
    "#convert columns to numpy arrays\n",
    "text = df['Text'].values\n",
    "relevancy = df['Relevancy'].values\n",
    "print (relevancy)\n",
    "urgency = df['Urgency'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevancyClassifier(nn.Module):\n",
    "    def __init__(self, index, embeddings, embed_len, num_classes):\n",
    "        super(RelevancyClassifier, self).__init__()\n",
    "        self.hidden_size = 30\n",
    "        self.embed_len = embed_len\n",
    "        #print(embeddings.shape)\n",
    "        #self.embedding = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.fc1 = nn.Linear(embed_len, self.hidden_size)\n",
    "        self.nl = nn.LeakyReLU()\n",
    "        self.fc2 = nn.Linear(self.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.nl(self.fc1(x)))\n",
    "\n",
    "def weightingHeuristic(wv, word, keys, embeddings, glove):\n",
    "    #new weighting heuristic finds the meaning of a tweet by looking at how each word\n",
    "    #relates to the keys.  Finds cosine similarity of this word to all the keys and finds the max\n",
    "    #and average of all the cosine similarities.  Returns 0.75 * max + 0.25 * avg\n",
    "    \n",
    "    total = 0\n",
    "    num = 0\n",
    "    word = word.lower().replace(\"\\n\",\"\")\n",
    "    similarities = list()\n",
    "    for kili in keys:\n",
    "        #if the kili is multiple words long, do the average\n",
    "        #print (kili)\n",
    "        innerTotal = 0\n",
    "        innerNum = 0\n",
    "        for kWord in kili:\n",
    "            try:\n",
    "                #print (embeddings[kWord])\n",
    "                curCosineSimilarity = np.dot(embeddings[glove[kWord]], embeddings[glove[word]])\n",
    "                #print (curCosineSimilarity)\n",
    "                #curCosineSimilarity = wv.similarity(kWord, word)\n",
    "                innerNum += 1\n",
    "                innerTotal += curCosineSimilarity\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        innerAvg = 0\n",
    "        if innerNum != 0:\n",
    "            innerAvg = float(innerTotal)/float(innerNum)\n",
    "            similarities.append(innerAvg)\n",
    "        total += innerAvg\n",
    "        num += 1\n",
    "    if len(similarities) == 0:\n",
    "        return 0\n",
    "    \n",
    "    maxSimilarity = max(similarities);\n",
    "    avgSimilarity = float(total)/float(num)\n",
    "    return (0.75 * maxSimilarity) + (0.25 * avgSimilarity)\n",
    "def train_relevancy_classifier(train_exs, train_labels, embeddings, index):\n",
    "    try:\n",
    "        wv = KeyedVectors.load(\"data/word2vec.kv\", mmap='r')\n",
    "        epochs = 70\n",
    "        lr = .00005\n",
    "        num_classes = 2\n",
    "        rc = RelevancyClassifier(index, embeddings, len(embeddings[0]), num_classes)\n",
    "        optimizer = optim.Adam(rc.parameters(), lr=lr)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        idxToCurEmbed = dict()\n",
    "        for epoch in range(epochs):\n",
    "            ex_indices = [i for i in range(len(train_exs))]\n",
    "            random.shuffle(ex_indices)\n",
    "            total_loss = 0.0\n",
    "            num_tweets = len(ex_indices)\n",
    "            \n",
    "            for idx in ex_indices:\n",
    "                \n",
    "                cur_tweet = train_exs[idx]\n",
    "                cur_embed = []\n",
    "                if idx in idxToCurEmbed:\n",
    "                    cur_embed = idxToCurEmbed[idx]\n",
    "                else:\n",
    "                    for i in cur_tweet.split():\n",
    "                        if i in index:\n",
    "                            #print (\"here\")\n",
    "                            newHeuristic = weightingHeuristic(wv, i, keys, embeddings, index)\n",
    "                            cur_embed.append(newHeuristic * embeddings[index[i]])\n",
    "                    idxToCurEmbed[idx] = cur_embed\n",
    "                if len(cur_embed) == 0:\n",
    "                    num_tweets -= 1\n",
    "                    continue\n",
    "                \n",
    "                x = torch.from_numpy(np.asarray(np.mean(cur_embed, axis=0)).reshape(1,vec_length)).float()\n",
    "                y = np.asarray(train_labels[idx]).reshape(1)\n",
    "                if y[0] > 0:\n",
    "                    y[0] = 1\n",
    "                y = torch.tensor(y).long()\n",
    "                rc.zero_grad()\n",
    "                probs = rc.forward(x)\n",
    "                cur_loss = loss(probs, y)\n",
    "                total_loss += cur_loss\n",
    "                cur_loss.backward()\n",
    "                optimizer.step()\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"Avg loss on epoch %i: %f\" % (epoch, total_loss/num_tweets))\n",
    "        return rc\n",
    "    except KeyboardInterrupt:\n",
    "        return rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'].values, relevancy, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss on epoch 0: 0.729923\n",
      "Avg loss on epoch 10: 0.442603\n",
      "Avg loss on epoch 20: 0.410383\n",
      "Avg loss on epoch 30: 0.387440\n",
      "Avg loss on epoch 40: 0.369778\n",
      "Avg loss on epoch 50: 0.351115\n",
      "Avg loss on epoch 60: 0.334685\n"
     ]
    }
   ],
   "source": [
    "model = train_relevancy_classifier(X_train, y_train, embeddings, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numCorrect: 385\n",
      "numTruePos: 57\n",
      "numFalsePos: 33\n",
      "numFalseNeg: 74\n",
      "accuracy: 0.782520\n",
      "precision: 0.633333\n",
      "recall: 0.435115\n",
      "f1: 0.515837\n"
     ]
    }
   ],
   "source": [
    "val_tweets = X_test\n",
    "val_labels = y_test\n",
    "for i in range(len(val_labels)):\n",
    "    if val_labels[i] > 0:\n",
    "        val_labels[i] = 1\n",
    "\n",
    "num_correct = 0\n",
    "num_true_pos = 0\n",
    "num_false_pos = 0\n",
    "num_false_neg = 0\n",
    "\n",
    "wv = KeyedVectors.load(\"data/word2vec.kv\", mmap='r')\n",
    "\n",
    "for i in range(len(val_tweets)):\n",
    "    cur_embed = []\n",
    "    cur_tweet = val_tweets[i]\n",
    "    cur_label = val_labels[i]\n",
    "    for i in cur_tweet.split():\n",
    "        if i in glove:\n",
    "            newHeuristic = weightingHeuristic(wv, i, keys, embeddings, glove)\n",
    "            cur_embed.append(newHeuristic * embeddings[glove[i]])\n",
    "    if len(cur_embed) == 0:\n",
    "        print (\"curembed was 0\")\n",
    "        continue\n",
    "    x = torch.from_numpy(np.asarray(np.mean(cur_embed, axis=0)).reshape(1,vec_length)).float()\n",
    "    probs = model.forward(x).detach().numpy().reshape(2)\n",
    "    pred_label = np.argmax(probs)\n",
    "    #print (\"pred_label: \" + str(pred_label) + \" cur_label: \" + str(cur_label))\n",
    "    if pred_label == cur_label:\n",
    "        num_correct += 1\n",
    "        if pred_label > 0:\n",
    "            num_true_pos += 1\n",
    "    else:\n",
    "        if pred_label == 0:\n",
    "            num_false_neg += 1\n",
    "        else:\n",
    "            num_false_pos += 1\n",
    "print (\"numCorrect: \" + str(num_correct))\n",
    "print (\"numTruePos: \" + str(num_true_pos))\n",
    "print (\"numFalsePos: \" + str(num_false_pos))\n",
    "print (\"numFalseNeg: \" + str(num_false_neg))\n",
    "accuracy = float(num_correct)/len(val_tweets)\n",
    "precision = float(num_true_pos)/(num_true_pos + num_false_pos)\n",
    "recall = float(num_true_pos)/(num_true_pos + num_false_neg)\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print('accuracy: %f' % accuracy)\n",
    "print('precision: %f' % precision)\n",
    "print('recall: %f' % recall)\n",
    "print('f1: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss on epoch 0: 0.856552\n",
      "Avg loss on epoch 10: 0.254290\n",
      "Avg loss on epoch 20: 0.235053\n",
      "Avg loss on epoch 30: 0.215762\n",
      "Avg loss on epoch 40: 0.201644\n",
      "Avg loss on epoch 50: 0.186645\n",
      "Avg loss on epoch 60: 0.172226\n"
     ]
    }
   ],
   "source": [
    "#urgency\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'].values, urgency, test_size=0.33)\n",
    "model = train_relevancy_classifier(X_train, y_train, embeddings, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numCorrect: 422\n",
      "numTruePos: 9\n",
      "numFalsePos: 13\n",
      "numFalseNeg: 57\n",
      "accuracy: 0.857724\n",
      "precision: 0.409091\n",
      "recall: 0.136364\n",
      "f1: 0.204545\n"
     ]
    }
   ],
   "source": [
    "val_tweets = X_test\n",
    "val_labels = y_test\n",
    "for i in range(len(val_labels)):\n",
    "    if val_labels[i] > 0:\n",
    "        val_labels[i] = 1\n",
    "\n",
    "num_correct = 0\n",
    "num_true_pos = 0\n",
    "num_false_pos = 0\n",
    "num_false_neg = 0\n",
    "\n",
    "wv = KeyedVectors.load(\"data/word2vec.kv\", mmap='r')\n",
    "\n",
    "for i in range(len(val_tweets)):\n",
    "    cur_embed = []\n",
    "    cur_tweet = val_tweets[i]\n",
    "    cur_label = val_labels[i]\n",
    "    for i in cur_tweet.split():\n",
    "        if i in glove:\n",
    "            newHeuristic = weightingHeuristic(wv, i, keys, embeddings, glove)\n",
    "            cur_embed.append(newHeuristic * embeddings[glove[i]])\n",
    "    if len(cur_embed) == 0:\n",
    "        print (\"curembed was 0\")\n",
    "        continue\n",
    "    x = torch.from_numpy(np.asarray(np.mean(cur_embed, axis=0)).reshape(1,vec_length)).float()\n",
    "    probs = model.forward(x).detach().numpy().reshape(2)\n",
    "    pred_label = np.argmax(probs)\n",
    "    #print (\"pred_label: \" + str(pred_label) + \" cur_label: \" + str(cur_label))\n",
    "    if pred_label == cur_label:\n",
    "        num_correct += 1\n",
    "        if pred_label > 0:\n",
    "            num_true_pos += 1\n",
    "    else:\n",
    "        if pred_label == 0:\n",
    "            num_false_neg += 1\n",
    "        else:\n",
    "            num_false_pos += 1\n",
    "print (\"numCorrect: \" + str(num_correct))\n",
    "print (\"numTruePos: \" + str(num_true_pos))\n",
    "print (\"numFalsePos: \" + str(num_false_pos))\n",
    "print (\"numFalseNeg: \" + str(num_false_neg))\n",
    "accuracy = float(num_correct)/len(val_tweets)\n",
    "precision = float(num_true_pos)/(num_true_pos + num_false_pos)\n",
    "recall = float(num_true_pos)/(num_true_pos + num_false_neg)\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print('accuracy: %f' % accuracy)\n",
    "print('precision: %f' % precision)\n",
    "print('recall: %f' % recall)\n",
    "print('f1: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only handles binary classification for now\n",
    "def tweets_to_df(df, labels, embeddings, glove):\n",
    "    wv = KeyedVectors.load(\"data/word2vec.kv\", mmap='r')\n",
    "    weights = []\n",
    "    index_omit = []\n",
    "    index = -1\n",
    "    tweets = df['Text']\n",
    "    \n",
    "    for i in range(vec_length+1):\n",
    "        weights.append([])\n",
    "    \n",
    "    for i in range(len(tweets)):\n",
    "        index += 1\n",
    "        cur_embed = []\n",
    "        cur_tweet = tweets[i]\n",
    "        cur_label = labels[i]\n",
    "        for i in cur_tweet.split():\n",
    "            if i in glove:\n",
    "                newHeuristic = weightingHeuristic(wv, i, keys, embeddings, glove)\n",
    "                cur_embed.append(newHeuristic * embeddings[glove[i]])\n",
    "        \n",
    "        if len(cur_embed) == 0:\n",
    "            #make sure we drop this row from the input dataframe\n",
    "            index_omit.append(index)\n",
    "            continue\n",
    "        \n",
    "        x = np.asarray(np.mean(cur_embed, axis=0))\n",
    "        \n",
    "        for j in range(vec_length):\n",
    "            weights[j].append(x[j])\n",
    "        weights[vec_length].append(0 if cur_label == 0 else 1)\n",
    "        #weights[vec_length].append(cur_label)\n",
    "        \n",
    "    df_pruned = df.drop(index_omit)\n",
    "    \n",
    "    #convert to dataframe\n",
    "    cols = {}\n",
    "    for i in range(vec_length):\n",
    "       cols['v' + str(i)] = weights[i]\n",
    "    \n",
    "    cols['class'] = weights[vec_length]\n",
    "    \n",
    "    df2 = pd.DataFrame(data=cols)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>v10</th>\n",
       "      <th>v11</th>\n",
       "      <th>v12</th>\n",
       "      <th>v13</th>\n",
       "      <th>v14</th>\n",
       "      <th>v15</th>\n",
       "      <th>v16</th>\n",
       "      <th>v17</th>\n",
       "      <th>...</th>\n",
       "      <th>v45</th>\n",
       "      <th>v46</th>\n",
       "      <th>v47</th>\n",
       "      <th>v48</th>\n",
       "      <th>v49</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.998035</td>\n",
       "      <td>5.347114</td>\n",
       "      <td>0.950726</td>\n",
       "      <td>-1.466031</td>\n",
       "      <td>-58.230695</td>\n",
       "      <td>2.642284</td>\n",
       "      <td>6.643521</td>\n",
       "      <td>1.476769</td>\n",
       "      <td>-6.476167</td>\n",
       "      <td>3.053299</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.344103</td>\n",
       "      <td>2.369313</td>\n",
       "      <td>2.817833</td>\n",
       "      <td>1.587355</td>\n",
       "      <td>7.929418</td>\n",
       "      <td>3.433002</td>\n",
       "      <td>0.103426</td>\n",
       "      <td>-4.444109</td>\n",
       "      <td>3.601828</td>\n",
       "      <td>-7.109326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.605123</td>\n",
       "      <td>11.142267</td>\n",
       "      <td>-5.486157</td>\n",
       "      <td>-0.896788</td>\n",
       "      <td>-95.222239</td>\n",
       "      <td>-4.549225</td>\n",
       "      <td>-6.169168</td>\n",
       "      <td>4.328559</td>\n",
       "      <td>1.857858</td>\n",
       "      <td>-2.361180</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.594231</td>\n",
       "      <td>-1.014837</td>\n",
       "      <td>-2.841846</td>\n",
       "      <td>-4.090874</td>\n",
       "      <td>2.883946</td>\n",
       "      <td>-1.407894</td>\n",
       "      <td>21.839877</td>\n",
       "      <td>-2.375379</td>\n",
       "      <td>4.392766</td>\n",
       "      <td>2.105177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.350399</td>\n",
       "      <td>13.051163</td>\n",
       "      <td>-3.558580</td>\n",
       "      <td>-6.126958</td>\n",
       "      <td>-68.863806</td>\n",
       "      <td>-7.538091</td>\n",
       "      <td>2.289408</td>\n",
       "      <td>3.754879</td>\n",
       "      <td>-0.834112</td>\n",
       "      <td>1.938239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102241</td>\n",
       "      <td>1.185584</td>\n",
       "      <td>1.660065</td>\n",
       "      <td>-4.959424</td>\n",
       "      <td>0.344646</td>\n",
       "      <td>-6.032435</td>\n",
       "      <td>11.982321</td>\n",
       "      <td>-1.327724</td>\n",
       "      <td>6.428528</td>\n",
       "      <td>-11.003581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.809885</td>\n",
       "      <td>5.458992</td>\n",
       "      <td>-2.471590</td>\n",
       "      <td>1.227460</td>\n",
       "      <td>-77.099327</td>\n",
       "      <td>-3.857330</td>\n",
       "      <td>-1.095846</td>\n",
       "      <td>4.377424</td>\n",
       "      <td>0.624772</td>\n",
       "      <td>-3.602200</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.280418</td>\n",
       "      <td>5.682988</td>\n",
       "      <td>1.137328</td>\n",
       "      <td>-3.541299</td>\n",
       "      <td>3.428471</td>\n",
       "      <td>-1.458458</td>\n",
       "      <td>12.939525</td>\n",
       "      <td>4.937977</td>\n",
       "      <td>-1.503626</td>\n",
       "      <td>0.343659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.606920</td>\n",
       "      <td>11.167610</td>\n",
       "      <td>1.128147</td>\n",
       "      <td>-7.107333</td>\n",
       "      <td>-74.370362</td>\n",
       "      <td>-0.794647</td>\n",
       "      <td>5.265394</td>\n",
       "      <td>11.115533</td>\n",
       "      <td>-2.088763</td>\n",
       "      <td>0.902111</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.667678</td>\n",
       "      <td>1.449313</td>\n",
       "      <td>2.282958</td>\n",
       "      <td>-1.661326</td>\n",
       "      <td>1.893424</td>\n",
       "      <td>-8.783475</td>\n",
       "      <td>12.074510</td>\n",
       "      <td>-2.774548</td>\n",
       "      <td>3.245433</td>\n",
       "      <td>-12.295974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          v0         v1       v10       v11        v12       v13       v14  \\\n",
       "0   3.998035   5.347114  0.950726 -1.466031 -58.230695  2.642284  6.643521   \n",
       "1  10.605123  11.142267 -5.486157 -0.896788 -95.222239 -4.549225 -6.169168   \n",
       "2  12.350399  13.051163 -3.558580 -6.126958 -68.863806 -7.538091  2.289408   \n",
       "3   8.809885   5.458992 -2.471590  1.227460 -77.099327 -3.857330 -1.095846   \n",
       "4  11.606920  11.167610  1.128147 -7.107333 -74.370362 -0.794647  5.265394   \n",
       "\n",
       "         v15       v16       v17    ...           v45       v46       v47  \\\n",
       "0   1.476769 -6.476167  3.053299    ...     -2.344103  2.369313  2.817833   \n",
       "1   4.328559  1.857858 -2.361180    ...     -7.594231 -1.014837 -2.841846   \n",
       "2   3.754879 -0.834112  1.938239    ...      0.102241  1.185584  1.660065   \n",
       "3   4.377424  0.624772 -3.602200    ...     -3.280418  5.682988  1.137328   \n",
       "4  11.115533 -2.088763  0.902111    ...     -1.667678  1.449313  2.282958   \n",
       "\n",
       "        v48       v49        v5         v6        v7        v8         v9  \n",
       "0  1.587355  7.929418  3.433002   0.103426 -4.444109  3.601828  -7.109326  \n",
       "1 -4.090874  2.883946 -1.407894  21.839877 -2.375379  4.392766   2.105177  \n",
       "2 -4.959424  0.344646 -6.032435  11.982321 -1.327724  6.428528 -11.003581  \n",
       "3 -3.541299  3.428471 -1.458458  12.939525  4.937977 -1.503626   0.343659  \n",
       "4 -1.661326  1.893424 -8.783475  12.074510 -2.774548  3.245433 -12.295974  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfv = tweets_to_df(df, relevancy, embeddings, glove)\n",
    "labels = dfv.pop('class')\n",
    "dfv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import * \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import *\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "def average(x):\n",
    "    return sum(x)/len(x)\n",
    "\n",
    "def get_stats(model, X, y, cv, verbose=False):\n",
    "    \n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    auc = []\n",
    "        \n",
    "    cv_results = cross_validate(model, X, y, scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc'], \n",
    "                                cv=cv, return_train_score=False)\n",
    "    \n",
    "    if verbose:\n",
    "        print(cv_results)\n",
    "    \n",
    "    #now return the data\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.76 ± 0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.47 ± 0.08</td>\n",
       "      <td>0.55 ± 0.10</td>\n",
       "      <td>0.41 ± 0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Voting</td>\n",
       "      <td>0.74 ± 0.03</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.55 ± 0.06</td>\n",
       "      <td>0.49 ± 0.06</td>\n",
       "      <td>0.62 ± 0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.68 ± 0.04</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.53 ± 0.04</td>\n",
       "      <td>0.42 ± 0.04</td>\n",
       "      <td>0.70 ± 0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.73 ± 0.04</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.42 ± 0.08</td>\n",
       "      <td>0.48 ± 0.09</td>\n",
       "      <td>0.38 ± 0.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Method     accuracy   auc           f1    precision       recall\n",
       "0  Naive Bayes  0.76 ± 0.04  0.77  0.47 ± 0.08  0.55 ± 0.10  0.41 ± 0.09\n",
       "1       Voting  0.74 ± 0.03  0.76  0.55 ± 0.06  0.49 ± 0.06  0.62 ± 0.09\n",
       "2          MLP  0.68 ± 0.04  0.74  0.53 ± 0.04  0.42 ± 0.04  0.70 ± 0.06\n",
       "3     AdaBoost  0.73 ± 0.04  0.73  0.42 ± 0.08  0.48 ± 0.09  0.38 ± 0.10"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {'Naive Bayes': GaussianNB(),\n",
    "          'Voting': VotingClassifier(estimators=[('mlp', MLPClassifier()),\n",
    "                                            ('ada', AdaBoostClassifier()),\n",
    "                                            ('nb', GaussianNB())], voting='soft'),\n",
    "          'Perceptron': MLPClassifier(),\n",
    "          'AdaBoost': AdaBoostClassifier()}\n",
    "\n",
    "vals = []\n",
    "metric = []\n",
    "model_name = []\n",
    "\n",
    "f1 = []\n",
    "precision = []\n",
    "recall = []\n",
    "accuracy = []\n",
    "auc = []\n",
    "method = ['Naive Bayes', 'Voting', 'MLP', 'AdaBoost']\n",
    "\n",
    "cv = 10\n",
    "for k,v in models.items():\n",
    "    stats = get_stats(v, dfv, labels, cv)\n",
    "    accuracy_avg = np.average(stats['test_accuracy'])\n",
    "    accuracy_std = np.std(stats['test_accuracy'])\n",
    "    precision_avg = np.average(stats['test_precision'])\n",
    "    precision_std = np.std(stats['test_precision'])\n",
    "    recall_avg = np.average(stats['test_recall'])\n",
    "    recall_std = np.std(stats['test_recall'])\n",
    "    f1_avg = np.average(stats['test_f1'])\n",
    "    f1_std = np.std(stats['test_f1'])\n",
    "    auc_avg = np.average(stats['test_roc_auc'])\n",
    "    \n",
    "    f1.append('%.2f ± %.2f' % (f1_avg, f1_std))\n",
    "    precision.append('%.2f ± %.2f' % (precision_avg, precision_std))\n",
    "    recall.append('%.2f ± %.2f' % (recall_avg, recall_std))\n",
    "    accuracy.append('%.2f ± %.2f' % (accuracy_avg, accuracy_std))\n",
    "    auc.append('%.2f' % auc_avg)\n",
    "    \n",
    "#     print('%s (%.2f, %.4f) (%.2f, %.4f) (%.2f, %.4f) (%.2f, %.4f) %.2f' % \n",
    "#           (k, accuracy_avg, accuracy_std, precision_avg, precision_std, recall_avg, \n",
    "#                recall_std, f1_avg, f1_std, auc_avg))\n",
    "\n",
    "df_view = pd.DataFrame(data={'Method': method, 'f1': f1, \n",
    "                             'precision':precision, 'recall':recall,\n",
    "                             'accuracy':accuracy, 'auc':auc})\n",
    "df_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
