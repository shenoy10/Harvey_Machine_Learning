{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/aman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import random\n",
    "import time\n",
    "\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#create spell checker/word splitter\n",
    "def create_symspell(max_edit_distance, prefix_length, freq_file_path):\n",
    "    # create object\n",
    "    sym_spell = SymSpell(max_edit_distance, prefix_length)\n",
    "    \n",
    "    # create dictionary using corpus.txt\n",
    "    if not sym_spell.create_dictionary(freq_file_path):\n",
    "        print(\"Corpus file not found\")\n",
    "        return None\n",
    "    return sym_spell\n",
    "\n",
    "def is_valid_token(w):\n",
    "    special = ['<url>','<number>', '<user>']\n",
    "    return w.isalpha() or w in special\n",
    "\n",
    "def process_tweet(tweet, tknzr, sym_spell=None, advanced=False):\n",
    "    st_1 = []\n",
    "    for w in tknzr.tokenize(tweet):\n",
    "        #remove retweet annotation if present:\n",
    "        if w == 'RT':\n",
    "            if advanced:\n",
    "                st_1.append('rt')\n",
    "        elif w[0] == '@':\n",
    "            if advanced:\n",
    "                st_1.append('<user>')\n",
    "        #remove hashtag symbol\n",
    "        elif w[0] == '#':\n",
    "            st_1.append(w[1:])\n",
    "        #replace link with LINK keyword\n",
    "        elif w[:4] == 'http':\n",
    "            st_1.append('<url>')\n",
    "        elif w.isnumeric():\n",
    "            if advanced:\n",
    "                st_1.append('<number>')\n",
    "        else:\n",
    "            st_1.append(w)\n",
    "    \n",
    "    st_2 = []\n",
    "    \n",
    "    #remove stop words and punctuation, make everything lowercase\n",
    "    if sym_spell != None:\n",
    "        st_2 = [sym_spell.word_segmentation(w.lower()).corrected_string \n",
    "                for w in st_1 if w.isalpha() and not w.lower() in stop_words]\n",
    "    elif advanced:\n",
    "        st_2 = [w.lower() for w in st_1 if is_valid_token(w) and \n",
    "                    not w.lower() in stop_words]\n",
    "    else:\n",
    "        st_2 = [w.lower() for w in st_1 if w.isalpha() and\n",
    "                not w.lower() in stop_words]\n",
    "    \n",
    "    #lemmatization (converts all words to root form for standardization)\n",
    "    lem = WordNetLemmatizer()\n",
    "    st_3 = list(map(lambda x: lem.lemmatize(x, pos='v'), st_2))\n",
    "    \n",
    "    #now do word segmentation/spell check\n",
    "    return ' '.join(st_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>Urgency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>More millions in #Afghanistan even with ZERO a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>These are the last post my brother made on soc...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In @cityofcc listening to local officials abou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So so so damn proud of @5ugarcane who is tirel...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can you help with #Harvey disaster respons...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Relevancy  Urgency\n",
       "0  More millions in #Afghanistan even with ZERO a...          0        0\n",
       "1  These are the last post my brother made on soc...          2        1\n",
       "2  In @cityofcc listening to local officials abou...          0        0\n",
       "3  So so so damn proud of @5ugarcane who is tirel...          3        0\n",
       "4  How can you help with #Harvey disaster respons...          0        0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/labeled_prelim.csv').dropna()\n",
    "df.pop('Id')\n",
    "df = df.astype({'Relevancy':np.int32, 'Urgency':np.int32}).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished mapping 25000 embeddings\n",
      "unmapped line: ['-0.29736', '-0.57305', '-0.39627', '0.11851', '0.16625', '0.20137', '0.15891', '0.27938', '-0.078399', '-0.12866', '0.21086', '0.10652', '-0.45356', '-0.60928', '-0.44878', '-0.10511', '0.32838', '-0.088057', '0.051537', '0.46852', '-0.13936', '-0.71007', '-0.65363', '0.23445', '-0.19538', '0.6608', '0.1313', '-0.045464', '0.43522', '-0.96466', '0.18855', '0.93414', '0.68161', '-0.64802', '0.059672', '-0.69549', '-0.31669', '-0.48399', '-0.63895', '-0.35644', '0.14326', '0.79823', '0.41653', '-0.10187', '0.17715', '-0.20817', '-0.47895', '0.36954', '0.4828', '0.37621', '-0.3492', '-0.089045', '0.40169', '-0.8378', '0.19303', '-0.16941', '0.2664', '0.49512', '-0.20796', '0.69913', '0.43428', '0.15835', '0.38629', '0.24039', '0.031994', '-0.14381', '0.52596', '0.28369', '-0.27033', '0.22807', '0.23541', '-0.39603', '-0.31054', '-0.78715', '-0.71227', '-0.029253', '0.24174', '-0.44296', '-0.836', '0.064297', '-0.94075', '-0.18824', '-0.16903', '0.5849', '-0.0074337', '0.626', '-0.49226', '-0.71578', '0.35292', '-0.21006', '-0.24776', '0.57754', '-0.27919', '0.70211', '0.039619', '0.34539', '-0.14673', '-0.81167', '0.68231', '0.52827', '-0.52141', '-0.69099', '-0.75099', '0.11661', '0.98226', '0.35352', '-0.11707', '0.45133', '0.69767', '0.19557', '-0.364', '-0.035521', '-0.71357', '-0.83975', '0.20347', '-0.039052', '-0.63665', '-0.4491', '-0.16223', '0.51879', '-0.7832', '0.0896', '-0.037932', '0.23763', '-0.51888', '-0.17253', '-0.014441', '-0.5044', '0.26391', '-0.53308', '0.92899', '0.043442', '-0.17849', '-0.24523', '-0.45531', '-0.069423', '-0.21187', '-0.41407', '-0.090711', '-0.34815', '0.1754', '-0.21396', '-0.13499', '-0.64721', '-0.3795', '-0.14429', '-0.30074', '0.61857', '-0.065655', '-0.14137', '0.45494', '0.26353', '-1.1331', '1.0426', '-0.027096', '0.23131', '0.32532', '-0.25335', '-0.34065', '0.28641', '-0.25686', '-1.1398', '0.22298', '-0.2051', '-0.48052', '-0.065082', '-0.32023', '-0.045533', '0.093544', '-0.28296', '-0.34975', '0.19851', '0.0086796', '0.12968', '0.96043', '0.4946', '0.47144', '-0.10981', '0.67961', '-0.42269', '0.23401', '0.38641', '-0.18864', '-0.8254', '-0.098215', '-0.27643', '-0.17081', '0.30223', '-0.62112', '-0.2338', '-0.39195', '-0.049065', '-0.28386', '0.24707', '-0.13131', '-0.33601', '-0.92245', '-0.32083', '-0.28469', '-0.43977']\n",
      "Finished mapping 50000 embeddings\n",
      "Finished mapping 75000 embeddings\n",
      "Finished mapping 100000 embeddings\n",
      "Finished mapping 125000 embeddings\n",
      "Finished mapping 150000 embeddings\n",
      "Finished mapping 175000 embeddings\n",
      "Finished mapping 200000 embeddings\n",
      "Finished mapping 225000 embeddings\n",
      "Finished mapping 250000 embeddings\n",
      "Finished mapping 275000 embeddings\n",
      "Finished mapping 300000 embeddings\n",
      "Finished mapping 325000 embeddings\n",
      "Finished mapping 350000 embeddings\n",
      "Finished mapping 375000 embeddings\n",
      "Finished mapping 400000 embeddings\n",
      "Finished mapping 425000 embeddings\n",
      "Finished mapping 450000 embeddings\n",
      "Finished mapping 475000 embeddings\n",
      "Finished mapping 500000 embeddings\n",
      "Finished mapping 525000 embeddings\n",
      "Finished mapping 550000 embeddings\n",
      "Finished mapping 575000 embeddings\n",
      "Finished mapping 600000 embeddings\n",
      "Finished mapping 625000 embeddings\n",
      "Finished mapping 650000 embeddings\n",
      "Finished mapping 675000 embeddings\n",
      "Finished mapping 700000 embeddings\n",
      "Finished mapping 725000 embeddings\n",
      "Finished mapping 750000 embeddings\n",
      "Finished mapping 775000 embeddings\n",
      "Finished mapping 800000 embeddings\n",
      "Finished mapping 825000 embeddings\n",
      "Finished mapping 850000 embeddings\n",
      "Finished mapping 875000 embeddings\n",
      "Finished mapping 900000 embeddings\n",
      "Finished mapping 925000 embeddings\n",
      "Finished mapping 950000 embeddings\n",
      "Finished mapping 975000 embeddings\n",
      "Finished mapping 1000000 embeddings\n",
      "Finished mapping 1025000 embeddings\n",
      "Finished mapping 1050000 embeddings\n",
      "Finished mapping 1075000 embeddings\n",
      "Finished mapping 1100000 embeddings\n",
      "Finished mapping 1125000 embeddings\n",
      "Finished mapping 1150000 embeddings\n",
      "Finished mapping 1175000 embeddings\n",
      "Finished mapping 1193515 embeddings\n"
     ]
    }
   ],
   "source": [
    "#list of embeddings\n",
    "vec_length = 200\n",
    "embeddings = []\n",
    "\n",
    "#two-way map, index->word and word->index\n",
    "indexer = {}\n",
    "embeddings.append(np.zeros(vec_length))\n",
    "indexer[0] = 'UNK'\n",
    "indexer['UNK'] = 0\n",
    "\n",
    "embeddings.append(np.zeros(vec_length))\n",
    "indexer[1] = 'PAD'\n",
    "indexer['PAD'] = 1\n",
    "\n",
    "index = 2\n",
    "with open('data/glove.twitter.27B/glove.twitter.27B.%dd.txt' % vec_length) as f:\n",
    "    for l in f:\n",
    "        line = []\n",
    "        try:\n",
    "            line = l.split()\n",
    "            if len(line) != vec_length+1:\n",
    "                print('unmapped line: ' + str(line))\n",
    "                continue\n",
    "            \n",
    "            word = line[0]\n",
    "            embeddings.append(np.array(line[1:]).astype(np.float))\n",
    "            indexer[index] = word\n",
    "            indexer[word] = index\n",
    "            index += 1\n",
    "        except:\n",
    "            print(line)\n",
    "            print(index)\n",
    "            break\n",
    "        if index % 25000 == 0:\n",
    "            print('Finished mapping %d embeddings' % index)\n",
    "        \n",
    "    print('Finished mapping %d embeddings' % len(embeddings))\n",
    "embeddings = np.asarray(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>Urgency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>millions afghanistan even zero attack isis &lt;nu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>last post brother make social media phone go v...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;user&gt; listen local officials epa help harvey ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>damn proud &lt;user&gt; tirelessly help fellow texan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>help harvey disaster response help victims nat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Relevancy  Urgency\n",
       "0  millions afghanistan even zero attack isis <nu...          0        0\n",
       "1  last post brother make social media phone go v...          1        1\n",
       "2  <user> listen local officials epa help harvey ...          0        0\n",
       "3  damn proud <user> tirelessly help fellow texan...          1        0\n",
       "4  help harvey disaster response help victims nat...          0        0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "df['Text'] = df['Text'].map(lambda x: process_tweet(x, tknzr, None, True))\n",
    "df['Relevancy'] = df['Relevancy'].map(lambda x: 0 if x < 1 else 1)\n",
    "df['Urgency'] = df['Urgency'].map(lambda x: 0 if x < 1 else 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03990969586676692\n",
      "409\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "unk_percent = []\n",
    "unk_words = set()\n",
    "max_len = 0\n",
    "for tweet in df['Text']:\n",
    "    indices = []\n",
    "    words = tweet.split()\n",
    "    if len(words) > max_len:\n",
    "        max_len = len(words)\n",
    "    unknown = 0\n",
    "    for word in words:\n",
    "        if word in indexer:\n",
    "            indices.append(indexer[word])\n",
    "        else:\n",
    "            indices.append(indexer['UNK'])\n",
    "            unk_words.add(word)\n",
    "            unknown += 1\n",
    "        unk_percent.append(unknown/len(words))\n",
    "    X.append(indices)\n",
    "    \n",
    "\n",
    "# add padding to make every tweet the same length\n",
    "for i in range(len(X)):\n",
    "    tweet = X[i]\n",
    "    if len(tweet) < max_len:\n",
    "        tweet = np.append(tweet, np.ones(max_len - len(tweet))) # b/c 'PAD' corresponds to index 1\n",
    "    X[i] = tweet\n",
    "    \n",
    "\n",
    "X = np.asarray(X)\n",
    "y_r = df['Relevancy'].values\n",
    "y_u = df['Urgency'].values\n",
    "print(np.mean(unk_percent))\n",
    "# print(unk_words)\n",
    "print(len(unk_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates binary classification model\n",
    "def calc_metrics(model, X_test, y_test):\n",
    "    num_correct = 0\n",
    "    num_true_pos = 0\n",
    "    num_false_pos = 0\n",
    "    num_false_neg = 0\n",
    "    \n",
    "    num_test_exs = len(X_test)\n",
    "\n",
    "    for i in range(num_test_exs):\n",
    "        y_pred = model.predict(X_test[i])\n",
    "        y_gold = y_test[i]\n",
    "        if y_pred == y_gold:\n",
    "            num_correct += 1\n",
    "            if y_gold > 0:\n",
    "                num_true_pos += 1\n",
    "        else:\n",
    "            if y_pred == 0:\n",
    "                num_false_neg += 1\n",
    "            else:\n",
    "                num_false_pos += 1\n",
    "\n",
    "    accuracy = num_correct/num_test_exs\n",
    "    precision = num_true_pos/(num_true_pos + num_false_pos)\n",
    "    recall = num_true_pos/(num_true_pos + num_false_neg)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    print('accuracy: %f' % accuracy)\n",
    "    print('precision: %f' % precision)\n",
    "    print('recall: %f' % recall)\n",
    "    print('f1: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevancyModel(nn.Module):\n",
    "    def __init__(self, weights, indexer, embed_len, hidden_size, seq_len):\n",
    "        super(RelevancyModel, self).__init__()\n",
    "        self.indexer = indexer\n",
    "        self.embed_len = embed_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embeddings = nn.Embedding.from_pretrained(weights)\n",
    "        self.rnn = nn.LSTM(input_size=embed_len, \n",
    "                           hidden_size=self.hidden_size,\n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.hidden_size*self.seq_len, 2)\n",
    "#         self.softmax - nn.Softmax() # only necessary if we use NLLLoss\n",
    "        \n",
    "    def forward(self, x, batch_size):\n",
    "        x = self.embeddings(x).reshape(batch_size, self.seq_len, self.embed_len)\n",
    "        x, _ = self.rnn(x) # output as (batch, seq_len, #directions*hidden)\n",
    "        x = x.reshape(batch_size, self.seq_len*self.hidden_size)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.argmax(self.forward(torch.from_numpy(x).long(), 1).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrgencyModel(nn.Module):\n",
    "    def __init__(self, weights, indexer, embed_len, hidden_size, seq_len):\n",
    "        super(UrgencyModel, self).__init__()\n",
    "        self.indexer = indexer\n",
    "        self.embed_len = embed_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embeddings = nn.Embedding.from_pretrained(weights)\n",
    "        self.rnn = nn.LSTM(input_size=embed_len, \n",
    "                           hidden_size=self.hidden_size,\n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.hidden_size*self.seq_len, 2)\n",
    "#         self.softmax - nn.Softmax() # only necessary if we use NLLLoss\n",
    "        \n",
    "    def forward(self, x, batch_size):\n",
    "        x = self.embeddings(x).reshape(batch_size, self.seq_len, self.embed_len)\n",
    "        x, _ = self.rnn(x) # output as (batch, seq_len, #directions*hidden)\n",
    "        x = x.reshape(batch_size, self.seq_len*self.hidden_size)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.argmax(self.forward(torch.from_numpy(x).long(), 1).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X_train: 2d np array, where each row is the indices corresponding to each word of a specific tweet\n",
    "y_train: 1d np array of same length as X_train with 0/1 based on relevant/not relevant or urgent/not urgent\n",
    "indexer: map of word->index and index->word\n",
    "embeddings: GloVe word embeddings created above\n",
    "\"\"\"\n",
    "def train_rnn_classifier(X_train, y_train, indexer, embeddings, rel, batch_size, epochs, lr, hidden_size, num_classes):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        embeddings = torch.from_numpy(embeddings).float()\n",
    "        embed_len = len(embeddings[0])\n",
    "        seq_len = len(X_train[0])\n",
    "        if rel:\n",
    "            rnn = RelevancyModel(embeddings, indexer, embed_len, hidden_size, seq_len)\n",
    "        else:\n",
    "            rnn = UrgencyModel(embeddings, indexer, embed_len, hidden_size, seq_len)\n",
    "        optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        for epoch in range(epochs):\n",
    "            ex_indices = [i for i in range(len(X_train))]\n",
    "            random.shuffle(ex_indices)\n",
    "            total_loss = 0.0\n",
    "            for idx in range(int(len(ex_indices)/batch_size)):\n",
    "                cur_batch_idx = ex_indices[idx*batch_size:(idx+1)*batch_size]\n",
    "                cur_X = torch.from_numpy(np.asarray([X_train[i] for i in cur_batch_idx])).long()\n",
    "                cur_y = torch.from_numpy(np.asarray([y_train[i] for i in cur_batch_idx]))\n",
    "                rnn.zero_grad()\n",
    "                probs = rnn.forward(cur_X, batch_size)#.reshape(batch_size, 2)\n",
    "    #             probs = torch.from_numpy(np.argmax(probs.detach().numpy(), axis=1))\n",
    "                cur_loss = loss(probs, cur_y)\n",
    "                total_loss += cur_loss\n",
    "                cur_loss.backward()\n",
    "                optimizer.step()\n",
    "            print(\"Avg loss on epoch %i: %f\" % (epoch+1, total_loss/len(ex_indices)))\n",
    "        end = time.time()\n",
    "        print(\"Time taken: %f seconds\" % (end-start))\n",
    "        return rnn\n",
    "    except KeyboardInterrupt:\n",
    "        end = time.time()\n",
    "        print(\"Time taken: %f seconds\" % (end-start))\n",
    "        return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains the relevancy and urgency classifiers and passes in the necessary hyperparameters:\n",
    "\n",
    "- Batch size: the number of examples simultaneously evaluated\n",
    "- Epochs: the number of times the model is trained on the entire training set\n",
    "- Learning rate (lr): affects how much the model learns from training example (usually between .001 and .0001; anything higher will likely do very poorly)\n",
    "- Hidden size: number of features in the hidden state of the RNN; basically number of features used to represent each word from the input to the RNN\n",
    "- Number of classes: how many output classes there are; binary in our case because we are doing relevant/not relevant and urgent/not urgent classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Relevancy RNN Classifier\n",
      "Avg loss on epoch 1: 0.059429\n",
      "Avg loss on epoch 2: 0.053223\n",
      "Avg loss on epoch 3: 0.048806\n",
      "Avg loss on epoch 4: 0.044359\n",
      "Avg loss on epoch 5: 0.042037\n",
      "Avg loss on epoch 6: 0.039093\n",
      "Avg loss on epoch 7: 0.037203\n",
      "Avg loss on epoch 8: 0.034819\n",
      "Avg loss on epoch 9: 0.033594\n",
      "Avg loss on epoch 10: 0.031804\n",
      "Time taken: 8.001290 seconds\n",
      "\n",
      "Relevancy metrics:\n",
      "accuracy: 0.727642\n",
      "precision: 0.485380\n",
      "recall: 0.643411\n",
      "f1: 0.553333\n"
     ]
    }
   ],
   "source": [
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X, y_r, test_size=0.33)\n",
    "\n",
    "print('Training Relevancy RNN Classifier')\n",
    "rnn_r = train_rnn_classifier(X_train_r, y_train_r, indexer, embeddings, rel=True, \n",
    "                             batch_size=10, \n",
    "                             epochs=10, \n",
    "                             lr=.0001, \n",
    "                             hidden_size=100, \n",
    "                             num_classes=2)\n",
    "print('\\nRelevancy metrics:')\n",
    "calc_metrics(rnn_r, X_test_r, y_test_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Urgency RNN Classifier\n",
      "Avg loss on epoch 1: 0.040895\n",
      "Avg loss on epoch 2: 0.029574\n",
      "Avg loss on epoch 3: 0.026931\n",
      "Avg loss on epoch 4: 0.026035\n",
      "Avg loss on epoch 5: 0.024469\n",
      "Avg loss on epoch 6: 0.023774\n",
      "Avg loss on epoch 7: 0.022641\n",
      "Avg loss on epoch 8: 0.021547\n",
      "Avg loss on epoch 9: 0.020346\n",
      "Avg loss on epoch 10: 0.018623\n",
      "Avg loss on epoch 11: 0.016934\n",
      "Avg loss on epoch 12: 0.015448\n",
      "Avg loss on epoch 13: 0.013958\n",
      "Avg loss on epoch 14: 0.012442\n",
      "Avg loss on epoch 15: 0.009752\n",
      "Avg loss on epoch 16: 0.009608\n",
      "Avg loss on epoch 17: 0.006937\n",
      "Avg loss on epoch 18: 0.005164\n",
      "Avg loss on epoch 19: 0.003680\n",
      "Avg loss on epoch 20: 0.003832\n",
      "Time taken: 23.027568 seconds\n",
      "\n",
      "Urgency metrics:\n",
      "accuracy: 0.853659\n",
      "precision: 0.393939\n",
      "recall: 0.448276\n",
      "f1: 0.419355\n"
     ]
    }
   ],
   "source": [
    "X_train_u, X_test_u, y_train_u, y_test_u = train_test_split(X, y_u, test_size=0.33)\n",
    "\n",
    "print('Training Urgency RNN Classifier')\n",
    "rnn_u = train_rnn_classifier(X_train_u, y_train_u, indexer, embeddings, rel=False, \n",
    "                             batch_size=10, \n",
    "                             epochs=20, \n",
    "                             lr=.0001, \n",
    "                             hidden_size=128, \n",
    "                             num_classes=2)\n",
    "\n",
    "print('\\nUrgency metrics:')\n",
    "calc_metrics(rnn_u, X_test_u, y_test_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
