{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ashwin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ashwin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import random\n",
    "import time\n",
    "\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#create spell checker/word splitter\n",
    "def create_symspell(max_edit_distance, prefix_length, freq_file_path):\n",
    "    # create object\n",
    "    sym_spell = SymSpell(max_edit_distance, prefix_length)\n",
    "    \n",
    "    # create dictionary using corpus.txt\n",
    "    if not sym_spell.create_dictionary(freq_file_path):\n",
    "        print(\"Corpus file not found\")\n",
    "        return None\n",
    "    return sym_spell\n",
    "\n",
    "def is_valid_token(w):\n",
    "    special = ['<url>','<number>', '<user>']\n",
    "    return w.isalpha() or w in special\n",
    "\n",
    "def process_tweet(tweet, tknzr, sym_spell=None, advanced=False):\n",
    "    st_1 = []\n",
    "    for w in tknzr.tokenize(tweet):\n",
    "        #remove retweet annotation if present:\n",
    "        if w == 'RT':\n",
    "            if advanced:\n",
    "                st_1.append('rt')\n",
    "        elif w[0] == '@':\n",
    "            if advanced:\n",
    "                st_1.append('<user>')\n",
    "        #remove hashtag symbol\n",
    "        elif w[0] == '#':\n",
    "            st_1.append(w[1:])\n",
    "        #replace link with LINK keyword\n",
    "        elif w[:4] == 'http':\n",
    "            st_1.append('<url>')\n",
    "        elif w.isnumeric():\n",
    "            if advanced:\n",
    "                st_1.append('<number>')\n",
    "        else:\n",
    "            st_1.append(w)\n",
    "    \n",
    "    st_2 = []\n",
    "    \n",
    "    #remove stop words and punctuation, make everything lowercase\n",
    "    if sym_spell != None:\n",
    "        st_2 = [sym_spell.word_segmentation(w.lower()).corrected_string \n",
    "                for w in st_1 if w.isalpha() and not w.lower() in stop_words]\n",
    "    elif advanced:\n",
    "        st_2 = [w.lower() for w in st_1 if is_valid_token(w) and \n",
    "                    not w.lower() in stop_words]\n",
    "    else:\n",
    "        st_2 = [w.lower() for w in st_1 if w.isalpha() and\n",
    "                not w.lower() in stop_words]\n",
    "    \n",
    "    #lemmatization (converts all words to root form for standardization)\n",
    "    lem = WordNetLemmatizer()\n",
    "    st_3 = list(map(lambda x: lem.lemmatize(x, pos='v'), st_2))\n",
    "    \n",
    "    #now do word segmentation/spell check\n",
    "    return ' '.join(st_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1741, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Urgency</th>\n",
       "      <th>Relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>your rescue boats, vehicles, volunteer craft ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!!  According to #PBS, #Houston convention cen...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"100,000 homes have been damaged by #Harvey an...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Do you need us to get a boat?\" Things I never...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Flood of epic proportions.\" #HurricaneHarvey ...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Urgency  Relevancy\n",
       "0   your rescue boats, vehicles, volunteer craft ...        0          0\n",
       "1  !!  According to #PBS, #Houston convention cen...        0          1\n",
       "2  \"100,000 homes have been damaged by #Harvey an...        0          2\n",
       "3  \"Do you need us to get a boat?\" Things I never...        0          0\n",
       "4  \"Flood of epic proportions.\" #HurricaneHarvey ...        0          3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/labeled_prelim.csv').dropna().astype(\n",
    "        {'Relevancy':np.int32, 'Urgency':np.int32}).reset_index(drop=True)\n",
    "df.pop('Id')\n",
    "\n",
    "df2 = pd.read_csv('data/mturk_results_0-2000_processed.csv')\n",
    "#df = pd.concat([df, df2], ignore_index=True)\n",
    "df = df2\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished mapping 25000 embeddings\n",
      "unmapped line: ['0.45973', '-0.16703', '-1.2028', '0.41675', '0.14643', '-0.39861', '-0.35118', '-0.46944', '0.63799', '0.49569', '-0.038122', '-0.37854', '-1.2221', '-1.0439', '-1.2604', '0.01232', '-0.5159', '0.1357', '-0.093283', '0.12307', '0.48072', '-0.66419', '0.50046', '-0.58255', '0.81583', '0.72197', '-0.101', '-0.17283', '0.51572', '0.3296', '-0.0024615', '0.19475', '2.1163', '0.20636', '-1.2026', '-0.0767', '-0.1058', '-0.82518', '-0.31287', '-0.19303', '0.061489', '-0.30422', '0.75731', '-0.53688', '-0.22277', '-0.22173', '-0.37943', '0.17821', '-0.34743', '0.3064']\n",
      "Finished mapping 50000 embeddings\n",
      "Finished mapping 75000 embeddings\n",
      "Finished mapping 100000 embeddings\n",
      "Finished mapping 125000 embeddings\n",
      "Finished mapping 150000 embeddings\n",
      "Finished mapping 175000 embeddings\n",
      "Finished mapping 200000 embeddings\n",
      "Finished mapping 225000 embeddings\n",
      "Finished mapping 250000 embeddings\n",
      "Finished mapping 275000 embeddings\n",
      "Finished mapping 300000 embeddings\n",
      "Finished mapping 325000 embeddings\n",
      "Finished mapping 350000 embeddings\n",
      "Finished mapping 375000 embeddings\n",
      "Finished mapping 400000 embeddings\n",
      "Finished mapping 425000 embeddings\n",
      "Finished mapping 450000 embeddings\n",
      "Finished mapping 475000 embeddings\n",
      "Finished mapping 500000 embeddings\n",
      "Finished mapping 525000 embeddings\n",
      "Finished mapping 550000 embeddings\n",
      "Finished mapping 575000 embeddings\n",
      "Finished mapping 600000 embeddings\n",
      "Finished mapping 625000 embeddings\n",
      "Finished mapping 650000 embeddings\n",
      "Finished mapping 675000 embeddings\n",
      "Finished mapping 700000 embeddings\n",
      "Finished mapping 725000 embeddings\n",
      "Finished mapping 750000 embeddings\n",
      "Finished mapping 775000 embeddings\n",
      "Finished mapping 800000 embeddings\n",
      "Finished mapping 825000 embeddings\n",
      "Finished mapping 850000 embeddings\n",
      "Finished mapping 875000 embeddings\n",
      "Finished mapping 900000 embeddings\n",
      "Finished mapping 925000 embeddings\n",
      "Finished mapping 950000 embeddings\n",
      "Finished mapping 975000 embeddings\n",
      "Finished mapping 1000000 embeddings\n",
      "Finished mapping 1025000 embeddings\n",
      "Finished mapping 1050000 embeddings\n",
      "Finished mapping 1075000 embeddings\n",
      "Finished mapping 1100000 embeddings\n",
      "Finished mapping 1125000 embeddings\n",
      "Finished mapping 1150000 embeddings\n",
      "Finished mapping 1175000 embeddings\n",
      "Finished mapping 1193515 embeddings\n"
     ]
    }
   ],
   "source": [
    "#list of embeddings\n",
    "vec_length = 50\n",
    "embeddings = []\n",
    "\n",
    "#two-way map, index->word and word->index\n",
    "indexer = {}\n",
    "embeddings.append(np.zeros(vec_length))\n",
    "indexer[0] = 'UNK'\n",
    "indexer['UNK'] = 0\n",
    "\n",
    "embeddings.append(np.zeros(vec_length))\n",
    "indexer[1] = 'PAD'\n",
    "indexer['PAD'] = 1\n",
    "\n",
    "index = 2\n",
    "with open('data/glove.twitter.27B/glove.twitter.27B.%dd.txt' % vec_length) as f:\n",
    "    for l in f:\n",
    "        line = []\n",
    "        try:\n",
    "            line = l.split()\n",
    "            if len(line) != vec_length+1:\n",
    "                print('unmapped line: ' + str(line))\n",
    "                continue\n",
    "            \n",
    "            word = line[0]\n",
    "            embeddings.append(np.array(line[1:]).astype(np.float))\n",
    "            indexer[index] = word\n",
    "            indexer[word] = index\n",
    "            index += 1\n",
    "        except:\n",
    "            print(line)\n",
    "            print(index)\n",
    "            break\n",
    "        if index % 25000 == 0:\n",
    "            print('Finished mapping %d embeddings' % index)\n",
    "        \n",
    "    print('Finished mapping %d embeddings' % len(embeddings))\n",
    "embeddings = np.asarray(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Urgency</th>\n",
       "      <th>Relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rescue boat vehicles volunteer craft need txwx...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accord pbs houston convention center need whee...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>home damage harvey financial toll begin &lt;url&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need us get boat things never think ask friend...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flood epic proportion hurricaneharvey inundate...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Urgency  Relevancy\n",
       "0  rescue boat vehicles volunteer craft need txwx...        0          0\n",
       "1  accord pbs houston convention center need whee...        0          1\n",
       "2      home damage harvey financial toll begin <url>        0          1\n",
       "3  need us get boat things never think ask friend...        0          0\n",
       "4  flood epic proportion hurricaneharvey inundate...        0          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "df['Text'] = df['Text'].map(lambda x: process_tweet(x, tknzr, None, True))\n",
    "df['Relevancy'] = df['Relevancy'].map(lambda x: 0 if x < 1 else 1)\n",
    "df['Urgency'] = df['Urgency'].map(lambda x: 0 if x < 1 else 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.043034201184915744\n",
      "459\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "unk_percent = []\n",
    "unk_words = set()\n",
    "max_len = 0\n",
    "for tweet in df['Text']:\n",
    "    indices = []\n",
    "    words = tweet.split()\n",
    "    if len(words) > max_len:\n",
    "        max_len = len(words)\n",
    "    unknown = 0\n",
    "    for word in words:\n",
    "        if word in indexer:\n",
    "            indices.append(indexer[word])\n",
    "        else:\n",
    "            indices.append(indexer['UNK'])\n",
    "            unk_words.add(word)\n",
    "            unknown += 1\n",
    "        unk_percent.append(unknown/len(words))\n",
    "    X.append(indices)\n",
    "    \n",
    "\n",
    "# add padding to make every tweet the same length\n",
    "for i in range(len(X)):\n",
    "    tweet = X[i]\n",
    "    if len(tweet) < max_len:\n",
    "        tweet = np.append(tweet, np.ones(max_len - len(tweet))) # b/c 'PAD' corresponds to index 1\n",
    "    X[i] = tweet\n",
    "    \n",
    "\n",
    "X = np.asarray(X)\n",
    "y_r = df['Relevancy'].values\n",
    "y_u = df['Urgency'].values\n",
    "print(np.mean(unk_percent))\n",
    "# print(unk_words)\n",
    "print(len(unk_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates binary classification model\n",
    "def calc_metrics(model, X_test, y_test):\n",
    "    num_correct = 0\n",
    "    num_true_pos = 0\n",
    "    num_false_pos = 0\n",
    "    num_false_neg = 0\n",
    "    \n",
    "    num_test_exs = len(X_test)\n",
    "\n",
    "    for i in range(num_test_exs):\n",
    "        y_pred = model.predict(X_test[i])\n",
    "        y_gold = y_test[i]\n",
    "        if y_pred == y_gold:\n",
    "            num_correct += 1\n",
    "            if y_gold > 0:\n",
    "                num_true_pos += 1\n",
    "        else:\n",
    "            if y_pred == 0:\n",
    "                num_false_neg += 1\n",
    "            else:\n",
    "                num_false_pos += 1\n",
    "\n",
    "    accuracy = num_correct/num_test_exs\n",
    "    precision = num_true_pos/(num_true_pos + num_false_pos)\n",
    "    recall = num_true_pos/(num_true_pos + num_false_neg)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    print('accuracy: %f' % accuracy)\n",
    "    print('precision: %f' % precision)\n",
    "    print('recall: %f' % recall)\n",
    "    print('f1: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevancyModel(nn.Module):\n",
    "    def __init__(self, weights, indexer, embed_len, hidden_size, seq_len):\n",
    "        super(RelevancyModel, self).__init__()\n",
    "        self.indexer = indexer\n",
    "        self.embed_len = embed_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embeddings = nn.Embedding.from_pretrained(weights)\n",
    "        self.rnn = nn.LSTM(input_size=embed_len, \n",
    "                           hidden_size=self.hidden_size,\n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.hidden_size*self.seq_len, 2)\n",
    "#         self.softmax - nn.Softmax() # only necessary if we use NLLLoss\n",
    "        \n",
    "    def forward(self, x, batch_size):\n",
    "        x = self.embeddings(x).reshape(batch_size, self.seq_len, self.embed_len)\n",
    "        x, _ = self.rnn(x) # output as (batch, seq_len, #directions*hidden)\n",
    "        x = x.reshape(batch_size, self.seq_len*self.hidden_size)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.argmax(self.forward(torch.from_numpy(x).long(), 1).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrgencyModel(nn.Module):\n",
    "    def __init__(self, weights, indexer, embed_len, hidden_size, seq_len):\n",
    "        super(UrgencyModel, self).__init__()\n",
    "        self.indexer = indexer\n",
    "        self.embed_len = embed_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embeddings = nn.Embedding.from_pretrained(weights)\n",
    "        self.rnn = nn.LSTM(input_size=embed_len, \n",
    "                           hidden_size=self.hidden_size,\n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.hidden_size*self.seq_len, 2)\n",
    "#         self.softmax - nn.Softmax() # only necessary if we use NLLLoss\n",
    "        \n",
    "    def forward(self, x, batch_size):\n",
    "        x = self.embeddings(x).reshape(batch_size, self.seq_len, self.embed_len)\n",
    "        x, _ = self.rnn(x) # output as (batch, seq_len, #directions*hidden)\n",
    "        x = x.reshape(batch_size, self.seq_len*self.hidden_size)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.argmax(self.forward(torch.from_numpy(x).long(), 1).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X_train: 2d np array, where each row is the indices corresponding to each word of a specific tweet\n",
    "y_train: 1d np array of same length as X_train with 0/1 based on relevant/not relevant or urgent/not urgent\n",
    "indexer: map of word->index and index->word\n",
    "embeddings: GloVe word embeddings created above\n",
    "\"\"\"\n",
    "def train_rnn_classifier(X_train, y_train, indexer, embeddings, rel, batch_size, epochs, lr, hidden_size, num_classes):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        embeddings = torch.from_numpy(embeddings).float()\n",
    "        embed_len = len(embeddings[0])\n",
    "        seq_len = len(X_train[0])\n",
    "        if rel:\n",
    "            rnn = RelevancyModel(embeddings, indexer, embed_len, hidden_size, seq_len)\n",
    "        else:\n",
    "            rnn = UrgencyModel(embeddings, indexer, embed_len, hidden_size, seq_len)\n",
    "        optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        for epoch in range(epochs):\n",
    "            ex_indices = [i for i in range(len(X_train))]\n",
    "            random.shuffle(ex_indices)\n",
    "            total_loss = 0.0\n",
    "            for idx in range(int(len(ex_indices)/batch_size)):\n",
    "                cur_batch_idx = ex_indices[idx*batch_size:(idx+1)*batch_size]\n",
    "                cur_X = torch.from_numpy(np.asarray([X_train[i] for i in cur_batch_idx])).long()\n",
    "                cur_y = torch.from_numpy(np.asarray([y_train[i] for i in cur_batch_idx]))\n",
    "                rnn.zero_grad()\n",
    "                probs = rnn.forward(cur_X, batch_size)#.reshape(batch_size, 2)\n",
    "    #             probs = torch.from_numpy(np.argmax(probs.detach().numpy(), axis=1))\n",
    "                cur_loss = loss(probs, cur_y)\n",
    "                total_loss += cur_loss\n",
    "                cur_loss.backward()\n",
    "                optimizer.step()\n",
    "            print(\"Avg loss on epoch %i: %f\" % (epoch+1, total_loss/len(ex_indices)))\n",
    "        end = time.time()\n",
    "        print(\"Time taken: %f seconds\" % (end-start))\n",
    "        return rnn\n",
    "    except KeyboardInterrupt:\n",
    "        end = time.time()\n",
    "        print(\"Time taken: %f seconds\" % (end-start))\n",
    "        return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains the relevancy and urgency classifiers and passes in the necessary hyperparameters:\n",
    "\n",
    "- Batch size: the number of examples simultaneously evaluated\n",
    "- Epochs: the number of times the model is trained on the entire training set\n",
    "- Learning rate (lr): affects how much the model learns from training example (usually between .001 and .0001; anything higher will likely do very poorly)\n",
    "- Hidden size: number of features in the hidden state of the RNN; basically number of features used to represent each word from the input to the RNN\n",
    "- Number of classes: how many output classes there are; binary in our case because we are doing relevant/not relevant and urgent/not urgent classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Relevancy RNN Classifier\n",
      "Avg loss on epoch 1: 0.068230\n",
      "Avg loss on epoch 2: 0.066012\n",
      "Avg loss on epoch 3: 0.063008\n",
      "Avg loss on epoch 4: 0.060070\n",
      "Avg loss on epoch 5: 0.057887\n",
      "Avg loss on epoch 6: 0.056686\n",
      "Avg loss on epoch 7: 0.055477\n",
      "Avg loss on epoch 8: 0.054661\n",
      "Avg loss on epoch 9: 0.053667\n",
      "Avg loss on epoch 10: 0.053547\n",
      "Avg loss on epoch 11: 0.052828\n",
      "Avg loss on epoch 12: 0.052231\n",
      "Avg loss on epoch 13: 0.051536\n",
      "Avg loss on epoch 14: 0.051882\n",
      "Avg loss on epoch 15: 0.050419\n",
      "Avg loss on epoch 16: 0.049982\n",
      "Avg loss on epoch 17: 0.049605\n",
      "Avg loss on epoch 18: 0.048696\n",
      "Avg loss on epoch 19: 0.047956\n",
      "Avg loss on epoch 20: 0.046954\n",
      "Avg loss on epoch 21: 0.046595\n",
      "Avg loss on epoch 22: 0.046591\n",
      "Avg loss on epoch 23: 0.046334\n",
      "Avg loss on epoch 24: 0.044648\n",
      "Avg loss on epoch 25: 0.044244\n",
      "Avg loss on epoch 26: 0.042484\n",
      "Avg loss on epoch 27: 0.042403\n",
      "Avg loss on epoch 28: 0.040682\n",
      "Avg loss on epoch 29: 0.040298\n",
      "Avg loss on epoch 30: 0.039303\n",
      "Avg loss on epoch 31: 0.038309\n",
      "Avg loss on epoch 32: 0.037111\n",
      "Avg loss on epoch 33: 0.035722\n",
      "Avg loss on epoch 34: 0.035480\n",
      "Avg loss on epoch 35: 0.034639\n",
      "Avg loss on epoch 36: 0.033194\n",
      "Avg loss on epoch 37: 0.033373\n",
      "Avg loss on epoch 38: 0.031666\n",
      "Avg loss on epoch 39: 0.029841\n",
      "Avg loss on epoch 40: 0.031559\n",
      "Avg loss on epoch 41: 0.030169\n",
      "Avg loss on epoch 42: 0.028318\n",
      "Avg loss on epoch 43: 0.028800\n",
      "Avg loss on epoch 44: 0.026073\n",
      "Avg loss on epoch 45: 0.025510\n",
      "Avg loss on epoch 46: 0.024372\n",
      "Avg loss on epoch 47: 0.023939\n",
      "Avg loss on epoch 48: 0.022512\n",
      "Avg loss on epoch 49: 0.023114\n",
      "Avg loss on epoch 50: 0.021384\n",
      "Time taken: 55.051860 seconds\n",
      "\n",
      "Relevancy metrics:\n",
      "accuracy: 0.673043\n",
      "precision: 0.703349\n",
      "recall: 0.538462\n",
      "f1: 0.609959\n"
     ]
    }
   ],
   "source": [
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X, y_r, test_size=0.33)\n",
    "\n",
    "print('Training Relevancy RNN Classifier')\n",
    "rnn_r = train_rnn_classifier(X_train_r, y_train_r, indexer, embeddings, rel=True, \n",
    "                             batch_size=10, \n",
    "                             epochs=50, \n",
    "                             lr=.0001, \n",
    "                             hidden_size=100, \n",
    "                             num_classes=2)\n",
    "print('\\nRelevancy metrics:')\n",
    "calc_metrics(rnn_r, X_test_r, y_test_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Urgency RNN Classifier\n",
      "Avg loss on epoch 1: 0.040895\n",
      "Avg loss on epoch 2: 0.029574\n",
      "Avg loss on epoch 3: 0.026931\n",
      "Avg loss on epoch 4: 0.026035\n",
      "Avg loss on epoch 5: 0.024469\n",
      "Avg loss on epoch 6: 0.023774\n",
      "Avg loss on epoch 7: 0.022641\n",
      "Avg loss on epoch 8: 0.021547\n",
      "Avg loss on epoch 9: 0.020346\n",
      "Avg loss on epoch 10: 0.018623\n",
      "Avg loss on epoch 11: 0.016934\n",
      "Avg loss on epoch 12: 0.015448\n",
      "Avg loss on epoch 13: 0.013958\n",
      "Avg loss on epoch 14: 0.012442\n",
      "Avg loss on epoch 15: 0.009752\n",
      "Avg loss on epoch 16: 0.009608\n",
      "Avg loss on epoch 17: 0.006937\n",
      "Avg loss on epoch 18: 0.005164\n",
      "Avg loss on epoch 19: 0.003680\n",
      "Avg loss on epoch 20: 0.003832\n",
      "Time taken: 23.027568 seconds\n",
      "\n",
      "Urgency metrics:\n",
      "accuracy: 0.853659\n",
      "precision: 0.393939\n",
      "recall: 0.448276\n",
      "f1: 0.419355\n"
     ]
    }
   ],
   "source": [
    "X_train_u, X_test_u, y_train_u, y_test_u = train_test_split(X, y_u, test_size=0.33)\n",
    "\n",
    "print('Training Urgency RNN Classifier')\n",
    "rnn_u = train_rnn_classifier(X_train_u, y_train_u, indexer, embeddings, rel=False, \n",
    "                             batch_size=10, \n",
    "                             epochs=20, \n",
    "                             lr=.0001, \n",
    "                             hidden_size=128, \n",
    "                             num_classes=2)\n",
    "\n",
    "print('\\nUrgency metrics:')\n",
    "calc_metrics(rnn_u, X_test_u, y_test_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
