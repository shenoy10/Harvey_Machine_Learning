{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty line\n"
     ]
    }
   ],
   "source": [
    "#read in the word embeddings\n",
    "vec_length = 100\n",
    "embeddings = np.zeros((1193514+2, vec_length))\n",
    "\n",
    "#two-way map, index->word and word->index\n",
    "glove = {}\n",
    "\n",
    "#add special tokens for unknown and padding\n",
    "embeddings[0] = np.zeros(vec_length)\n",
    "glove[0] = 'UNK'\n",
    "glove['UNK'] = 0\n",
    "\n",
    "embeddings[1] = np.zeros(vec_length)\n",
    "glove[1] = 'PAD'\n",
    "glove['PAD'] = 1\n",
    "\n",
    "index = 2\n",
    "with open('glove.twitter.27B.%dd.txt' % vec_length) as f:\n",
    "    for l in f:\n",
    "        line = []\n",
    "        try:\n",
    "            line = l.split()\n",
    "            if len(line) != vec_length+1:\n",
    "                print('empty line')\n",
    "                continue\n",
    "            \n",
    "            word = line[0]\n",
    "            embeddings[index] = np.array(line[1:]).astype(np.float)\n",
    "            glove[index] = word\n",
    "            glove[word] = index\n",
    "            index += 1\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in character-level embeddings\n",
    "char_vec_length = 300\n",
    "char_embeddings = np.zeros((94+2, char_vec_length))\n",
    "\n",
    "#two-way map, index->word and word->index\n",
    "char_glove = {}\n",
    "\n",
    "#add special tokens for unknown and padding\n",
    "char_embeddings[0] = np.zeros(char_vec_length)\n",
    "char_glove[0] = 'UNK'\n",
    "char_glove['UNK'] = 0\n",
    "\n",
    "char_embeddings[1] = np.zeros(char_vec_length)\n",
    "char_glove[1] = 'PAD'\n",
    "char_glove['PAD'] = 1\n",
    "\n",
    "index = 2\n",
    "with open('glove.840B.%dd-char.txt' % char_vec_length) as f:\n",
    "    for l in f:\n",
    "        line = []\n",
    "        try:\n",
    "            line = l.split()\n",
    "            if len(line) != char_vec_length+1:\n",
    "                print('empty line')\n",
    "                continue\n",
    "            \n",
    "            word = line[0]\n",
    "            char_embeddings[index] = np.array(line[1:]).astype(np.float)\n",
    "            char_glove[index] = word\n",
    "            char_glove[word] = index\n",
    "            index += 1\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4078, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>Urgency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>harveystorm water waist deep respect cop help ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>find help affect hurricane harvey yeg hurrican...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mountainview heroes deploy help harvey &lt;url&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>help impact hurricaneharvey weave activate don...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>much flood houston wow tune news prayers sympa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Relevancy  Urgency\n",
       "0  harveystorm water waist deep respect cop help ...          0        0\n",
       "1  find help affect hurricane harvey yeg hurrican...          1        0\n",
       "2       mountainview heroes deploy help harvey <url>          3        0\n",
       "3  help impact hurricaneharvey weave activate don...          0        0\n",
       "4  much flood houston wow tune news prayers sympa...          0        0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the dataset\n",
    "df = pd.read_csv('final_dataset_processed.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.035785945166337485\n",
      "number of unknown words: 735\n",
      "max tweet length: 22\n"
     ]
    }
   ],
   "source": [
    "#now convert the tweets into a list of indices\n",
    "X = []\n",
    "unk_percent = []\n",
    "unk_words = set()\n",
    "max_len = 0\n",
    "\n",
    "# set of all words found in all tweets\n",
    "all_words = set() \n",
    "\n",
    "for tweet in df['Text']:\n",
    "    indices = []\n",
    "    words = tweet.split()\n",
    "    if len(words) > max_len:\n",
    "        max_len = len(words)\n",
    "    \n",
    "    unknown = 0\n",
    "    for word in words:\n",
    "        all_words.add(word)\n",
    "        if word in glove:\n",
    "            indices.append(glove[word])\n",
    "        else:\n",
    "            indices.append(glove['UNK'])\n",
    "            unk_words.add(word)\n",
    "            unknown += 1\n",
    "        unk_percent.append(unknown/len(words))\n",
    "    X.append(indices)\n",
    "\n",
    "# add padding to make every tweet the same length\n",
    "for i in range(len(X)):\n",
    "    tweet = X[i]\n",
    "    if len(tweet) < max_len:\n",
    "        tweet = np.append(tweet, np.ones(max_len - len(tweet)))\n",
    "    X[i] = tweet\n",
    "\n",
    "X = np.asarray(X, dtype=np.int64)\n",
    "y = np.array(list(map(lambda x: 1 if x > 0 else 0, df['Relevancy'].values)), dtype=np.int64)\n",
    "\n",
    "print(np.mean(unk_percent))\n",
    "print('number of unknown words: ' + str(len(unk_words)))\n",
    "print('max tweet length: ' + str(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create map of words to arrays of character indices in two-way embedding map\n",
    "word_to_charr = {}\n",
    "for word in all_words:\n",
    "    if word[0] == '<':\n",
    "        continue\n",
    "    word_to_charr[word] = list(map(lambda x: char_glove[x] if x in char_glove else 0, list(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 189\n",
      "2 737\n",
      "3 3399\n",
      "4 8961\n",
      "5 9685\n",
      "6 8544\n",
      "7 3812\n",
      "8 2799\n",
      "9 1729\n",
      "10 833\n",
      "11 592\n",
      "12 870\n",
      "13 512\n",
      "14 136\n",
      "15 1566\n",
      "16 79\n",
      "17 38\n",
      "18 19\n",
      "19 32\n",
      "20 8\n",
      "21 14\n",
      "22 4\n",
      "23 3\n",
      "24 1\n",
      "25 2\n",
      "29 2\n",
      "30 2\n",
      "\n",
      "chosen word length threshold: 10\n"
     ]
    }
   ],
   "source": [
    "#figure out a good cutoff for word length\n",
    "word_lengths = {}\n",
    "for tweet in df['Text']:\n",
    "    for word in tweet.split():\n",
    "        if len(word) not in word_lengths:\n",
    "            word_lengths[len(word)] = 1\n",
    "        else:\n",
    "            word_lengths[len(word)] += 1\n",
    "\n",
    "sorted_lengths = sorted(list(word_lengths.keys()))\n",
    "\n",
    "for l in sorted_lengths:\n",
    "    print(l, word_lengths[l])\n",
    "\n",
    "#now figure out the max word length threshold that captures 90% words\n",
    "num_words = sum(word_lengths.values())\n",
    "curr_sum = 0\n",
    "index = -1\n",
    "while curr_sum < 0.9*num_words:\n",
    "    index += 1\n",
    "    curr_sum += word_lengths[sorted_lengths[index]]\n",
    "\n",
    "max_word_len = sorted_lengths[index]\n",
    "print('\\nchosen word length threshold: ' + str(max_word_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4078, 22, 10)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we generate the character index array\n",
    "X_char = []\n",
    "\n",
    "for tweet in df['Text']:\n",
    "    tweet_indices = []\n",
    "    for word in tweet.split():\n",
    "        \n",
    "        word_indices = []\n",
    "        if word in word_to_charr:\n",
    "            word_indices = word_to_charr[word]\n",
    "        else:\n",
    "            #word is not in map, fill it with dummy chars\n",
    "            word_indices = [char_glove['PAD']] * max_word_len\n",
    "        \n",
    "        if len(word) > max_word_len:\n",
    "            word_indices = word_indices[:max_word_len]\n",
    "        else:\n",
    "            word_indices = word_indices + [char_glove['PAD']]*(max_word_len-len(word_indices))\n",
    "        \n",
    "        tweet_indices.append(word_indices)\n",
    "    \n",
    "    X_char.append(tweet_indices)\n",
    "\n",
    "#now add more padding to make all tweets same length\n",
    "for i in range(len(X_char)):\n",
    "    tweet_indices = X_char[i]\n",
    "    if len(tweet_indices) < max_len:\n",
    "        taco = len(tweet_indices)\n",
    "        for j in range(max_len-taco):\n",
    "            tweet_indices.append([char_glove['PAD']]*max_word_len)\n",
    "        X_char[i] = tweet_indices \n",
    "\n",
    "X_char = np.array(X_char, dtype=np.int64) \n",
    "X_char.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This architecture is inspired by the one used in the paper\n",
    "'Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts' (Santos and Gatti, 2014)\n",
    "\"\"\"\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, w_embeddings, c_embeddings, n_w_filters, n_c_filters,\n",
    "                    w_filter_sizes, c_filter_sizes, n_classes, dropout):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        #length of the word and character embeddings\n",
    "        word_embedding_dim = w_embeddings.shape[1]\n",
    "        char_embedding_dim = c_embeddings.shape[1]\n",
    "        \n",
    "        #architecture\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(w_embeddings).cuda()\n",
    "        self.char_embedding = nn.Embedding.from_pretrained(c_embeddings).cuda()\n",
    "        \n",
    "        self.word_convs = [nn.Conv2d(in_channels = 1, \n",
    "                               out_channels = n_w_filters, \n",
    "                               kernel_size = (f_size, word_embedding_dim + len(c_filter_sizes)*n_c_filters)).cuda() \n",
    "                     for f_size in w_filter_sizes]\n",
    "        \n",
    "        self.char_convs = [nn.Conv2d(in_channels = 1, \n",
    "                               out_channels = n_c_filters, \n",
    "                               kernel_size = (f_size, char_embedding_dim)).cuda() \n",
    "                     for f_size in c_filter_sizes]\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(len(w_filter_sizes) * n_w_filters, n_classes).cuda()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout).cuda()\n",
    "        self.softmax = nn.Softmax().cuda()\n",
    "        \n",
    "    def forward(self, word_indices, char_indices):\n",
    "        \n",
    "        #first generate word vectors from character embeddings\n",
    "        char_indices = self.char_embedding(char_indices)\n",
    "        char_vectors = [self.process_chars(tweet) for tweet in char_indices]\n",
    "        char_vectors = torch.reshape(torch.cat(char_vectors, dim=0), \n",
    "                                    (len(char_vectors), *char_vectors[0].shape))\n",
    "        char_vectors = char_vectors.unsqueeze(1)\n",
    "        \n",
    "        embedded = self.word_embedding(word_indices)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        embedded = torch.cat((embedded, char_vectors), dim=3)\n",
    "        \n",
    "        conved = [F.tanh(conv(embedded)).squeeze(3) for conv in self.word_convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        return self.softmax(self.fc(cat))\n",
    "    \n",
    "    def process_chars(self, tweet):\n",
    "        tweet = tweet.unsqueeze(1)\n",
    "        conved = [F.tanh(conv(tweet)).squeeze(3) for conv in self.char_convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        word_vector = torch.cat(pooled, dim=1)\n",
    "        return word_vector\n",
    "    \n",
    "    def predict(self, tweet, chars):\n",
    "        return np.argmax(self.forward(tweet, chars).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn_classifier(X_train, X_char_train, y_train, w_embeddings, c_embeddings, num_classes, manual_params=None, verbose=False):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        w_embeddings = torch.from_numpy(w_embeddings).float().to(device)\n",
    "        c_embeddings = torch.from_numpy(c_embeddings).float().to(device)\n",
    "        \n",
    "        #default parameters for the model\n",
    "        params = {'batch_size': 10, 'epochs': 50, 'lr': 0.0001, 'n_w_filters': 400, \n",
    "                  'n_c_filters': 100, 'w_filter_sizes': [1,1,1], 'c_filter_sizes': [3,4,5], 'dropout': 0.75}\n",
    "        \n",
    "        #replace default parameters with any user-defined ones\n",
    "        if manual_params is not None:\n",
    "            for p in manual_params:\n",
    "                params[p] = manual_params[p]\n",
    "                \n",
    "        batch_size = params['batch_size']\n",
    "        epochs = params['epochs']\n",
    "        lr = params['lr']\n",
    "        \n",
    "        #initialize network and optimizer\n",
    "        cnn = CNN(w_embeddings, c_embeddings, n_w_filters=params['n_w_filters'], \n",
    "                  n_c_filters=params['n_c_filters'], w_filter_sizes=params['w_filter_sizes'],\n",
    "                  c_filter_sizes=params['c_filter_sizes'], n_classes=num_classes, dropout=params['dropout'])\n",
    "        cnn.to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(cnn.parameters(), lr=lr)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        cnn.train()\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            ex_indices = [i for i in range(len(X_train))]\n",
    "            random.shuffle(ex_indices)\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            for idx in range(len(ex_indices)//batch_size):\n",
    "                \n",
    "                #create input batch to feed in\n",
    "                cur_batch_idx = ex_indices[idx*batch_size:(idx+1)*batch_size]\n",
    "                cur_X = torch.from_numpy(np.asarray([X_train[i] for i in cur_batch_idx])).long().to(device)\n",
    "                cur_X_char = torch.from_numpy(np.asarray([X_char_train[i] for i in cur_batch_idx])).long().to(device)\n",
    "                cur_y = torch.from_numpy(np.asarray([y_train[i] for i in cur_batch_idx])).to(device)\n",
    "                \n",
    "                #train\n",
    "                cnn.zero_grad()\n",
    "                probs = cnn.forward(cur_X, cur_X_char)\n",
    "                \n",
    "                #calculate loss and update weights\n",
    "                cur_loss = loss(probs, cur_y)\n",
    "                total_loss += cur_loss\n",
    "                cur_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Avg loss on epoch %i: %f\" % (epoch+1, total_loss/len(ex_indices)))\n",
    "        end = time.time()\n",
    "        print(\"Time taken: %f seconds\" % (end-start))\n",
    "        return cnn\n",
    "    except KeyboardInterrupt:\n",
    "        end = time.time()\n",
    "        print(\"Time taken: %f seconds\" % (end-start))\n",
    "        return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates binary classification model\n",
    "def calc_metrics(model, X_test, X_char_test, y_test):\n",
    "    num_correct = 0\n",
    "    num_true_pos = 0\n",
    "    num_false_pos = 0\n",
    "    num_false_neg = 0\n",
    "    \n",
    "    num_test_exs = len(X_test)\n",
    "    \n",
    "    model.eval()\n",
    "    for i in range(num_test_exs):\n",
    "        \n",
    "        cur_batch_idx = [i]\n",
    "        cur_X = torch.from_numpy(np.asarray([X_test[i] for i in cur_batch_idx])).long().to(device)\n",
    "        cur_X_char = torch.from_numpy(np.asarray([X_char_test[i] for i in cur_batch_idx])).long().to(device)\n",
    "        \n",
    "        y_pred = model.predict(cur_X, cur_X_char)\n",
    "        y_gold = y_test[i]\n",
    "        if y_pred == y_gold:\n",
    "            num_correct += 1\n",
    "            if y_gold > 0:\n",
    "                num_true_pos += 1\n",
    "        else:\n",
    "            if y_pred == 0:\n",
    "                num_false_neg += 1\n",
    "            else:\n",
    "                num_false_pos += 1\n",
    "\n",
    "    accuracy = num_correct/num_test_exs\n",
    "    precision = num_true_pos/(num_true_pos + num_false_pos)\n",
    "    recall = num_true_pos/(num_true_pos + num_false_neg)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(X, X_char, y, embeddings, char_embeddings, manual_params=None, k=10):\n",
    "    ex_indices = list(range(X.shape[0]))\n",
    "    random.shuffle(ex_indices)\n",
    "    \n",
    "    accuracy = np.zeros(k)\n",
    "    precision = np.zeros(k)\n",
    "    recall = np.zeros(k)\n",
    "    f1 = np.zeros(k)\n",
    "    \n",
    "    #calculate the splitting scheme\n",
    "    splits = [X.shape[0]//k] * k\n",
    "    for i in range(X.shape[0] % k):\n",
    "        splits[i] += 1\n",
    "    \n",
    "    #keeps track of current location in \n",
    "    index = 0\n",
    "    for i in range(k):\n",
    "        #come up with the train-test split\n",
    "        X_test = np.asarray([X[i] for i in ex_indices[index:index+splits[i]]])\n",
    "        X_char_test = np.asarray([X_char[i] for i in ex_indices[index:index+splits[i]]])\n",
    "        y_test = np.asarray([y[i] for i in ex_indices[index:index+splits[i]]])\n",
    "        \n",
    "        train_indices = ex_indices[0:index] + ex_indices[index+splits[i]:]\n",
    "        X_train = np.asarray([X[i] for i in train_indices])\n",
    "        X_char_train = np.asarray([X_char[i] for i in train_indices])\n",
    "        y_train = np.asarray([y[i] for i in train_indices])\n",
    "        \n",
    "        #now train the model on this split and save the metrics\n",
    "        cnn = train_cnn_classifier(X_train, X_char_train, y_train, \n",
    "                                   embeddings, char_embeddings, num_classes=2, \n",
    "                                   manual_params=manual_params, verbose=False)\n",
    "        \n",
    "        results = calc_metrics(cnn, X_test, X_char_test, y_test)\n",
    "        accuracy[i] = results['accuracy']\n",
    "        precision[i] = results['precision']\n",
    "        recall[i] = results['recall']\n",
    "        f1[i] = results['f1']\n",
    "        \n",
    "        index += splits[i]\n",
    "    \n",
    "    return {'accuracy': np.mean(accuracy), 'precision': np.mean(precision), \n",
    "           'recall': np.mean(recall), 'f1': np.mean(f1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(X, X_char, y, embeddings, char_embeddings, params, metric='f1', k=10):\n",
    "    \n",
    "    results = []\n",
    "    keys = []\n",
    "    values = []\n",
    "    for key in params:\n",
    "        keys.append(key)\n",
    "        values.append(params[key])\n",
    "    \n",
    "    for config in product(*values):\n",
    "        p = {}\n",
    "        for i, v in enumerate(config):\n",
    "            p[keys[i]] = v\n",
    "        \n",
    "        res = kfold(X, X_char, y, embeddings, char_embeddings, manual_params=p, k=k)\n",
    "        results.append((p, res))\n",
    "    \n",
    "    return sorted(results, reverse=True, key=lambda x: x[1][metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Relevancy CNN Classifier\n",
      "Time taken: 251.318575 seconds\n",
      "\n",
      "Relevancy metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6588235294117647,\n",
       " 'precision': 0.6293436293436293,\n",
       " 'recall': 0.7353383458646616,\n",
       " 'f1': 0.6782246879334257}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taco = int(2*len(X)/3)\n",
    "X_train = X[:taco]\n",
    "X_char_train = X_char[:taco]\n",
    "y_train = y[:taco]\n",
    "\n",
    "X_test = X[taco:]\n",
    "X_char_test = X_char[taco:]\n",
    "y_test = y[taco:]\n",
    "\n",
    "print('Training Relevancy CNN Classifier')\n",
    "cnn = train_cnn_classifier(X_train, X_char_train, y_train, embeddings, char_embeddings, 2)\n",
    "print('\\nRelevancy metrics:')\n",
    "calc_metrics(cnn, X_test, X_char_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 406.577297 seconds\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "calc_metrics() missing 1 required positional argument: 'y_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-942e649a15cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-130-0b8cd7e53691>\u001b[0m in \u001b[0;36mkfold\u001b[0;34m(X, X_char, y, embeddings, char_embeddings, manual_params, k)\u001b[0m\n\u001b[1;32m     31\u001b[0m                                    manual_params=manual_params, verbose=False)\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0maccuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprecision\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: calc_metrics() missing 1 required positional argument: 'y_test'"
     ]
    }
   ],
   "source": [
    "kfold(X, X_char, y, embeddings, char_embeddings, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first gridsearch for the best region size\n",
    "#figure out region size first\n",
    "filter_sizes = []\n",
    "for i in range(10):\n",
    "    filter_sizes.append([i+1])\n",
    "filter_sizes.append([15])\n",
    "filter_sizes_search = gridsearch(X, y, embeddings, params={'filter_sizes': filter_sizes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
